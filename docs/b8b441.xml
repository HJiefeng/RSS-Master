<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>Stories by Pinterest Engineering on Medium</title>
        <link>https://medium.com/@Pinterest_Engineering?source=rss-ef81ef829bcb------2</link>
        
        <item>
            <id>https://medium.com/p/9eb356fee676</id>
            <title>500X Scalability of Experiment Metric Computing with Unified Dynamic Framework</title>
            <link>https://medium.com/pinterest-engineering/500x-scalability-of-experiment-metric-computing-with-unified-dynamic-framework-9eb356fee676?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/9eb356fee676</guid>
            <pubDate></pubDate>
            <updated>2025-05-13T19:01:37.736Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Xinyue Cao | Software Engineer Tech Lead; Bojun Lin | Software Engineer</p><h3>Overview</h3><p>Experimentation is a cornerstone of data-driven decision making at Pinterest. Every day, thousands of experiments run on our platform, generating insights that drive product decisions and business strategies. By the end of 2024, over 1,500 metrics — created by experiment users — are computed daily on our Experimentation Platform to meet diverse analytical needs.</p><p>But as the scale of experimentation grew, so did the challenges. Delays in upstream data ingestion, difficulties in backfilling skipped metrics, and frequent scalability issues began to hinder our ability to deliver timely and reliable results.</p><p>Enter the <strong>Unified Dynamic Framework (UDF)</strong> — a scalable, resilient solution that has transformed how we compute experiment metrics. With UDF, our pipelines now support 100X metrics and are designed to scale up to 500X in the coming years. This framework has not only ditched all upstream dependencies and accelerated metric delivery but also reduced engineering effort from months to days to build data pipelines, enabling faster experimentation and innovation. Essentially, it achieves standardization of metric processing for Pinterest experimentation. Engineers are able to focus solely on innovation of metric computation because the majority of the infrastructure challenges and pipeline creation complexities are offloaded to the unified dynamic framework.</p><p>In this post, we’ll dive into how UDF addresses these challenges and the impact it has had on our experimentation platform.</p><h3>Experiment Metric Computing at Pinterest</h3><p>Experiment analysis at Pinterest is powered by Helium, our in-house experimentation platform. Helium supports end-to-end experiment analysis, from setup to metric evaluation. Users can define custom action types (e.g., distinguishing between long and short clickthroughs) and analyze aggregated metrics across user groups, including lift values, p-values, and more.</p><h4>Metric Computing Workflow</h4><ol><li>Data Ingestion: Metrics are ingested into a standard source table from various data jobs created by experiment users.</li><li>Metric Computation: Once all ingestions are complete, the metric computing pipelines calculate the metrics and store the results in Druid tables for visualization on Helium dashboards.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*E0_WnUSAeIvj2321" /></figure><p>While this design offers flexibility for users to incorporate custom actions, it also introduces several challenges:</p><ol><li>Dependency on Upstream Jobs: Metric computation relies heavily on the readiness of upstream data insertion jobs, which are built and maintained by experiment users. Delays in one upstream job can stall the entire pipeline.</li><li>Backfill Complexity: Skipped metrics due to upstream delays are difficult to backfill, and tracking these skipped actions is cumbersome.</li><li>Scalability Issues: As the number of metrics and data volume grow, the pipelines frequently face scalability challenges, such as out-of-memory (OOM) errors.</li></ol><p>These pain points highlighted the need for a more scalable, resilient, and unified solution.</p><h3>UDF Approach: Dynamic Processing</h3><h4>• Addressing Various Upstream Dependencies with Dynamic DAGs</h4><p>To resolve bottlenecks caused by inconsistent SLAs across upstream data insertion jobs, we leveraged <a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/dynamic-dag-generation.html">Apache Airflow Dynamic DAG</a>, which supports creation of DAGs that have a structure generated dynamically. Pipeline structures are generated dynamically based on the readiness of input data. The framework checks for available input data every few hours and processes metrics in parallel batches to balance workloads. Batch sizes are dynamically adjusted based on the traffic and resource utilization of the DAG run, ensuring efficient resource allocation. So instead of waiting for all upstream jobs to complete, the framework processes metrics in small, parallel batches using a first-come, first-served approach: ready inputs are prioritized and processed immediately, while delayed upstreams are handled incrementally. This approach ensures that even if some upstream jobs are delayed, others can proceed without blocking the entire pipeline.</p><h4>• Avoiding Duplicate Computation</h4><p>To prevent redundant processing or metrics being double counted, we introduced a mechanism to persist the list of metrics being computed. Each DAG execution checks this list to exclude metrics already in progress, eliminating duplicate computations and ensuring data integrity.</p><h4>• Automatic Backfill for Skipped Metrics</h4><p>Metrics are refreshed daily, but upstream delays can cause some metrics to be skipped. To address this, the framework looks back N days to identify and backfill skipped metrics automatically. As long as the upstream delay is within the backfill window, metrics are computed and delivered to dashboards within hours.</p><h4>• Notifications</h4><p>Metric owners receive email notifications if their metrics are delayed or skipped (when the delay is out of the backfill window). A separate pipeline tracks metric ownership, checks for delays, and sends notifications to the respective metric owners.</p><h4>• Metrics Processing Tracking</h4><p>To ensure transparency and reliability in metric processing, we’ve built a unified tracking within the framework. It monitors the end-to-end lifecycle of every metric, including delays in source or computation, processing status actively or queues for next executions, skipped due to SLA violations or queued for backfills.</p><p>They enable robust governance by providing visibility into pipeline health and drive automation for automatic backfills, as well as prevention of partial data or duplicate computation. By centralizing this logic, the framework ensures metrics are processed accurately, efficiently, and cost-effectively — even at scale.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*mK4o82_tpQOYCfDm" /></figure><h3>UDF Approach: Unified Framework for All Experiment Pipelines</h3><p>The UDF encapsulates all dynamic features above, enabling experiment data pipelines to handle delays in various data sources, automatically backfill skipped metrics within a customizable window, and send notifications for delayed or skipped metrics.</p><h4>• Simplified Pipeline Creation</h4><p>Creating a new dynamic metric processing pipeline with UDF is as simple as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*X88ER9du0QJx3cYs" /></figure><p>You only need to specify:</p><ol><li>metric_type: Supported types include aggregated, calendar day, daysin, and any new metric types with user-defined computation logic (queries or Spark functions).</li><li>dag_creation_args: Dynamic schedules, parallel batch allocation, and execution configurations.</li></ol><h4>• Separations of Metric Computation and Pipeline Creation</h4><p>UDF separates metric computation logic from pipeline creation and is structured into two layers:</p><ol><li><strong>Metric Computing Layer</strong>: Handles computation logic for different metric types.</li><li><strong>Dynamic DAG Creation Layer</strong>: Generates the DAG layout and provides built-in dynamic features, such as:</li></ol><ul><li>Backfilling metrics from the past N days.</li><li>Balancing workload with dynamically allocated batches.</li><li>Tracking metric processing progress and history.</li><li>Extended plugins for full experiment range backfills and on-demand metrics for specific experiments.</li></ul><h4>• Experiment Metrics Metadata</h4><p>All metrics processed by UDF are sourced from the Experiment Metrics Metadata system, a centralized storage designed to streamline user interaction and metrics management. This system enables users to define metrics or segments, configure metric types, and select metrics for experiments, among other tasks.</p><h3>UDF Architecture Diagram</h3><p>Based on the principles outlined earlier, this section demonstrates how the UDF integrates its core components shared as above. The diagram below illustrates the architecture in practice, showcasing how dynamic adaptability and unified framework with centralized governance work synergistically to deliver scalable, reliable metric computation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6ldUBGlsYtxBm0VZ" /></figure><h3>Results and Conclusions</h3><p>The launch of the Unified Dynamic Framework has revolutionized experiment metric computing at Pinterest. We successfully onboarded all organic experiment pipelines to UDF in just two weeks, achieving standardization across the platform — a testament to the framework’s simplicity, efficiency, and ease of use.</p><p>It improved developer velocity (from months to days to build data pipelines end to end). Developers now spend zero time on pipeline design and implementation and only focus on innovation of metric computation rather than infrastructure challenges.</p><p>For each pipeline using UDF, we achieved:</p><ul><li><strong>Flexibility: </strong>enable users to bring any customized metrics effortlessly by self-serve without concern of upstream dependencies or metrics left behind.</li><li><strong>Scalability</strong>: UDF supports <strong>100X more metrics</strong> today and is designed to scale to <strong>500X</strong> in the future. It accommodates growing data volumes and metric complexity with minimal maintenance.</li><li><strong>Speed</strong>: metrics are delivered <strong>4X faster</strong> at least. This is measured against the duration between source data ready and final metric results available with pipeline completion.</li><li><strong>Reliability</strong>: <strong>90% of partial data issues</strong> are resolved through automatic backfills, and <strong>100% of maintenance effort</strong> for upstream failures is eliminated.</li></ul><p>The standardization of metric computing across the experimentation platform led to immense improvements on driving innovation and business outcomes. We’re excited to see how this framework will continue to empower experimentation and deliver value to our Pinners!</p><h3>Acknowledgement</h3><p>I want to sincerely thank the individuals and teams whose outstanding work made this project possible. Your contributions were invaluable.</p><ul><li>Lu Liu, Lu Yang, Bryant Xiao, and Chunyan Wang who established the prototypes of Pinterest experiment metric pipelines</li><li>Yulei Li and workflow team who integrated Dynamic Airflow into Pinterest workflow platform</li><li>Yi Yang and Druid team who supported Druid partitions during the workflow redesign</li></ul><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9eb356fee676" width="1" /><hr /><p><a href="https://medium.com/pinterest-engineering/500x-scalability-of-experiment-metric-computing-with-unified-dynamic-framework-9eb356fee676">500X Scalability of Experiment Metric Computing with Unified Dynamic Framework</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/08ec7f4aa857</id>
            <title>Multi-gate-Mixture-of-Experts (MMoE) model architecture and knowledge distillation in Ads…</title>
            <link>https://medium.com/pinterest-engineering/multi-gate-mixture-of-experts-mmoe-model-architecture-and-knowledge-distillation-in-ads-08ec7f4aa857?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/08ec7f4aa857</guid>
            <pubDate></pubDate>
            <updated>2025-04-24T16:02:27.371Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <h3>Multi-gate-Mixture-of-Experts (MMoE) model architecture and knowledge distillation in Ads Engagement modeling development</h3><p>Authors: Jiacheng Li | Machine Learning Engineer II, Ads Ranking; Matt Meng | Staff Machine Learning Engineer, Ads Ranking; Kungang Li | Principal Machine Learning Engineer, Ads Performance; Qifei Shen | Senior Staff Machine Learning Engineer, Ads Ranking</p><h3>Introduction</h3><p>Multi-gate Mixture-of-Experts (MMoE)[1,2] is a recent industry-proven powerful architecture in neural network models that offers several significant benefits. First, it enhances model efficiency by dynamically allocating computational resources to different sub-networks (experts) based on the input data, ensuring that only the most relevant experts are activated for each task. This selective activation reduces computational overhead and improves inference speed. Second, MMoE promotes better generalization and performance by allowing the model to learn specialized features through multiple experts, each focusing on different aspects of the data. This specialization helps in capturing complex patterns and relationships that a single monolithic model might miss. Additionally, the multi-gate mechanism enables the model to handle multi-task learning more effectively, as it can tailor the contribution of each expert to different tasks, leading to improved accuracy and robustness across various applications. Overall, MMoE provides a flexible, efficient, and powerful approach to building advanced neural network models.</p><p>On top of the MMoE model architecture, we also propose to use knowledge distillation[4] to mitigate the performance gap due to short data retention period and further enhance the new model performance. More specifically, we distill knowledge from existing production models to experimental models during the batch training stage to make the experimental models converge to a better state.</p><h3>Multi-gate-Mixture-of-Experts (MMoE) model architecture</h3><p>A known problem for the DCNv2[4] style recommendation system model is that simply adding more layers and more parameters cannot bring proportional metric gain. How to effectively make the model larger and bring proportional sizable gain is a question for all. The MMoE model architecture is a potential solution to help our engagement model learn more complicated patterns and relationships between users and their ads engagement actions, and eventually perform better at the ads-user matching task.</p><p>We started from our current shared-bottom model architecture with DCNv2. We experimented with various architectures for experts and their combinations: DCNv2, Masknet[5], FinalMLP[6] etc. Besides these seemingly advanced architectures, we also notice that adding MLP-based experts can further lift the offline metrics. Through experiments, we realized that the return of investment (ROI) is decreasing with more experts added and for our use case DCNv2 has the highest ROI. Therefore we chose the most suitable combination of experts via careful consideration of metrics-infrastructure cost tradeoff.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-iPLCErfhSS-phAG" /><figcaption>Figure 1: (a) Shared-Bottom model. (b) One-gate MoE model. © Multi-gate MoE model. From [2]</figcaption></figure><p>Serving an MMoE style model would bring noticeable infrastructure cost increase since multiple experts are introduced, and we also explored various techniques to reduce the infrastructure cost while keeping the model performance on-par. One promising solution is mixed precision inference. Luckily our team has already explored and productionized mixed precision inference [7] and we can apply this great technique out of the box. From experiments, we verified that mixed precision inference has nearly zero impact on model offline performance. By applying this mixed precision inference, we observe 40% inference latency reduction in benchmarking and this latency reduction translates to a significant amount of infrastructure cost.</p><p>Besides mixed precision inference, through experiments, we observed that the gate layers of MMoE only need very simple architecture and small amounts of parameters to achieve similar performance as very sophisticated architectures that we use for experts. Lightweight gate layers bring further infrastructure cost reduction.</p><h3>Knowledge distillation from production model</h3><p>In the ads engagement modeling world, the volume of data is huge. A common practice is to retain the data for a relatively short period of time, usually a few months to a year. When a new modeling idea is well tested offline and ready for online experimentation to validate online metrics movements, a common issue is that the training data of the current production model is no longer available, making the comparison between the new experimental model and the production model unfair.</p><p>To mitigate this issue, we proposed to use knowledge distillation from the production model to help the new experimental model “learn” from old data that has been deleted from the training dataset, thus enhancing the performance of the new experimental model. More specifically, on top of the standard cross entropy loss calculated using binary labels, we add a new loss to calculate the prediction differences between the experimental model and the production model. We experimented with various loss functions and we found out that pairwise style loss not only can mitigate the performance gap caused by missing data, but also can boost the experimental model offline metrics further. Applying the knowledge distillation loss in the batch training stage seems undoubtedly correct, but the story is different for the incremental training stage. Moreover, when the experimental model is promoted to production, the question becomes harder to answer: should a model distill from itself? Through experiments, in our current design, we observe significant overfitting if we keep the distillation loss in incremental training and thus we decide to remove distillation loss in the incremental training stage.</p><p>Besides smoothing the metric movements oriented model training, knowledge distillation can also help for no metric movement cases, such as feature upgrade or new computation graph improvement for serving latency reduction. In such cases, warm starting from a production model checkpoint is no longer available and retraining a production model is necessary. Retraining a production model will also suffer from the above mentioned data missing problem, and knowledge distillation can kick in here to make sure the retrained production model can have on-par offline and online performance.</p><h3>Evaluation</h3><p>In this section, we show some offline and online results [8] for the MMoE model on different view types (RelatedPins and Search). The baseline model is our production model with DCNv2 architecture and internal training data. It is to be noted that 0.1% offline accuracy improvement in the Engagement ranking model is considered to be significant. Therefore the MMoE model architecture with knowledge distillation increases both online and offline metrics very significantly.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*qDjKYb0eFAQS9ui15icwGQ.png" /></figure><h3>Conclusion</h3><p>MMoE model architecture is capable of modeling more sophisticated tasks, especially in multitask learning (MTL).</p><p>By leveraging knowledge distillation technique, we successfully mitigate the performance gap caused by short data retention period and help the new experimental model learn from deleted data. The practically longer training data window and large data volume can effectively help us to improve the ads matching quality and Pinner experience.</p><p>As a result of these endeavors, Pinterest continues to deliver highly desirable, adaptive, and relevant recommendations that inspire and drive discovery for each unique user.</p><h3>Acknowledgements</h3><p>This work represents a result of collaboration of the engagement modeling team members and across multiple teams at Pinterest.</p><p>Engineering Teams:<br />Ads Ranking: Duna Zhan, Liangzhe Chen, Dongtao Liu<br />Ads Infra: Shantam Shorewala, Yiran Zhao, Haoyang Li<br />Data Science: Milos Curcic, Adriaan ten Kate<br />Leadership: Ling Leng, Caijie Zhang, Prathibha Deshikachar</p><h3>References</h3><p>[1] Jacobs, Robert A., et al. “<a href="https://ieeexplore.ieee.org/abstract/document/6797059">Adaptive mixtures of local experts</a>.” <em>Neural computation</em> 3.1 (1991): 79–87.<br />[2] Ma, Jiaqi, et al. “<a href="https://dl.acm.org/doi/abs/10.1145/3219819.3220007">Modeling task relationships in multi-task learning with multi-gate mixture-of-experts</a>.” Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining. 2018.<br />[3] Hinton, Geoffrey, et al. “<a href="https://arxiv.org/pdf/1503.02531">Distilling the Knowledge in a Neural Network</a>.” arXiv preprint arXiv:1503.02531.<br />[4] Wang, Ruoxi, et al. “<a href="https://dl.acm.org/doi/abs/10.1145/3442381.3450078">Dcn v2: Improved deep &amp; cross network and practical lessons for web-scale learning to rank systems</a>.” Proceedings of the web conference 2021. 2021.<br />[5] Wang, Zhiqiang, et al. “<a href="https://arxiv.org/pdf/2102.07619">Masknet: Introducing feature-wise multiplication to CTR ranking models by instance-guided mask</a>.” arXiv preprint arXiv:2102.07619 (2021).<br />[6] Mao, Kelong, et al. “<a href="https://ojs.aaai.org/index.php/AAAI/article/download/25577/25349">FinalMLP: an enhanced two-stream MLP model for CTR prediction</a>.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. №4. 2023.<br />[7] Lei, Yulin, et al. “<a href="https://medium.com/pinterest-engineering/user-action-sequence-modeling-for-pinterest-ads-engagement-modeling-21139cab8f4e">User Action Sequence Modeling for Pinterest Ads Engagement Modeling</a>”. Pinterest Engineering Blog.<br />[8] Pinterest Internal Data, US, 2024.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=08ec7f4aa857" width="1" /><hr /><p><a href="https://medium.com/pinterest-engineering/multi-gate-mixture-of-experts-mmoe-model-architecture-and-knowledge-distillation-in-ads-08ec7f4aa857">Multi-gate-Mixture-of-Experts (MMoE) model architecture and knowledge distillation in Ads…</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/8a836c88fea5</id>
            <title>Migrating 3.7 Million Lines of Flow Code to TypeScript</title>
            <link>https://medium.com/pinterest-engineering/migrating-3-7-million-lines-of-flow-code-to-typescript-8a836c88fea5?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/8a836c88fea5</guid>
            <pubDate></pubDate>
            <updated>2025-04-16T19:17:42.564Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><strong>Authors</strong>: Jack Hsu | Staff Software Engineer, Core Web Platform; Mark Molinaro | Staff Software Engineer, Code and Language Runtime</p><p>Pinterest migrated 3.7 million lines of code from Flow to TypeScript in eight months, leading to better type safety, developer experience, and improved hiring. Along the way, we learned a lot and greatly benefited from the open source community, and it’s our turn to give back. Today, we’re excited to share our learnings and <a href="https://github.com/stripe-archive/flow-to-typescript-codemod/pull/7">contributions</a> to <a href="https://github.com/stripe-archive/flow-to-typescript-codemod">Stripe’s flow-to-typescript codemod</a>!</p><h3>Why?</h3><p>In 2016, Pinterest began adding types to our JavaScript codebase. At that time, we chose Flow over TypeScript for several reasons:</p><ol><li><strong>Gradual Adoption</strong>: Flow allowed for easier incremental adoption, which was crucial since we had no static type checking and needed to add types file-by-file.</li><li><strong>Working with React</strong>: Flow, which was open sourced by Meta, appeared more future-proof — particularly with React, which was also open sourced by Meta — promising seamless integration.</li></ol><p>However, over the years, the industry settled on TypeScript as the standard for JavaScript type checking. This is reflected in <a href="https://npmtrends.com/flow-bin-vs-typescript">npm download trends</a>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FogjflaZ-AxGIFPw9MaFXQ.png" /></figure><p>In late 2023, we decided to adopt TypeScript for the following reasons:</p><ul><li><strong>Feasibility: </strong>We achieved 100% Flow coverage and a variety of OSS Flow-to-TS migration tools were released, meaning we could migrate in a single (massive) commit!</li><li><strong>Better Community Support</strong>: In terms of both community and libraries.</li><li><strong>Language Features</strong>: TypeScript provides better language features, such as conditional types, const assertions, non-null assertions, etc.</li><li><strong>Talent Availability</strong>: It’s easier to hire TypeScript-proficient developers.</li></ul><h3>How we did it</h3><p>After researching prior migrations by companies such as <a href="https://medium.com/@michaelsholty/migrating-500k-lines-of-flow-code-to-typescript-15a8cad43fec">Zapier</a>, <a href="https://medium.com/airtable-eng/the-continual-evolution-of-airtables-codebase-migrating-a-million-lines-of-code-to-typescript-612c008baf5c">Airtable</a>, and <a href="https://stripe.com/blog/migrating-to-typescript">Stripe</a>, we decided to employ a similar “big bang” approach: migrating the entire codebase at once using a codemod.</p><p>The migration can be divided into three key phases: (1) <strong>Setup</strong>, (2) <strong>Conversion</strong>, and (3) <strong>Integration</strong>.</p><h4>1. Setup</h4><p>We began by installing Typescript and <a href="https://typescript-eslint.io/">@typescript-eslint</a>, and proceeded with the the following configurations:</p><ul><li>tsconfig.json: We configured it to be strict, in line with the recommendations from the popular <a href="https://www.totaltypescript.com/tsconfig-cheat-sheet">TSConfig Cheat Sheet</a>, though making a few exceptions in the interest of time.</li><li>package.json: We configured it so that running yarn tsc would execute as NODE_OPTIONS= — max-old-space-size=&lt;memory in MB&gt; yarn tsc. This prevents out-of-memory (OOM) errors with the TypeScript compiler.</li><li>.vscode/settings.json: We modified settings for Visual Studio Code, our officially supported IDE for web, for better TypeScript support:</li></ul><pre>typescript.tsserver.maxTsServerMemory: &lt;memory in MB&gt;,<br />typescript.enablePromptUseWorkspaceTsdk: true,<br />typescript.tsdk: &quot;node_modules/typescript/lib&quot;,<br />typescript.tsserver.experimental.enableProjectDiagnostics: true,</pre><p>We found @typescript-eslint was incompatible with the version of ESLint we were using. Additionally, newer versions of ESLint were not compatible with <a href="https://github.com/pinterest/esprint">esprint</a>, our in-house ESLint runner designed for fast linting. Fortunately, <a href="https://github.com/discord/eslint/tree/parallel">Discord’s ESLint fork</a> provided the perfect alternative. Adopting this fork resolved the ESLint upgrade blockage <strong>and</strong> improved the full-project ESLint linting latency by 40% (from 120s to 70s).</p><h4>2. Conversion</h4><p>We aim to convert Flow to TypeScript in a single automated run. The steps include migrating dependencies, running codemods, suppressing ESLint errors, and automating the entire process.</p><p><strong>2.1 Migrate type dependencies</strong></p><p>We needed to add TypeScript support for all our auto-generated types:</p><ul><li><a href="https://thrift.apache.org/">Apache Thrift</a> (cross-platform data structures): The generated types remain largely unchanged, except for adding the as const assertion to TypeScript enums and objects, which ensures TypeScript can infer the types as narrowly as possible.</li><li><a href="https://www.openapis.org/">OpenAPI</a> (REST): We modified the OpenAPI type generation jobs to generate TypeScript types instead of Flow types. Using <a href="https://www.npmjs.com/package/openapi-typescript">openapi-typescript</a> made this transition simple. We only had to build a thin wrapper built around it.</li><li><a href="https://relay.dev/">Relay</a> (GraphQL): Relay offers native support for TypeScript. However, for the usage of Relay’s React hooks, we needed to rewrite most of them due to significant differences in type definitions for React hooks.</li></ul><p><strong>2.2 Run code conversion codemod<br /></strong>The goal of the code transformation step is to convert Flow syntax to valid TypeScript syntax without altering any code logic. We achieved this using the “convert” feature in the typescriptify codemod:</p><p>The codemod skips files that trigger foundDeclarationFile warnings, so we double-checked the resulting CSV file to ensure all files were converted.</p><p>While the codemod can successfully handle the majority of cases, we needed to implement <a href="https://github.com/stripe-archive/flow-to-typescript-codemod/pull/7">some patches</a> for specific use cases, including but not limited to:</p><ul><li>Handling more syntax: T[K], $Partial, $ReadOnlySet, $ReadOnlyMap.</li><li>Fixing bugs with maybe functions: ?() =&gt; void</li><li>Fixing bugs with interaction types: (A | B) &amp; (C | D)</li><li>Fixing false positives of private types</li><li>Improving handling of flow-related comments</li><li>Improving support for react and react-router-dom types</li></ul><p>Additionally, we developed an ESLint autofix rule to differentiate React.Node and React.Element from the global DOM types Node and Element:</p><pre>import { type Node, type Element } from 'react'; // Before ❌<br />import { type Node as ReactNode, type Element as ReactElement } <br />from 'react'; // After ✅</pre><p><strong>2.3 Run TypeScript error suppression codemod<br /></strong>The goal of the error suppression step is to suppress all TypeScript errors using @ts-expect-error comments, ensuring no type errors. We achieved this using the “fix” feature in the typescriptify codemod:</p><pre>NODE_OPTIONS=--max-old-space-size=&lt;memory in MB&gt; yarn typescriptify fix <br />--autoSuppressErrors --config tsconfig.json --path /path/to/codebase/</pre><p>A common issue with the codemod is that it can insert TypeScript error suppressions below eslint-disable-next-line, which expect the error to be on the next line:</p><pre>// eslint-disable-next-line ❌<br />// @ts-expect-error ✅<br />[some code]</pre><p>To resolve this, we wrote a script to swap the order of the error suppressions:</p><pre>// @ts-expect-error ✅<br />// eslint-disable-next-line ✅<br />[some code]</pre><p><strong>2.4 Suppress ESLint errors<br /></strong>ESLint’s autofix is another powerful tool for enhancing TypeScript code quality:</p><ul><li>To enable type-only imports, which is <a href="https://typescript-eslint.io/blog/consistent-type-imports-and-exports-why-and-how/#benefits-of-enforcing-type-only-importsexports">recommended</a>, we enabled the following:<br />– <a href="https://typescript-eslint.io/rules/consistent-type-imports/">consistent-type-imports</a> with fixStyle: ‘inline-type-imports’ option<br />– <a href="https://github.com/import-js/eslint-plugin-import/blob/main/docs/rules/consistent-type-specifier-style.md">import/consistent-type-specifier-style</a> with prefer-inline option</li><li>To remove unused imports, we enabled <a href="https://github.com/sweepline/eslint-plugin-unused-imports">unused-imports/no-unused-imports</a>.</li><li>Additionally, we updated some Flow and JavaScript-specific ESLint rules to their TypeScript equivalents:<br />– <a href="https://eslint.org/docs/latest/rules/no-unused-vars">no-unused-vars</a> ⇒ <a href="https://typescript-eslint.io/rules/no-unused-vars/">@typescript-eslint/no-unused-vars</a><br />– <a href="https://github.com/gajus/eslint-plugin-flowtype/blob/master/.README/rules/no-weak-types.md">flowtype/no-weak-types</a> ⇒ <a href="https://typescript-eslint.io/rules/no-explicit-any/">@typescript-eslint/no-explicit-any</a></li></ul><p><strong>2.5 Automate the conversion<br /></strong>We wrote a script to automate all the steps mentioned above. Inevitably, the codemod won’t catch all issues. We consolidated all the manual fixes into a single commit, which we added as a cherry-pick commit command to the end of the automation script.</p><p>By running the automation daily, we significantly minimized merge conflicts and the number of issues requiring manual intervention. On rollout day, the final automation run was stress-free, as we had already executed the process numerous times with ease.</p><h4>3. Integration</h4><p>We aim to adapt all our existing systems to function within the new TypeScript environment in a manner that maintains both forward and backward compatibility. The steps include TypeScript transpilation and updating JS filename references.</p><p><strong>3.1 TypeScript Transpilation<br /></strong>Our codebase already utilizes Babel for transpilation from Flow in addition to a collection of in-house transpilation plugins. With a quick introduction of <a href="https://babel.dev/docs/babel-plugin-transform-typescript">babel-plugin-transform-typescript</a> and the requisite fiddling of configs, we completed the majority of our transpilation work. After implementing a few additional adjustments to our custom plugins to ensure compatibility with both Flow and TypeScript transpiled outputs, we could successfully transpile the codebase with minimal differences in the output. You can find the steps we took to validate this output against our current Flow setup in the Validation section below.</p><p><strong>3.2 Updating JS Filename References<br /></strong>With confidence that the transpiled output matches the existing one, we must be close to the finish line, right? Alas, we still had a whole host of fixups to be made to ensure parity across our build &amp; CI systems. As <a href="https://youtu.be/Wbo2CSh-2ys?si=-45fS-lUZt-FrjOj&amp;t=2695">Tyler Krupicka noted during Stripe’s migration</a>, not only are we changing the type annotations in each file, but we are also changing the file names (or, really, extensions) as well!</p><p>Tracking down all the places we relied on filenames was laborious, as we found many cases that required integrations with systems even outside our codebase. Below are a few types of issues we had work through:</p><p><em>In-code JS filename references<br /></em>Across our webpack, linting, and testing configuration, in addition to a bunch of ad-hoc automation, we historically made assumptions about which file types included source code. A large swath of scripts and tools that needed to find all source code had some logic like if filename.endsWith(“.js”) or glob(“**.*.js”).</p><p>While using Flow, all source files were indeed .js files. However, our React/JSX configuration allowed for JSX syntax without a .jsx extension. Ultimately, we had to track down almost 100 in-code references to JS files and modify them all to check both .ts &amp; .tsx (in addition to the currently-present .js) files.</p><p><em>Service-side JS filename references<br /></em>We utilize Cypress for our end-to-end (E2E) tests, along with a complementary in-house service (Metro) to track test outcomes over time, identify flaky tests, and overall support our testing efforts. Metro’s data model, however, keys everything off of the test path (with extension included). To keep the steamship rolling, we shimmed all our interactions with the metro service to account for sending and receiving the expected file paths on the client side. It helps that there was only one client which was totally in our control.</p><p><em>File-system based routing JS filename references<br /></em>We uniquely identify every page on <a href="https://www.pinterest.com">pinterest.com</a> with a HandlerId. Historically, this HandlerId was simply the relative path to the <strong>source file</strong> for each page. During local development, this setup allows us to lazily build pages by using the HandlerId-as-a-filepath as a Webpack entry point, building the page only when it is requested. As valuable identifiers tend to do, this build-time ID ended up being used across various monitoring, logging, and alerting systems.</p><p>Just as above, the integrations with other systems demanded we maintain consistency with our handler IDs. We achieved this by encapsulating the handler in a new container so we could more precisely track down all its uses instead of string-hunting. From there, we added a similar shim based on the system of interest — in build we look for TS file, in reporting we keep it JS. Maintaining consistency in HandlerId also provided the additional benefit of enabling a side-by-side comparison of all our metrics when rolling out our first Typescript payload!</p><h3>Validation</h3><p>Extraordinary claims require extraordinary evidence. To demonstrate that the migration of million lines of code functioned as intended, we tested the entire <a href="https://kentcdodds.com/blog/the-testing-trophy-and-testing-classifications">testing trophy</a>:</p><ol><li><strong>Daily Automated Testing</strong>: We regenerated the TypeScript branch from the master branch and validated it daily. By the time of the cutover, we had generated and validated the branch over 60 times.</li><li><strong>Multiple Rounds of Manual Testing</strong>: Our quality assurance team conducted three full rounds of manual testing, during which no significant issues were found. Before the rollout, we also temporarily deployed the TypeScript branch to a small percentage of production traffic to identify production-specific problems early.</li><li><strong>Byte-for-Byte Static Analysis</strong>: Since runtime execution is determined by the Babel-transpiled JavaScript code, rather than the Flow/TypeScript source code<strong>,</strong> we performed a byte-for-byte comparison between the JavaScript code transpiled from Flow and that transpiled from TypeScript, verifying that they were equivalent.</li></ol><h3>Rollout</h3><p>Given the scale of the migration and the number of engineers affected, we needed to address several key considerations:</p><ul><li><strong>Separate TypeScript Changes: </strong>How can we isolate the migration changes from others, making it easy to identify any issues related to the migration?</li><li><strong>Minimize Developer Disruption: </strong>How can we minimize the time developers are blocked during the migration process?</li><li><strong>Enable Quick Recovery: </strong>How can we quickly recover if something goes wrong?</li></ul><p>After careful consideration, we developed the following rollout plan:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*56JLfMOsb_Uzf7Gbuglnaw.png" /></figure><p>On Thursday, after merging the TypeScript branch, we encountered failures in CI jobs triggered specifically by merges to the master branch. Fortunately, we quickly identified the root cause, implemented a forward fix, and unlocked the repository without any delays.</p><p>On Friday, we deployed the changes to our canary environment and re-ran all automated tests. While some tests failed initially, we quickly identified the root cause as an unrelated issue stemming from a backend service deployment. After resolving this, we achieved a successful deployment, marking Pinterest’s official transition to TypeScript!</p><p>Deploying on a Friday might seem unconventional, but we were confident in the success of the migration and prepared to address any issues over the weekend if necessary. We chose Friday because, should any issues arise and be discovered late, they would have a smaller impact due to the lower website traffic typically experienced during weekends, compared to the weekdays.</p><p>After a quiet weekend, as expected, we lifted the code freeze and returned to business as usual. The entire rollout was incident-free!</p><h3>Reflections</h3><h4>Wins</h4><ul><li><strong>Smooth Migration</strong>: 97% of survey¹ respondents rated the overall migration experience as positive.</li><li><strong>Comprehensive developer education</strong>: We hosted several internal engineering training sessions, and 64% of survey respondents found the training helpful.</li><li><strong>More Intuitive Errors</strong>: 85% of survey responders found TS errors easier to understand.</li><li><strong>Better Library Types</strong>: We are able to achieve more third-party library type coverage with less code.</li><li><strong>Latest &amp; Greatest TypeScript Features</strong>: Compared to the past Flow upgrades, TypeScript upgrades have been significantly easier.</li></ul><h4>Learns</h4><ul><li><strong>Slow Type check</strong>: Performing a full type check on our entire codebase took about 3–4 minutes. Our profiling indicated that a high memory footprint was the primary cause. While we managed to reduce some latency by following the <a href="https://github.com/microsoft/TypeScript/wiki/Performance-Tracing">TypeScript performance tracing wiki</a>, further optimization proved challenging.<br />– This March, TypeScript team <a href="https://devblogs.microsoft.com/typescript/typescript-native-port/">announced 10x performance improvements</a>, and we are hoping the performance problem will be addressed soon.</li></ul><p>This migration was one of the largest projects in terms of scale and impact at Pinterest, and we believe it was also one of the most successful. We gained numerous insights and achievements, and we hope the information shared in this blog post will serve as a valuable resource for other companies undertaking similar migrations.</p><h3>Kudos</h3><p>We are thankful for:</p><ul><li>The Stripe team (<a href="https://github.com/tylerkrupicka">Tyler Krupicka</a>, <a href="https://github.com/ken-kenware">Ken Deland</a>, <a href="https://github.com/RussGlover">Russill Glover</a>, <a href="https://github.com/benbayard">Ben Bayard</a>, <a href="https://github.com/alunny">Andrew Lunny</a>) and the Airtable team (<a href="https://github.com/calebmer">Caleb Meredith</a> and <a href="https://github.com/umbrant">Andrew Wang</a>) for both the open-sourced migration codemod and the blog posts,</li><li>Alberto Carreras Carrasco, Andrew Lutz, Kessy Similien, and Yen-Wei Liu for their code contributions and code reviews,</li><li>Alice Yang, Joao Bueno, Jorge Roberto Rojas Villarreal, Juan Benavides, and Vikrant Maniar for their help with testing,</li><li>Scott Hebert, Vasa Krishnamoorthy, and many others for the support/feedback.</li><li>Jordan Cutler, Mark Cerqueira, Robert Balicki, and Vasa Krishnamoorthy for reviewing the blog post.</li></ul><p>¹ The survey was internally conducted for all Pinterest web engineers two months after the migration.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8a836c88fea5" width="1" /><hr /><p><a href="https://medium.com/pinterest-engineering/migrating-3-7-million-lines-of-flow-code-to-typescript-8a836c88fea5">Migrating 3.7 Million Lines of Flow Code to TypeScript</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/fda0efc21083</id>
            <title>Handling Network Throttling with AWS EC2 at Pinterest</title>
            <link>https://medium.com/@Pinterest_Engineering/handling-network-throttling-with-aws-ec2-at-pinterest-fda0efc21083?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/fda0efc21083</guid>
            <pubDate></pubDate>
            <updated>2025-04-07T16:07:00.769Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Jia Zhan, Senior Staff Software Engineer, Pinterest<br />Sachin Holla, Principal Solution Architect, AWS</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*0P_58HVaNAScy-ZN" /></figure><h3>Summary</h3><p>Pinterest is a visual search engine and powers over 550¹ million monthly active users globally. Pinterest’s infrastructure runs on AWS and leverages Amazon EC2 instances for its compute fleet. In recent years, while managing Pinterest’s EC2 infrastructure, particularly for our essential online storage systems, we identified a significant challenge: the lack of clear insights into EC2’s network performance and its direct impact on our applications’ reliability and performance. In this blog post, we’ll discuss our experiences in identifying the challenges associated with EC2 network throttling. We’ll also delve into how we developed network performance monitoring for the Pinterest EC2 fleet and discuss various techniques we implemented to manage network bursts, ensuring dependable network performance for our critical online serving workloads.</p><h3>Motivation</h3><p>To illustrate why we are committed to conducting network performance analysis for our online workloads, we’ll share some recent examples from our production environment at Pinterest.</p><h4>User Sequence Serving</h4><p><a href="https://medium.com/pinterest-engineering/large-scale-user-sequences-at-pinterest-78a5075a3fe9">Real time user sequence</a> became very popular at Pinterest since its launch and drove significant user engagement wins. With the continuous addition of user events and features to the platform, there’s an increasing network throughput on the underlying <a href="https://medium.com/@Pinterest_Engineering/3-innovations-while-unifying-pinterests-key-value-storage-8cdcdf8cf6aa">KVStore</a>, which is responsible for storing and serving all machine learning features. Consequently, we started observing an increase in serving latency within the KVStore. This often resulted in application timeouts and, occasionally, cascading failures like retries or connection churns that eventually caused entire KVStore clusters to go down. As a result, we frequently encountered high-severity incidents that significantly decreased Homefeed engagement every time they occurred.</p><h4>EC2 Instance Migration</h4><p>In 2024, Pinterest conducted a fleet-wide EC2 instance type evaluation and migration. As part of this journey, we were moving to the AWS <a href="https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-nitro-instances.html">Nitro</a>-based instance family, including upgrading our storage fleet from the old i3 family to i4i. However, as we were migrating our <a href="https://medium.com/pinterest-engineering/building-pinterests-new-wide-column-database-using-rocksdb-f5277ee4e3d2">widecolumn database</a>, we saw significant performance degradation across many clusters, especially for our bulk-updated workloads. For these use cases, typically datasets are generated offline in batch jobs and get bulk uploaded from S3 to the database running on EC2. We observed a substantial increase in online read latency during these uploads, leading to application timeouts. Consequently, the migration of instances (amounting to over 20,000) was put on hold.</p><p>In both cases we suffered from latency increase in our storage services, despite the system running without obvious traffic pattern changes. This prompted us to engage with AWS and dive deep into the network performance of our clusters. In the remainder of this blog post, we’ll share how we root cause and mitigate the above issues.</p><h3>EC2 Network Bandwidth</h3><p>Some EC2 instances with “up to” a specified bandwidth, such as c6i.4xl with “up to 12.5 Gbps”, have a baseline bandwidth and the ability to burst above that baseline for limited periods. It’s important to understand both the baseline and burst capabilities when planning workloads. According to <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-network-bandwidth.html">AWS documentation</a>, typically, instances with 16 vCPUs or fewer (size 4xlarge and smaller) have “up to” a specified bandwidth. In other words, while the burst bandwidth is 12.5 Gbps, the baseline bandwidth can be half of that at 6.25 Gbps. This can vary depending on the EC2 instance families, so we should always look up the <a href="https://docs.aws.amazon.com/ec2/latest/instancetypes/co.html">EC2 spec</a>.</p><p>It’s important to note that the burst bandwidth is only <strong>best-effort: </strong>It uses a network I/O credit mechanism, which is designed to provide a baseline level of bandwidth with the ability to burst when additional network bandwidth is available. This allows for flexibility in handling varying network loads while maintaining fair allocation of resources across instances.</p><p>What happens when the network allowance exceeds the limit? Packets may be queued or dropped. As a result, from an application perspective, requests may experience unusual delay and even timeouts. Typically we also see TCP retransmits spike during this time.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tsLNdTzyMnN-45pYrKQXwA.png" /></figure><p>In the previously mentioned incidents concerning user sequence serving, we discovered that the peak traffic had significantly exceeded the baseline bandwidth on those EC2 instances. This resulted in unpredictable network latency caused by traffic throttling.</p><p>In the case during the instance migration, even though the measured network throughput was well <strong>below</strong> the baseline bandwidth, we still see TCP retransmits to spike during bulk data ingestion into EC2. Since our internal metrics systems measure network usage at a per-minute interval, we suspect there may be throughput bursts that weren’t captured in the one-minute average. To confirm that, we ran various Linux network analysis tools (e.g. iftop, ifstat, nload, sar, iptraf-ng) to measure the network throughput in real time at per second or sub-second level, and surprisingly we didn’t observe any throughput spike.</p><p>We had many deep investigations with AWS network engineers and eventually confirmed that those EC2 instances were experiencing network throttling due to <a href="https://repost.aws/knowledge-center/ec2-instance-exceeding-network-limits">microbursts</a> that exceeded the network allowance. Microbursts typically last only for seconds, milliseconds, or even microseconds. Neither standard Cloudwatch or Pinterest’s internal metrics (based on ifstat) can provide such visibility. This experience served as a wake-up call, revealing that the underlying networking systems may rigorously manage bandwidth limits to ensure fair resource allocation.</p><p>In conclusion, AWS network throttling happens at EC2 level when the network allowances are exceeded. This may occur when the network usage exceeds the baseline network bandwidth. In addition, due to microbursts, this condition of exceeding network allowance may not be obvious with conventional observability metrics.</p><h3>Network Performance Monitoring via ENA Metrics</h3><p>We aim to make EC2 network throttling behavior more transparent to developers at Pinterest. By upgrading our instances to the latest Amazon Machine Image (AMI), we can access the raw counters on an EC2 instance using tools like ethtool.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*c1c9XUM_0v6F2PZJjs3FsQ.png" /></figure><p>These counters are provided via the AWS Elastic Network Adapter (ENA) driver. For example, <strong>bw_in_allowance_exceeded</strong> shows the number of packets queued or dropped because the inbound aggregate bandwidth exceeded the maximum for the instance, and <strong>bw_out_allowance_exceeded</strong> similarly shows that for the outbound side. Refer to AWS <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-network-performance-ena.html#network-performance-metrics">documentation</a> for the up-to-date ENA stats.</p><p>With these counters available on EC2, we modify Pinterest’s internal metrics collection agent to scrape those counters on each host and ingest them into our metrics storage so that they can be queried in Pinterest’s dashboards. For example, here is a snapshot of the graphs we built based on bw_in_allowance_exceeded and bw_out_allowance_exceeded counters.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*aFAOqYyJ_cYMZa5e" /></figure><p>This shows in real-time the number of packets that are throttled (queued) due to inbound or outbound network bandwidth exceeded.</p><p>By rolling out the above ENA metrics to the entire Pinterest EC2 fleet, we are gaining unprecedented visibility into AWS traffic shaping. This insight enhances our ability to monitor network performance and guides us in implementing various optimizations to mitigate network throttling.</p><p>It’s worth noting that, when it comes to monitoring network performance, AWS recommends a multi-layered approach. While the ENA metrics provide invaluable insights, they’re most effective when used in conjunction with other AWS monitoring tools. CloudWatch metrics offer a big-picture view of network utilization trends, while VPC Flow Logs dive deeper into traffic patterns. For containerized applications, CloudWatch Container Insights can provide container-specific network data. And when real-time troubleshooting is needed, AWS Systems Manager Session Manager allows for secure, auditable instance access.</p><h3>Handling Network Bursts</h3><p>Now that we have a good visibility into how network throttling can happen due to network bursts, we’ll discuss a few techniques we explored to mitigate this issue.</p><h4>Fine-Grained S3 Rate Limiting</h4><p>For the database degradation issue, our analysis below leads us to conclude that network bursts happen during bulk data ingestion. In the database service, the application reads data (e.g. user activities) from the database continuously via online RPCs, while the database itself may periodically do a bulk ingestion of data from Amazon S3 into its local SSDs. Petabytes of data are downloaded into the database service on a daily basis. As a result, both online reads and offline bulk ingestion share the network bandwidth, and bursts in offline ingestion may cause online traffic to be throttled.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*4Rrj23ppEAXc6Wgd" /></figure><p>We leverage <a href="https://sdk.amazonaws.com/cpp/api/LATEST/root/html/index.html">AWS SDK (C++)</a> when downloading data from S3. It exposes an interface for conducting rate limiting when interacting with S3. At Pinterest, we have an in-house rate limiter implementation: it maintains a budget (number of credits) based on the configured rate (bytes per second) and the time elapsed in between requests. Upon each request, it applies the cost and calculates the remaining credits, and it sleeps to replenish the credits when the budget has been exhausted.</p><p>In the initial implementation, we had applied rate limits on a per-second level, which turned out to be prone to network microbursts as observed from our ENA metrics. After that, we reimplemented a new fine-grained rate limiter to efficiently track credits on per-<strong>millisecond</strong> buckets, leveraging <a href="https://github.com/facebook/folly/blob/main/folly/futures/ThreadWheelTimekeeper.h">folly::ThreadWheelTimekeeper</a> to overcome the stuckness issue we saw when relying on thread sleep. This significantly reduced network throttling.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*7OmYd1DPsXFTO5kZ" /></figure><h4>Data Backup Tuning</h4><p>Beside serving derived data (ML features) through bulk ingestion, the widecolumn database also supports a mutable mode to serve source-of-truth data via online writes and reads. In this scenario, it suffered from a similar network problem: the database periodically dumped data into S3 for backup purposes, which consumed significant network bandwidth. This ended up impacting online traffic.</p><p>Similarly, the fine-grained S3 rate delimiter was applied to the S3 backup client which mitigated the issue to a large extent. However, all the mutable database clusters took daily backup at roughly UTC-0, which magnified the network burst issue. To address this, we applied a few techniques:</p><ul><li>Pace the backup jobs by introducing random jitters into dataset backup jobs in the span of multiple hours. We also limited the concurrency of jobs to minimize the chance of multiple backup jobs overlapping.</li><li>Decouple the offline backup rate limits from what’s used for the online snapshot transfer during replica bootstrap, so that we can flexibly tune the offline backup speed without impacting online data rebalance speed.</li><li>Support Linux socket option <a href="https://man7.org/linux/man-pages/man8/tc-fq.8.html">SO_MAX_PACING_RATE</a> in the S3 client to ensure the socket doesn’t exceed a data transfer limit (measured in bytes per second).</li></ul><p>With above optimizations, we were able to largely smooth out the network bursts and minimize the performance impact when backups are running, cutting the p99 read latency from 100 milliseconds to less than 20 milliseconds. We also suspect that there may be link-level network congestion issues, although we haven’t completed the troubleshooting and analysis with our AWS partners yet.</p><h4>Network Compression</h4><p>Separately from the network throttling we see due to S3 download or upload jobs, we also observed significant packet throttling when serving requests with large payload (particularly larger than one megabyte). This caused tail latency (e.g. measured in p999) to increase when serving some of the ML use cases.</p><p>We worked with the clients to reduce the payload size which oftentimes involved purging unnecessary ML features. This mitigated the issues and reduced infra cost overall. In the meantime, we leveraged network compression to reduce the payload size over the wire. For example, by leveraging <a href="https://github.com/facebook/zstd">ZSTD</a> compression in our <a href="https://github.com/facebook/fbthrift">fbthrift</a> client library, we were able to reduce the network throughput by up to 60% in many cases.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*OzvL84IhabOTspzU" /></figure><p>However, there are always tradeoffs with compression and in some cases may cause noticeable CPU impact or even latency increase. This is a whole another topic and we may cover it in a future post.</p><h3>Conclusions and Takeaways</h3><p>We uncovered the AWS EC2 network throttling issues when serving critical ML workloads in our database systems at Pinterest. This became prevalent when migrating to the Nitro-based instances. Since then, we have rolled out the ENA metrics to the entire EC2 fleet at Pinterest for network performance monitoring and applied various techniques to smooth out network bursts in order to minimize throttling. In the future, we will continue to apply these techniques to more services at Pinterest for reliability and performance improvements.</p><p>To wrap up, we want to share a few high-level takeaways for the broader AWS EC2 users:</p><ul><li>Be aware of baseline and burst bandwidth of EC2 when planning your workloads, and set up alerts for network usage.</li><li>Monitor your network performance through ENA metrics with recent AMIs (2.2.10 or later).</li><li>Right-size your EC2 instance based on ENA metrics.</li><li>Minimize traffic bursts to avoid throttling. Common techniques include rate limiting, request/packet pacing, compression, etc.</li></ul><p>As we wrap up our journey through the intricacies of EC2 network performance at Pinterest, it’s clear that leveraging AWS support and documentation is crucial. Throughout our process of discovery and optimization, we found AWS Support to be an invaluable partner, offering insights that helped us navigate complex networking challenges.</p><p>Our journey highlighted the importance of proactive network performance management. Establishing baselines, implementing automated alerting based on ENA metrics, and regularly reviewing our system resource usage became cornerstone practices. We also learned to balance cost and performance carefully, considering both average and burst network requirements when sizing instances. Perhaps most importantly, we recognized that maintaining network performance is an ongoing process and our journey is far from being complete. Regular performance testing, keeping EC2 instances and AMIs current, and periodically reviewing our network architecture became part of our operational DNA.</p><h3>Acknowledgement</h3><p>We would like to thank our trusted AWS partners for their support and collaboration, and for reviewing this blog post. We also thank the following people at Pinterest who had significantly contributed to this project: Rakesh Kalidindi, Mahmoud Meariby, Rajath Prasad, Zhanyong Wan, Yutong Xie, Neil Enriquez, Ambud Sharma, Se Won Jang, Anton Arboleda.</p><p>¹Pinterest internal data; Global analysis; Q4 2024</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fda0efc21083" width="1" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/4cd938d4e892</id>
            <title>Improving Pinterest Search Relevance Using Large Language Models</title>
            <link>https://medium.com/pinterest-engineering/improving-pinterest-search-relevance-using-large-language-models-4cd938d4e892?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/4cd938d4e892</guid>
            <pubDate></pubDate>
            <updated>2025-04-04T18:49:17.698Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Han Wang | Machine Learning Engineer, Relevance &amp; Query Understanding; Mukuntha Narayanan | Machine Learning Engineer, Relevance &amp; Query Understanding; Onur Gungor | (former) Machine Learning Engineer, Relevance &amp; Query Understanding; Jinfeng Rao | Machine Learning Engineer, Pinner Discovery</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*B0rNJCYXSFt6FMxU" /><figcaption>Figure: Illustration of the search relevance system at Pinterest.</figcaption></figure><h3>Background</h3><p>Pinterest Search is one of the key surfaces on Pinterest where users can discover inspiring content that aligns with their information needs. Search relevance measures how well the search results aligned with the search query. Using a relevance objective allows the search engine to ensure that the content displayed to users is genuinely pertinent to their information needs, rather than overly relying on factors like past user engagement.</p><p>In this work, we focus on improving the search relevance model. To measure the relevance between queries and Pins, we use a 5-level guideline (see Table 1).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hI8cfbCXbrdB9osl" /><figcaption>Table 1: 5-scale Pin relevance guidelines.</figcaption></figure><p>In this blog, we will go through the technical design and share some offline and online results for our LLM-based search relevance pipeline. More details can be found in our <a href="https://arxiv.org/abs/2410.17152">full paper</a>.</p><h3>Technical Design</h3><h3>LLM as Relevance Model</h3><h4>Model Architecture</h4><p>We use a cross-encoder language model to predict a Pin’s relevance to a query, along with Pin text, as shown in Figure 1. The task is formulated as a multiclass classification problem. We fine-tune the models using human-annotated data, minimizing cross-entropy loss.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*5wV4_5lGwMOoE2--" /><figcaption>Figure 1: The cross-encoder architecture in the relevance teacher model. Take the encoder language models (e.g., <a href="https://arxiv.org/abs/1810.04805">BERT</a>-based models) for illustration.</figcaption></figure><h4>Pin Text Representations</h4><p>Pins on Pinterest are rich multimedia entities that feature images, videos, and other contents, often linked to external webpages or blogs. To represent each Pin, we use the following varied set of text features derived from metadata, the image itself, as well as user-curated data. These features are designed with a focus on providing reliable high-quality representations, while retaining high coverage across Pins on Pinterest Search.</p><ul><li><strong>Pin titles and descriptions</strong>: the titles and the descriptions assigned by the user who created the Pin.</li><li><strong>Synthetic image captions</strong>: synthetic image descriptions generated by <a href="https://arxiv.org/abs/2201.12086">Bootstrapping Language-Image Pre-training (BLIP)</a>, an off-the-shelf image captioning model.</li><li><strong>High-engagement query tokens</strong>: unique queries with the highest engagement with this Pin on the search surface over the past two years.</li><li><strong>User-curated board titles</strong>: titles of user-curated boards where the Pin has been saved.</li><li><strong>Link titles and descriptions</strong>: titles and descriptions from the linked external webpages.</li></ul><h3>Distill LLM into Servable Student Model</h3><h4>Model Architecture</h4><p>Our cross-encoder LLM-based classifier is hard to scale for Pinterest Search due to real-time latency and cost considerations. Therefore, we use knowledge distillation to distill the LLM-based teacher model into a lightweight student relevance model. The student model served online uses the following features:</p><ul><li>Query-level features: query interest features, shopping interest features, and <a href="https://arxiv.org/abs/2404.16260">SearchSAGE</a> query embeddings</li><li>Pin-level features: <a href="https://arxiv.org/abs/1806.01973">PinSAGE</a> embedding, <a href="https://arxiv.org/abs/2108.05887">visual embedding</a> for the image, and SearchSAGE Pin embeddings</li><li>Query-Pin interaction features: BM25 and text match scores for different text fields, historical engagement rates between the Pin and query, etc.</li></ul><p>These features are embedded and passed through a feed-forward network to predict 5-scale relevance scores, as shown in Figure 2.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CB-WjvTEH2fGmvXJ" /><figcaption>Figure 2: The architecture of the online-served student model, which is trained via distillation from the LLM-based teacher model.</figcaption></figure><h4>Knowledge Distillation and Semi-Supervised Learning</h4><p>To train our student relevance model, we employ the LLM-based teacher model to generate 5-scale relevance labels on a daily logged large search engagement and impression dataset with billions of rows. This labeled dataset is subsequently used to train the much smaller student model. A diagram of the search relevance system at Pinterest is shown in Figure 3. The relevance scores generated by the student model are then utilized alongside engagement predictions to determine the final ranking of search results.</p><p>This blend of knowledge distillation and semi-supervised learning not only makes effective use of vast amounts of initially unlabeled data, but also expands the data to a wide range of languages from around the world and new concepts not encountered in our human-labeled data owing to the seasonality in Pinterest Search. By using a multilingual LLM-based teacher model, we are able to successfully generalize from human-labeled data focused on US queries to unseen languages and countries.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*8f-X9PnsXjvUF0UD" /><figcaption>Figure 3: Diagram of the proposed search relevance system at Pinterest.</figcaption></figure><h3>Offline Experiments</h3><p>We now present offline experiments to demonstrate the effectiveness of each modeling decision. The teacher model is trained and evaluated using human-annotated relevance labels. In all offline experiments, we report the accuracy of 5-scale relevance predictions and the AUROC metrics for binarized labels with thresholds at 3, 4, and 5, since correctly identifying highly relevant content is more important for search ranking.</p><h4>Comparison of Language Models</h4><p>In this experiment, we evaluate the following pre-trained language models: <a href="https://huggingface.co/google-bert/bert-base-multilingual-cased">multilingual BERT-base</a>, <a href="https://huggingface.co/google-t5/t5-base">T5-base</a>, <a href="https://huggingface.co/microsoft/mdeberta-v3-base">mDeBERTa-V3-base</a>, <a href="https://huggingface.co/FacebookAI/xlm-roberta-large">XLM-RoBERTa-large</a>, and <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B">Llama-3–8B</a>. These models are initialized from Hugging Face checkpoints and fine-tuned using our in-house search relevance training data. For larger language models such as Llama, we first load quantized model weights and then apply <a href="https://arxiv.org/abs/2305.14314">qLoRA</a> for fine-tuning. Additionally, we incorporate <a href="https://arxiv.org/abs/1604.06174">gradient checkpointing</a> and mixed precision techniques to further improve training efficiency and memory usage.</p><p>Table 3 shows the performance of different language models. As a baseline, we include a model that relies solely on the <a href="https://arxiv.org/abs/2404.16260">SearchSAGE</a> embeddings. In this comparison, we keep the text features for each Pin and the maximum text length fixed, varying only the language models. The results in Table 3 clearly demonstrate that the language models offer additional improvements over our in-house content and query embedding. Furthermore, more sophisticated language models and larger model sizes consistently enhance the relevance prediction performance. Specifically, the Llama-3–8B outperforms the multilingual BERT-base model by 12.5% and the baseline model by 19.7% in terms of 5-scale accuracy.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HlhWpHYxGjD6fLho" /><figcaption>Table 3: Comparisons of different language models on 5-scale relevance prediction. The AUROC metrics are reported for binarized labels with thresholds 3, 4, and 5.</figcaption></figure><h4>Importance of Enriching Text Features</h4><p>To predict the relevance of a Pin to a query using only textual information, we enrich the Pin text representations with several carefully designed text features. We conduct an analysis to assess the impact of each text feature on relevance prediction, using mDeBERTa-V3-base as the language model and fixing the maximum text length to 256. The results, summarized in Table 4, demonstrate that the model’s performance consistently improves with the sequential addition of these text features. This indicates that enriched text features and metadata significantly contribute to building a more robust relevance model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EygKAmRO2XtljgYi" /><figcaption>Table 4: Benchmark the improvement with the sequential addition of text features.</figcaption></figure><h4>Scaling Up Training Labels through Distillation</h4><p>By using knowledge distillation and semi-supervised learning, we can effectively scale the training data beyond the limited human-annotated data. Table 5 demonstrates the benefits of training on increasing amounts of augmented teacher-generated labels.</p><p>Table 5: Comparisons of production model performance when training on different amounts of labels.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2UTwnZwQSS441Q36" /><figcaption>Table 5: Comparisons of production model performance when training on different amounts of labels.</figcaption></figure><h3>Online Results</h3><p>Besides offline experiments, we also conducted an online A/B experiment to assess the effectiveness of our new relevance model.</p><h4>Human Relevance Evaluations</h4><p>We set up evaluations with human annotators to assess the relevance of the search feeds with and without the new relevance model serving traffic. The nDCG@K here is calculated as follows (more details in <a href="https://arxiv.org/abs/2410.17152">our paper</a>):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-FEyCTWyF5T3M3JT" /></figure><p>Our proposed relevance modeling pipeline leads to a <strong>+2.18%</strong> improvement in search feed relevance, as measured by nDCG@20.</p><p>The results in Table 6 indicate that the multilingual LLM-based relevance teacher model effectively generalizes across languages not encountered during training (precision@8 used for controlling annotator costs).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*bLkyfkFCTCaweFq1" /><figcaption>Table 6: Human relevance judgements with search feeds seen across different countries demonstrate generalization across unseen languages.</figcaption></figure><h4>User Triggered Experiments</h4><p>In addition to relevance, another primary metric is the search fulfillment rate. This metric is defined as the number of search sessions that result in a high-significance user action. The improvements in relevance also result in increased fulfillment in non-US countries, despite not having annotated data available for those countries during model training (Table 7).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bvW__OYgK3zyhZUdER71Aw.png" /><figcaption>Table 7: Search Fulfillment Rate increases with the new relevance system show a significant uptick globally.</figcaption></figure><h3>Summary</h3><p>In this work, we presented an LLM-based relevance system for Pinterest Search. We thoroughly described each building block of this system, including model architecture, enriched text features, augmented label generation, and online serving. We conducted extensive offline experiments to validate the effectiveness of each modeling decision. Lastly, we presented the results from online A/B experiment, which showed an improvement of &gt;1% in search feed relevance and &gt;1.5% in search fulfillment rates.</p><h3>Future Work</h3><p>To further enhance the efficacy of our relevance system, future work will explore the integration of servable LLMs, vision-and-language multimodal models (VLMs), and active learning strategies to dynamically scale and improve the quality of the training data.</p><h3>Acknowledgement</h3><ul><li>Ads Relevance: Helen Xu, Rakesh Chalasani</li><li>Search Leadership: Krishna Kamath, Kurchi Subhra Hazra</li></ul><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4cd938d4e892" width="1" /><hr /><p><a href="https://medium.com/pinterest-engineering/improving-pinterest-search-relevance-using-large-language-models-4cd938d4e892">Improving Pinterest Search Relevance Using Large Language Models</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/0a48a0c8bfd8</id>
            <title>Building Holiday Finds: How Pinterest Engineers Reimagined Gift Discovery</title>
            <link>https://medium.com/pinterest-engineering/building-holiday-finds-how-pinterest-engineers-reimagined-gift-discovery-0a48a0c8bfd8?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/0a48a0c8bfd8</guid>
            <pubDate></pubDate>
            <updated>2025-03-26T15:28:14.392Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Megan Blake, Usha Amrutha Nookala, Jeremy Browning, Sarah Tao, AJ Oxendine, Siddarth Malreddy</p><h3>Overview &amp; Context</h3><p>The holiday shopping season presents a unique challenge: helping millions of Pinners discover and save perfect gifts across a vast sea of possibilities. While Pinterest has always been a destination for gift inspiration, our data showed that users were facing two key friction points: discovery overwhelm and fragmented wishlists. With 85% of weekly US Pinners having made a purchase based on Pins from brands¹, we saw an opportunity to create a more streamlined gift discovery experience that meets Pinners where they already are — at the early stages of shopping inspiration.</p><p><strong>The Challenge</strong></p><p>Holiday gift shopping traditionally involves juggling multiple browser tabs, wishlists, and recommendations. Pinterest users actively look for new brands and ideas, with 96% of top searches being unbranded², showing Pinners are open to discovering fresh products. The engineering challenge was clear: how could we create a unified experience that maintains the serendipity of Pinterest discovery while adding structure to the gift shopping journey?</p><p>Our solution needed to:</p><ul><li>Leverage our existing personalization infrastructure while introducing gift-specific optimizations</li><li>Create a seamless saving experience that automatically generates shoppable wishlists</li><li>Build a UI that feels fresh and distinct from our standard feed experience</li></ul><p><strong>Technical Foundation</strong></p><p>We approached this challenge by building on three core technical pillars:</p><ol><li>Personalization Stack: Rather than building a new recommendation system from scratch, we extended our proven Homefeed architecture with gift-specific candidate generators and ranking signals.</li><li>Dynamic UI Framework: We developed a new Structured Feed Framework that allows rapid iteration on UI components while maintaining platform-specific optimizations for iOS, Android, and Web.</li><li>Unified Logging System: We implemented comprehensive engagement tracking that helps us understand how users interact with gift content differently from standard Pins.</li></ol><h3>Personalization Stack</h3><p><strong>Building a Gift-Optimized Recommendation System</strong></p><p>The success of Holiday Finds hinges on our ability to surface the right gift ideas at the right time. Rather than building an entirely new recommendation system, we strategically adapted our proven Homefeed architecture through a three-stage pipeline: candidate generation, ranking, and blending.</p><p><strong>Candidate Generation:</strong> Smart Sourcing of Gift Ideas</p><p>Our first challenge was identifying “gift-worthy” content from Pinterest’s vast Pin corpus. We developed two specialized candidate generators:</p><ol><li>Proven-Shoppable Generator: This system identifies Pins with high purchase intent signals, focusing on items that historically perform well as gifts. We analyze metrics like save-to-purchase conversion and gift board additions to surface promising candidates.</li><li>Inspirational Corpus Generator: Launched post-initial release, this generator enriches recommendations with contextual gift ideas. It employs semantic filtering with a “seasonality” signal, ensuring that inspiration aligns with current gift-giving occasions.</li></ol><p><strong>Ranking:</strong> Personalized Gift Relevance</p><p>The ranking stage employs our Homefeed Pinnability model with gift-specific optimizations:</p><ul><li>Real-time <a href="https://medium.com/pinterest-engineering/how-pinterest-leverages-realtime-user-actions-in-recommendation-to-boost-homefeed-engagement-volume-165ae2e8cde8">user sequence features</a> capture immediate shopping intent</li><li>Integration with our inclusive AI systems ensures diverse gift recommendations across various styles and preferences</li></ul><p><strong>Blending:</strong> Crafting the Perfect Mix</p><p>The blending system orchestrates the final feed composition through three key mechanisms:</p><ol><li>Interest-Based Diversification: Our Determinantal Point Process (DPP) algorithm prevents recommendation tunneling by applying interest-penalties to similar items.</li><li>Gift-Specific Filtering: A post-ranking filter removes utilitarian products while elevating items with strong gift signals.</li><li>Module Integration: Specialized shopping modules are interwoven throughout the feed, providing structured entry points for different gift categories.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*8ekCZisHgThnDwco" /><figcaption>Figure 1: Holiday Finds Backend Design</figcaption></figure><h3>Autogenerated Wishlist Boards</h3><p><strong>Smart Wishlist Generation</strong></p><p>A key innovation of Holiday Finds was its ability to automatically create and populate shopping-focused boards. Rather than requiring users to manually create and organize boards, we developed a system that reduces friction in the gift discovery journey.</p><p><strong>Technical Implementation</strong></p><p>The wishlist generation system operates through several coordinated components:</p><ol><li>Trigger Detection</li></ol><ul><li>Monitors first save action from Gift Guide surfaces</li><li>Validates user eligibility (no existing wishlist for current campaign)</li><li>Handles edge cases like archived or soft-deleted boards</li></ul><p>2. Board Creation Pipeline</p><ul><li>Asynchronously creates new board with “Wishlist” designation</li><li>Maintains internal flags to track wishlist status regardless of user renaming</li><li>Implements soft deletion and archive states that pause automatic saving</li></ul><p>3. Quick Save Integration</p><ul><li>Introduces Pin Icon Grid Save (PIGS) for one-tap saving</li><li>Maintains save state persistence within Gift Guide sessions</li><li>Handles multi-board scenarios and undo functionality</li></ul><p><strong>Performance Impact</strong></p><p>The system has demonstrated promising results aligned with Pinterest’s shopping-oriented audience:</p><ul><li>Generated significant wishlist adoption, helping Pinners organize their gift ideas</li><li>Streamlined the saving experience for holiday shoppers</li><li>Drove higher engagement, reflecting Pinterest’s strength as a platform where 80% of weekly Pinners say they feel inspired by the shopping experience³</li></ul><p><strong>Edge Case Handling</strong></p><p>We implemented robust handling for several complex scenarios:</p><ul><li>Board deletion/restoration flows</li><li>Collaborative board permissions</li><li>Name collision resolution</li><li>Archive state management</li><li>Cross-platform state synchronization</li></ul><p>This infrastructure provides a foundation for future automatic organization features beyond holiday shopping, demonstrating how we can reduce friction in content organization while maintaining user control and expectations.</p><h3>UI Implementation</h3><p><strong>Crafting a Distinctive Shopping Experience</strong></p><p>The Holiday Finds interface needed to feel special while maintaining Pinterest’s familiar comfort. We developed several key innovations to achieve this balance:</p><p><strong>Dynamic Hero Header</strong></p><p>We built a new dynamic header system that creates an immersive entry point to gift discovery:</p><ul><li>Adaptive Imagery: The header automatically selects and blurs high-performing Pins from the user’s feed, creating a cohesive visual story that updates with each refresh.</li><li>Platform-Specific Optimization: We implemented distinct module structures for mobile and web, with iOS/Android utilizing a dual-module system while web maintains a unified approach.</li><li>Smooth Transitions: We engineered careful handling of status bar interactions and scroll behaviors to ensure the experience feels native on each platform.</li></ul><p><strong>Structured Feed Framework</strong></p><p>To support rapid iteration and maintain consistency across platforms, we developed a new Structured Feed Framework. Key features include:</p><ul><li>Configurable Module Headers: Easy definition and modification of module properties without code changes</li><li>Cross-Platform Compatibility: Automatic handling of platform-specific UI requirements</li><li>Performance Optimization: Smart image loading and caching strategies to maintain smooth scrolling</li></ul><p>The framework’s flexibility proved invaluable during development, allowing us to:</p><ul><li>Quickly test different header configurations</li><li>Adjust module layouts based on user feedback</li><li>Deploy platform-specific optimizations without duplicating code</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fBDFab2sab2FqBX0" /><figcaption>Holiday Finds on Web</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*frv-uYmdKyC4bLFv" /><figcaption>Figure 2: Holiday Finds Recommendation Landing Page</figcaption></figure><h3>Tab Ranking System</h3><p><strong>Solving the Cold Start Problem with Dynamic Tab Ranking</strong></p><p>One of our key challenges was introducing a new shopping surface while maintaining the integrity of Pinterest’s navigation system. We developed a two-phase tab ranking strategy that balances discovery with personalization.</p><p>Bootstrap Phase</p><p>To ensure users could discover Holiday Finds, we implemented a fixed-position strategy:</p><ul><li>Three-day bootstrap period with Holiday Finds locked to position 1 (immediately after “All”)</li><li>Existing Board More Ideas tabs maintain their engagement-based ranking</li><li>User behavior tracking begins immediately to inform future positioning</li></ul><p>This approach gave us two key advantages:</p><ul><li>Guaranteed visibility during the critical discovery period</li><li>Clean data collection for long-term ranking optimization</li></ul><p>Engagement-Based Dynamic Ranking</p><p>After the bootstrap period, we transition to a dynamic ranking system that considers multiple engagement signals:</p><p>- Pin saves to boards</p><p>- Direct Pin clicks</p><p>- Social interactions (likes, comments)</p><p>- Feed engagement (10+ impressions)</p><p>The ranking system updates daily through a batch process, helping us balance computational efficiency with responsiveness. To avoid latency issues, we built a smart caching system that only updates tab positions when the bootstrap period has conclusively ended.</p><h3>Logging &amp; Analytics</h3><p><strong>Building a Data Foundation for Shopping Innovation</strong></p><p>To measure success and enable continuous improvement, we implemented a comprehensive logging system that extends our existing Homefeed analytics infrastructure.</p><p><strong>Unified Event Tracking</strong></p><p>We designed our logging system to capture the unique aspects of gift discovery behavior:</p><ul><li>Feed-level metrics: unique feed IDs track content distribution and engagement patterns</li><li>Module-specific interactions: granular tracking of how users engage with different shopping modules</li><li>Tab transition analysis: understanding how users move between Holiday Finds and other Pinterest surfaces</li></ul><p><strong>Implementation Strategy</strong></p><p>Rather than building a separate logging system, we extended Homefeed’s Local Navigation logging capabilities:</p><ul><li>Reused existing tab action tracking infrastructure</li><li>Added gift-specific event types and attributes</li><li>Implemented validation queries to ensure data quality</li></ul><p>This approach allowed us to:</p><ul><li>Launch quickly with proven infrastructure</li><li>Maintain consistency with existing analytics</li><li>Enable seamless cross-surface analysis</li></ul><h3>Future Work</h3><p><strong>Impact and Looking Ahead</strong></p><p><strong>Results</strong></p><p>The Holiday Finds launch has demonstrated strong initial success:</p><ul><li>Improved engagement with wishlists compared to standard boards</li><li>Strong adoption among Gen Z users, our fastest growing audience³ that makes up 42% of our global user base³</li><li>Successful integration of curated gift guides globally, helping Pinners connect with brands at their first moment of inspiration</li></ul><p><strong>Technical Learnings</strong></p><p>Several key insights emerged from this project:</p><ul><li>The Structured Feed Framework proved highly adaptable, supporting rapid iteration across platforms</li><li>Our two-phase tab ranking strategy effectively solved the cold start problem while maintaining personalization</li><li>The reuse of Homefeed infrastructure significantly accelerated development while maintaining reliability</li></ul><p><strong>Future Directions</strong></p><p>We’re excited to build on this foundation in several ways:</p><ol><li>Tab Ranking Evolution</li></ol><ul><li>Exploring different ranking models for specific tab types</li><li>Investigating real-time ranking updates for increased responsiveness</li><li>Testing new engagement signals for better personalization</li></ul><p>2. Shopping Experience Enhancement</p><ul><li>Expanding the dynamic header system to other Pinterest surfaces</li><li>Developing new shopping-specific modules</li><li>Further optimizing the gift discovery algorithm</li></ul><p>3. Platform Innovation</p><ul><li>Extending the Structured Feed Framework to support more complex interactions</li><li>Building new tools for rapid shopping surface development</li><li>Improving cross-surface content discovery</li></ul><h3>Acknowledgement</h3><p>Kurby Gebremedhin, Samantha Lee, Jesse Andersen, Kirsten Browne, Jiaqi Tong, Ianyu Feng, Tian Li, Josh Arriola, Elizabeth Cheng, Jazz Ernest, Emma Li, Zihao Chen, J.J Hu, Raymond Hsu</p><p>¹ <a href="https://business.pinterest.com/en-ca/getting-started/">https://business.pinterest.com/en-ca/getting-started/</a><br />² <a href="https://business.pinterest.com/en-ca/audience/">https://business.pinterest.com/en-ca/audience/</a><br />³ <a href="https://business.pinterest.com/audience/">https://business.pinterest.com/audience/</a></p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0a48a0c8bfd8" width="1" /><hr /><p><a href="https://medium.com/pinterest-engineering/building-holiday-finds-how-pinterest-engineers-reimagined-gift-discovery-0a48a0c8bfd8">Building Holiday Finds: How Pinterest Engineers Reimagined Gift Discovery</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/ae76f8b545b2</id>
            <title>Module Relevance on Home Feed</title>
            <link>https://medium.com/pinterest-engineering/module-relevance-on-homefeed-ae76f8b545b2?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/ae76f8b545b2</guid>
            <pubDate></pubDate>
            <updated>2025-04-04T23:44:25.965Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Usha Amrutha Nookala, Jason Madeano, Siddarth Malreddy, Lucy Song, Alekhya P</p><p>In the past, home feed on Pinterest recommended a grid of Pins that are most relevant to a user. The grid limits our ability to provide more context on the recommendations as well as show new topics the user might be interested in. To address this, we introduced modules to the home feed. We developed several components to ensure that the user’s home feed remains relevant with this new content type. This blog post details these components. The figure below provides an overview of our system and demonstrates how it interacts with the existing main feed of Pins.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*1tcT1pb1H7nuAi-9" /><figcaption>Figure 1: Overview of the module relevance backend</figcaption></figure><h3>Modules</h3><p>Modules are a new content format that gives Pinners more context about their feed content and more ways to explore topics. Modules are heterogenous, with variations in UI and content based on factors like topics, boards, recent engagement, and shopping interests. We broadly have two types of modules:</p><ul><li><strong>Landing page modules:</strong> In this type of modules, tapping on the module takes the user to a separate landing page with Pins the user can interact with.</li><li><strong>Carousel modules:</strong> This type of module is a collection of Pins in the horizontal shelf on the feed that are shown to the user.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/400/0*vqKGzhDpKFiZzMCW" /><figcaption>Figure 2: Illustration of modules on home feed</figcaption></figure><p>Modules on home feed (HF) can give Pinners greater control of their browsing and more context about their recommendations, but new modules need to compete with the highly optimized HF grid. The HF Module Relevance Platform aims to dynamically blend modules and Pins to better optimize each Pinner’s HF experience. In the subsequent sections, we touch upon important components of Module Relevance Platform.</p><h3>Module Fatiguing</h3><p>When adding modules to the feed, the new content displaces the existing Pins. This made it hard to launch new modules to the platform without taking a hit on core company metrics. We were able to resolve this by introducing module fatiguing.</p><p>Fatiguing is underpinned by a simple idea: If a particular user has seen a particular module many times without interacting with it, we should (at least temporarily) stop showing that module to that user. We achieved this by running a daily workflow that aggregates each user’s historical module engagement with a focus on the number of impressions since the user’s last click on the module. If, over the past <em>n</em> days, the number of impressions without a click was above some threshold <em>k</em>, then that module would be hidden for that user for <em>d</em> days.</p><p>While this heuristic-based fatiguing approach is simple in theory, it was very effective in practice and directly enabled the launch of the first few modules on home feed by enabling personalization before any other relevance components were introduced.</p><p>However, as the module ecosystem grew, fatiguing was not sufficient for three main reasons.</p><ol><li>Fatiguing decides <em>whether or not</em> to show a module but does not inform <em>how</em> or <em>where</em> to show a module within a feed.</li><li>Simple, heuristic-based fatiguing is not sufficiently granular as it only works at the module-type level. For example, for a particular shopping module type that displays shoppable clothing content, the user may like it when the module shows shoes but not when it shows hats.</li><li>The fatiguing signal is not real-time, meaning that it was not sufficiently responsive.</li></ol><p>As we continued building the module ecosystem, we invested in better signals to help with point number 3, but the first two points required building entirely new relevance components such as a module ranking and module blending.</p><h3>Module Ranking</h3><p>The module ranker deals with ranking modules amongst each other so the most relevant modules are shown higher on the page. We started with a heuristic-based approach while leveraging existing components before building a dedicated ranking model.</p><h3>Baseline Approach</h3><p>We used home feed’s Pin Ranking model for ranking modules. For each module, we take the first four cover Pins and score them with the production Pin Ranking model. The module score is computed as the maximum of the calibrated save scores across those four cover Pins. We rank the modules based on this score. Then we filter “low quality” modules. By default, we filter modules with scores lower than <em>x</em> percentile, but some modules have their own filtering thresholds that overrides this default.</p><h3>Deep Ranking Model</h3><p>The off-the-shelf Pin Ranking model is already a large and complex model, and it serves a very important role in HF pin ranking. This makes it very difficult to make adjustments directly to the HF Pin ranker to improve module ranking performance. Furthermore, the Pin Ranker does not take into consideration the user-module engagement, thereby making it difficult to personalize the module distribution to Pinners.</p><p>So, we built a dedicated module ranking model that is trained on user engagement data on modules. Whereas the pin ranking-based scoring approach only considers module cover Pins, the dedicated ranker has a much more comprehensive feature set of modules and users. This also enables a feedback loop between user and module engagement and improves the module distribution.</p><p>The Module Ranker is a multi-head model optimizing for module actions such as module taps, Pin clicks on the carousel/landing page, and saves on the carousel/landing page. Those positive actions are mapped to binary labels, and we use binary cross entropy loss for optimization during the training process.</p><h4>Sampling Strategy</h4><p>We retain all positive labels leading to module engagement and impressions are downsampled in order to maintain a balance between positive and negative labels.</p><h4>Selected Features</h4><p>Module Features</p><ul><li>Aggregated (both overall + per user) features such as taps, impressions, repins, closeup, and click/long click across one, seven, and 28 day windows per module type</li><li>Module Title embedding(s), Module Type, UI type</li><li>Cover images Pin Ranker Predictions</li><li>Various pretrained embeddings widely used in Pinterest — encoding information related to visual information, shopping intent, Pin representations, and other onsite signals — of the first four cover Pins of the Module</li></ul><p>User Features</p><ul><li>Viewer engagement features</li><li>Viewer characteristics (such as gender, language, user state) and Context features</li></ul><h4>Calibration</h4><p>Since we downsample impressions, we need a calibration layer to map the predicted scores to empirical rates. The calibration training window with dates/data are distinct from normal model training. The calibrated scores are explainable and are further used in the blending layer to dynamically place modules on HF.</p><h4>Utility</h4><p>We combined the predictions from these multiple heads to get a single score used to rank the modules. Given business insights about the relative importance of each action along with offline and online tuning, we determine a utility value that is a weighted sum of all prediction scores from the module ranker.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IRcVMEC_r9Nt6muy" /></figure><p>Once we have the scores for each module from the module ranker, Module Blending decides how we dynamically blend modules and Pins together to generate the feed.</p><h3>Module Blending</h3><p>Our initial approach placed ordered modules in static fixed slots within the feed (such as 15, 35, 55, 75), regardless of the surrounding Pins. This could cannibalize home feed user engagement by potentially replacing a highly engaging Pin with a Module. To address this, our ‘skip slot’ approach dynamically blends Pins and Modules, ensuring a Module only replaces a Pin if its predicted engagement is higher.</p><h3>Skip-slot Blending</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*YP0ZEmNVUTfFAc6K" /><figcaption>Figure 3: Comparison between no blending and skip slot blending</figcaption></figure><p>Skip-slot Blending aims to dynamically place modules in the feed based on the predicted engagement of the surrounding Pins. To achieve this, we computed an Pin Utility score for each Pin using the outputs of the Pin ranker:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*W6T_8FwRtQjFt_xw" /></figure><h4>Comparison Function</h4><p>Our comparison function simply checked for the greater of the two, along with a weighting coefficient.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ZBmFW4XD6FQA6nqC" /></figure><p>In theory, w<em>ₚ</em> could be distributed to the other weight terms to reduce the number of weights, but the weight is useful for tuning in practice (i.e. it determines how much preference to give modules versus pins). The weight is also useful if the number of terms on each side of the equation and/or the score distributions between the module ranker and the Pin ranker are not sufficiently similar.</p><p>Modules are blended in the feed based on the predicted engagement of surrounding Pins. We order the modules based on their Module Utility score from the module ranker. Before placing a module at slot <em>s</em>, we check whether the Module Utility score &gt; weight_coefficient times the Pin Utility score of the Pin at that slot. If the module score is higher, we place the module. Otherwise, we repeat the check k slots lower. We repeat this process until all modules have been placed or there are no more slots remaining. This naturally allows us to move modules with lower scores further down in the feed (or remove them entirely).</p><h3>Future Plans</h3><p>As we rolled out these systems in production, we developed a broad set of real-time monitoring metrics to monitor the health of the Module Platform. We plan to iterate on these components to further improve relevance by using magnitude-based ranking/blending and Pin displacement based blending.</p><h3>Acknowledgement</h3><p>We would like to thank the home feed relevance and product teams for their support: Lianghao Chen, Matt Chun, Felix Zhou, Xinyuan Gui, Dhruvil Deven Badani, Jesse Andersen, AJ Oxendine, Yujiao Guo, Sarah Tao, Megan Blake, Jennifer Kong, Hanfei Ren, Minely Pereyra Rodriguez, Moises Antonio Hernandez Lopez, Jonatan Luna, Dylan Wang</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ae76f8b545b2" width="1" /><hr /><p><a href="https://medium.com/pinterest-engineering/module-relevance-on-homefeed-ae76f8b545b2">Module Relevance on Home Feed</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/3670363c467d</id>
            <title>Infrastructure Advancements at AWS ReInvent 2024</title>
            <link>https://medium.com/pinterest-engineering/infrastructure-advancements-at-aws-reinvent-2024-3670363c467d?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/3670363c467d</guid>
            <pubDate></pubDate>
            <updated>2025-02-19T21:57:29.241Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Madhuri Racherla, VP, Engineering — Infra &amp; SRE</p><p>At the recent AWS ReInvent 2024 conference, Madhuri Racherla, VP of Infrastructure &amp; SRE, shared an inspiring story of the company’s ongoing journey to optimize performance and reduce costs. Her talk not only highlighted Pinterest’s strategic infrastructure advancements but also aimed to motivate engineers to pursue innovation and excellence.</p><p><strong>A Vision of Boundless Creativity</strong></p><p>Madhuri kicked off her presentation by painting a picture of a world where creativity knows no bounds. With Pinterest’s mission to inspire over 553 million monthly active users* to create lives they love, the platform isn’t just a hub for ideas; it’s a movement celebrating creativity and discovery.</p><p><strong>Empowering Infrastructure on AWS</strong></p><p>Madhuri spotlighted the powerful infrastructure underpinning Pinterest’s operations, which seamlessly serves a growing user base. Utilizing tens of thousands of EC2 instances and storing about an exabyte in Amazon S3, Pinterest ensures all user content is secure and accessible. This robust setup fosters rapid feature rollout and reliability.</p><p>To keep pace with Pinterest’s growth, infrastructure strategies have been refined for flexibility, reliability, and cost-effectiveness. Through strategic moves, Pinterest ensures scalability and keeps delivering top-notch user experiences.</p><p><strong>The AMD M7a Integration</strong></p><p>Pinterest’s adoption of AMD M7a instances with EPYC Genoa CPUs represents a significant leap in optimizing infrastructure efficiency. These instances strike an ideal balance with a 1:4 vCPU to memory ratio and physical x86 cores, enhancing compute-bound microservices performance. By shifting from c5.9xl to m7a.4xl instances, Pinterest leveraged improved hardware to align resources precisely with actual needs.</p><p>This transition not only increased performance but also yielded significant cost savings — up to 40% in some cases, and 20% over Graviton 3. The M7a instances epitomize Pinterest’s commitment to continual infrastructure innovation and scalability, demonstrating that efficient right-sizing translates into substantial operational value.</p><p><strong>Migration Success and Strategy</strong></p><p>Madhuri detailed the strategic steps taken to migrate, including workload analysis, instance selection, and comprehensive testing. The migration was completed swiftly, showing consistent CPU usage and performance, with no impact on service level objectives.</p><p>These enhancements highlight Pinterest’s dedication to boosting both performance and cost-efficiency, aligning infrastructure with current and future needs. By making strategic decisions today, Pinterest lays the groundwork for greater possibilities tomorrow.</p><p><strong>Conclusion: A Call to Action</strong></p><p>Madhuri’s presentation at AWS ReInvent was more than an update on Pinterest’s technical accomplishments — it was a call to push boundaries and make thoughtful, strategic choices. Her message aims to inspire all engineers in tech, to seize opportunities for innovation, reminding us that each smart decision today sets the stage for future successes.</p><blockquote>*Pinterest internal data; Global analysis; Q3 2024</blockquote><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3670363c467d" width="1" /><hr /><p><a href="https://medium.com/pinterest-engineering/infrastructure-advancements-at-aws-reinvent-2024-3670363c467d">Infrastructure Advancements at AWS ReInvent 2024</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/8ab12ae97cda</id>
            <title>The Quest to Understand Metric Movements</title>
            <link>https://medium.com/pinterest-engineering/the-quest-to-understand-metric-movements-8ab12ae97cda?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/8ab12ae97cda</guid>
            <pubDate></pubDate>
            <updated>2025-02-11T18:19:26.591Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Charles Wu, Software Engineer | Isabel Tallam, Software Engineer | Franklin Shiao, Software Engineer | Kapil Bajaj, Engineering Manager</p><h3>Overview</h3><p>Suppose you just saw an interesting rise or drop in one of your key metrics. Why did that happen? It’s an easy question to ask, but much harder to answer.</p><p>One of the key difficulties in finding root causes for metric movements is that these causes can come in all shapes and sizes. For example, if your metric dashboard shows users experiencing higher latency as they scroll through their home feed, then that could be caused by anything from an OS upgrade, a logging or data pipeline error, an unusually large increase in user traffic, a code change landed recently, etc. The possible reasons go on and on.</p><p>At Pinterest, we have built different quantitative models to understand why metrics move the way they do. This blog outlines the three pragmatic approaches that form the basis of the root-cause analysis (RCA) platform at Pinterest. As you will see, all three approaches try to narrow down the search space for root causes in different ways.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*TCLUiYtQlxVEOPF0" /><figcaption><em>Figure 1: Narrowing down the search space for root causes.</em></figcaption></figure><h3>Slice and Dice</h3><p>This approach finds clues for a metric movement by drilling down on specific segments within the metric; it has found successes at Pinterest, especially in diagnosing video metric regressions.</p><p>For example, suppose we are monitoring video view rate (i.e., number of views over impressions). At Pinterest, a metric like video view rate is <em>multidimensional</em>: it has many dimensions like country, device type, Pin type, surface, streaming type, etc., that specify which subset of users the metric is describing. Using the different dimensions, we can break down the top-line metric into finer metric segments, each segment corresponding to a combination of dimension values. We are interested in identifying the most <em>significant </em>segments that have either significantly contributed to a top-line metric movement or have exhibited very unusual movements themselves not reflected in the top-line.</p><p>How we are analyzing the metric segments takes inspiration from the <a href="https://www.linkedin.com/blog/engineering/analytics/analyzing-anomalies-with-thirdeye">algorithm in Linkedin’s ThirdEye</a>. We organize the different metric segments into a tree structure, ordered by the dimensions we are using to segmentize the metric. Each node in the tree corresponds to a possible metric segment.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hHZkdbUwu_Oha4c2" /><figcaption><em>Figure 2: Example of a segment tree.</em></figcaption></figure><p>Depending on your use-case, you could then define your own <em>heuristics</em> in terms of the different <em>factors</em> that determine the significance of a metric segment, in the context of its parent segment and/or the top-line metric. You could then synthesize the factors into an overall significance score.</p><p>The LinkedIn blog already listed several factors that we found useful, including how many data points a metric segment represents, as well as how “unexpected” the metric segment’s movement is between what is observed and what is expected, especially compared to its parent segment in the tree.</p><p>Here are some additional suggestions based on our experience that you could try:</p><ul><li>Try tweaking how the factors are calculated; e.g., for each metric segment, what are the “observed” and “expected” values? Are they values taken at two discrete points in time or averages/percentiles of data from two time windows (i.e., one baseline window and one window in which the anomalous top-line metric movement happened)? Similarly, the metric segment size factor could also be aggregated from a time window.</li><li>Add new factors that make sense for your use-case; e.g., a factor like how well a metric segment correlates with the parent segment / top-line metric in the time window of interest.</li><li>Adjust the weights of the different factors over time based on continued evaluations.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*1RUQ_zGfPYohsrIW" /><figcaption><em>Figure 3: Analyzing a metric segment.</em></figcaption></figure><p>Note that for each metric segment (i.e. each node in the tree) you need to select enough data to calculate all the factors. A lot of OLAP databases support SQL features (e.g., <em>GROUP BY ROLLUP</em>) that can get the data for all metric segments. Once the segment tree is constructed, you can also choose to drill down starting from any metric segment as the top-line.</p><p>Lastly, note that the tree structure implies an order or hierarchy in the dimensions we are slicing each time. While some dimensions can indeed relate to one another in clear hierarchical order (e.g., dimensions <em>country</em> and <em>state</em>), others cannot (e.g., dimensions <em>country</em> and <em>device type</em>). Look at it this way: if this drill-down investigation were manual, the investigator would still have to choose an order of dimensions to slice along each time, from context or experience. The hierarchy in the tree structure captures that.</p><h3>General Similarity</h3><p>In this approach, we look for clues of why a metric movement happened by scanning through other metrics and finding ones that have moved very “similarly” in the same time period, whether in the same direction (positive association) or in the opposite direction (negative association).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Xl_H2IdSmcuzokZQ" /><figcaption><em>Figure 4: Positive and negative associations between metrics.</em></figcaption></figure><p>To measure the similarity of metric movements, we use a synthesis of four different factors:</p><ul><li><a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson correlation</a>: measures the strength of the linear relationship between two time-series</li><li><a href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient">Spearman’s rank correlation</a>: measures the strength of the monotonic relationship (not just linear) between two time-series; in some cases, this is more robust than Pearson’s correlation</li><li>Euclidean similarity: outputs a similarity measure based on inversing the <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance </a>between the two (standardized) time-series at each time point</li><li><a href="https://en.wikipedia.org/wiki/Dynamic_time_warping">Dynamic time warping</a>: while the above three factors measure similarities between two time-series in time windows of the same length (usually the same time window), this supports comparing metrics from time windows of different lengths based on the distance along the path that the two time-series best align</li></ul><p>In practice, we have found that the first two factors, Pearson and Spearman’s rank correlations, work best because:</p><ul><li>p-values can be computed for both, which help to gauge statistical significance</li><li>both have more natural support for measuring negative associations between two time-series</li><li>non-monotonic (e.g. quadratic) relationships, for which Pearson and Spearman’s rank correlations won’t apply, don’t tend to arise naturally so far in our use-cases / time window of analysis</li></ul><p>At Pinterest, one of the notable uses for this RCA functionality has been to discover the relationship between performance metrics and content distribution. Some types of Pins are more “expensive” to display, resource wise, than others (e.g., video Pins are more expensive than static image Pins), so could it be that the latency users experienced has increased because they saw more expensive Pins and less inexpensive ones as they scroll through their home feed or search feed? RCA has provided the initial statistical signals that performance regressions and content shifts could indeed be linked, motivating further investigations to estimate the exact causal effects.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*zFwDHeXQnoO7oOjA" /><figcaption><em>Figure 5: Content shifts and latency.</em></figcaption></figure><p>It’s important to keep in mind that this RCA approach is based on analyzing correlations and distances, which do not imply causation. The stronger statistical evidence for causation is of course established through experiments, which we will turn our attention to next.</p><h3>Experiment Effects</h3><p>This third approach looks for clues of why metric movements happened by looking at what a lot of internet companies have: experiments.</p><p>An experiment performs A/B testing to estimate the effect of a new feature. In an experiment, a portion of the users are randomly assigned to either a control or a treatment group, and the ones in the treatment group experience a new feature (e.g., a new recommendation algorithm). The experimenter sees if there is a statistically significant difference in some key metrics (e.g., increased user engagement) between the control and the treatment group.</p><p>In RCA, we perform the above in reverse: given a metric, we want to see which experiments have shifted that metric the most, whether <em>intended or not</em>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/922/0*DdYLD2xxUi7qiWJp" /><figcaption><em>Figure 6: RCA and experiments.</em></figcaption></figure><p>Each user request to RCA specifies the metric, segment, and time window the user is interested in. Then, RCA calculates each experiment’s impact on the metric segment over the course of that time window and ranks the top experiments by impact. The RCA calculation and ranking are carried out dynamically per user request and are <em>not </em>part of a pre-computation pipeline (although the process may rely on some pre-aggregated data); this supports analyzing the impacts for a maximum amount of metrics, often on an ad-hoc basis, without resulting in a systematic increase in computation or storage cost.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*mPbcZ16D_OlHuxy3" /><figcaption><em>Figure 7: RCA Experiment Effects workflow.</em></figcaption></figure><p>For each control and treatment group in an experiment, we perform a <a href="https://en.wikipedia.org/wiki/Welch%27s_t-test">Welch’s t-test</a> on the treatment effect, which is robust in the sense that it supports unequal variances between control and treatment groups. To further combat noise in the results, we filter experiments by each experiment’s <a href="https://en.wikipedia.org/wiki/Harmonic_mean_p-value">harmonic mean p-value</a> of its treatment effects over each day in the given time period, which helps <a href="https://www.pnas.org/doi/10.1073/pnas.1814092116">limit false positive rates</a>. We also detect imbalances in control and treatment group sizes (i.e., when they are being ramped up at a different rate from each other) and filter out cases when that happens.</p><p>We have integrated RCA Experiment Effects with the experimentation platform at Pinterest. With extensive application-level caching, as well as some query optimizations, we are able to have RCA dynamically find the top experiments affecting all metrics covered by the experimentation platform — close to 2000 of them at the time of writing, including a variety of system, user engagement, and trust and safety metrics.</p><h3>Using It All Together</h3><p>All three RCA services could be used together iteratively, as illustrated below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*O1b7oK8HrnPoQu-Q" /><figcaption><em>Figure 8: Using all RCA services together.</em></figcaption></figure><h3>Next Steps</h3><p>What is presented here are just three approaches to narrowing down the search space of root-causes of metric movements. There are other ways of doing this, which we will explore and add as demands arise.</p><p>For analytics tools like anomaly detection or root-cause analysis, the results are often mere suggestions for users who may not have a clear idea of the algorithms involved or how to tune them. Therefore, it would be nice to have an effective feedback mechanism in which the users could label the results as helpful or not, and that feedback is automatically taken into account by the algorithm going forward.</p><p>Another potential area of improvement that we are looking into is leveraging <a href="https://en.wikipedia.org/wiki/Exploratory_causal_analysis">causal discovery</a> to learn the causal relationships between different metrics. This would hopefully provide richer statistical evidence for causality with less noise, compared to the current RCA General Similarity.</p><p>As we improve the RCA services’ algorithms, we would also like to integrate them with more data platforms within Pinterest and make RCA readily accessible through the platforms’ respective web UIs. For example, we are exploring integrating RCA into the data exploration and visualization platforms at Pinterest.</p><h3>Acknowledgments</h3><p>We are incredibly grateful to the engineers and data scientists at Pinterest, who have been enthusiastic in trying and adopting the different RCA services and offering their valuable feedback.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8ab12ae97cda" width="1" /><hr /><p><a href="https://medium.com/pinterest-engineering/the-quest-to-understand-metric-movements-8ab12ae97cda">The Quest to Understand Metric Movements</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/d7d7971a409e</id>
            <title>Advancements in Embedding-Based Retrieval at Pinterest Homefeed</title>
            <link>https://medium.com/pinterest-engineering/advancements-in-embedding-based-retrieval-at-pinterest-homefeed-d7d7971a409e?source=rss-ef81ef829bcb------2</link>
            <guid isPermaLink="false">https://medium.com/p/d7d7971a409e</guid>
            <pubDate></pubDate>
            <updated>2025-02-03T18:50:34.500Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Zhibo Fan | Machine Learning Engineer, Homefeed Candidate Generation; Bowen Deng | Machine Learning Engineer, Homefeed Candidate Generation; Hedi Xia | Machine Learning Engineer, Homefeed Candidate Generation; Yuke Yan | Machine Learning Engineer, Homefeed Candidate Generation; Hongtao Lin | Machine Learning Engineer, ATG Applied Science; Haoyu Chen | Machine Learning Engineer, ATG Applied Science; Dafang He | Machine Learning Engineer, Homefeed Relevance; Jay Adams | Principal Engineer, Pinner Curation &amp; Growth; Raymond Hsu | Engineering Manager, Homefeed CG Product Enablement; James Li | Engineering Manager, Homefeed Candidate Generation; Dylan Wang | Engineering Manager, Homefeed Relevance</p><h3>Introduction</h3><p>At Pinterest Homefeed, embedding-based retrieval (a.k.a Learned Retrieval) is a key candidate generator to retrieve highly personalized, engaging, and diverse content to fulfill various user intents and enable multiple actionability, such as Pin saving and shopping. We have introduced the establishment of this two-tower model with its modeling basics and serving details. In this blog, we will focus on the improvements we made on embedding-based retrieval: how we scale up with advanced feature crossing and ID embeddings, upgrading the serving corpus, and our current journey to machine learning based retrieval revolution with state-of-the-art modeling.</p><h3>Feature Crossing</h3><p>We have various features provided to the model in the hope that it can reveal the latent pattern for user engagements, ranging from pretrained embedding features to categorical or numerical features. All these features are converted to dense representations through embedding or Multi-layer perceptron (MLP) layers. A prior knowledge of recommendation tasks is that incorporating more feature crossing may benefit model performance. For example, knowing the combination of movie author and genre provides more context than having these features alone.</p><p>The common philosophy for two-tower models is modeling simplicity; however, it’s more about having no user-item feature interaction and using simple similarity metrics like dot-product. Because the Pin tower is used offline and the user tower is only fetched once during a homefeed request, we can scale up to a complicated model structure within each tower. All the following structures are applied to both towers.</p><p>Our first attempt is to upgrade the model with MaskNet[1] for bitwise feature crossing. This process is different from the original paper: after embedding layer normalization and concatenation, our MaskNet block is implemented as the Hadamard product of the input embedding and a projection of itself via a two-layer MLP, followed by another two-layer MLP to refine the representation. We parallelize four such blocks with a bottleneck-style MLP. This setup simplifies the model architecture and brings high learnability with extensive feature crossing inside each tower. At Pinterest Homefeed, we use engaged sessions to measure the impact of recommendation system iterations, which are continuous interaction sessions larger than 60 seconds. This model architecture upgrade improved 0.15–0.35% engaged sessions across Pinterest.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/908/0*QYI--K2XzfrSxyhi" /><figcaption>Figure 1. Two-Tower Model with Parallel Mask Net[1] as Feature Crossing</figcaption></figure><p>We further upscale the architecture to the DHEN[2] framework, which ensembles multiple different feature crossing layers in both serial and parallel ways. We juxtapose an MLP layer with the same parallel mask net and append another layer of juxtaposition of an MLP and a transformer encoder[3]. This appended layer enhances field-wise interaction since the attention is applied at field level, while the dot-product based feature crossing is at bit level for MaskNet. This scaling up brings another +0.1–0.2% engaged sessions, together with &gt;1% homefeed saves and clicks.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*M68PoAXUYDENT3Cm" /><figcaption>Figure 2. Model Scaling with DHEN[2] Framework and Transformers[3] for Field-wise Crossing</figcaption></figure><h3>Adopting Pre-trained ID Embeddings</h3><p>Industry recommendation shows the benefit of having ID embeddings by memorizing user engagement patterns. At Pinterest, to overcome the well-known ID embedding overfitting issue and maximize ROI and flexibility in downstream ML models, we pre-train large-scale user and Pin ID embeddings by contrastive learning on sampled negatives over a cross-surface large window dataset with no positive engagement downsampling [7]. It brings great ID coverage and rich semantics tailored for recommendations at Pinterest. We adopt this large ID embedding table in the retrieval model to enhance the precision. At training time, we use the recently released <a href="https://pytorch.org/torchrec/">torchrec</a> library to implement and shared the large pin ID table across GPUs. We serve the CPU model artifact due to the loose latency requirement for offline inference.</p><p>However, although the training objectives for the two models are similar (i.e., contrastive learning over sampled negatives), directly fine-tuning the embeddings does perform well online. We found that the model suffered from overfitting severely. To mitigate this, we first fixed the embedding table and applied an aggressive dropout with dropout probability of 0.5 on top of the ID embeddings, which led to decent online gains (0.6–1.2% HF repins and clicks increase). Later, we found it is not optimal to simply use the latest pretrained ID embedding, as the overlap between cotraining window and model training window can worsen overfitting. We ended up choosing the latest ID embedding without overlap, providing 0.25–0.35% HF repins increase.</p><h3>Serving Corpus Upgrade</h3><p>Apart from model upgrades, we also renovate our serving corpus as it defines the upper-limit of retrieval performance. Our initial corpus setup was to individualize Pins based on their canonical image signature, then include Pins with the most accumulated engagements in the last 90 days. To better capture the trends at Pinterest, instead of directly summing over the engagements, we switch to a time decayed summation to determine the score of a Pin p at date d as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1022/0*8yi_ueKHWR2rPXNt" /></figure><p>In addition, we also figured out a discrepancy in image signature granularity between training data and serving corpus. Serving corpus operates on a more coarse granularity to deduplicate similar contents and reduce indexing size; however, it will cause statistical features drifting, such as Pin engagements because the looked up image signature is different compared to training data. Closing this gap with a dedicated image signature remapping logic plus the time decay heuristics, we achieved +0.1–0.2% engaged sessions without any modeling changes.</p><h3>Revolutionize Embedding Based Retrieval</h3><p>In this section, we will briefly showcase our recent journey to bootstrapping the impact of embedding-based retrieval with state-of-the-art modeling techniques.</p><h3>Multi-Embedding Retrieval</h3><p>Different from other surfaces, homefeed has users entering with diverse intents, and it can be inadequate to represent all sorts of intents by a single embedding. With extensive experiments, we found that a differentiable clustering module modified upon Capsule Networks[4][5] performs better than other variants such as multi-head attention and pre-clustering based methods. We switched the cluster initialization with maxmin initialization[6] to speed up clustering convergence, and enforce single-assignment routing where each history item can only contribute to one cluster’s embedding to enhance diversification. We combine each of the cluster embeddings with other user features to generate multiple embeddings.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*R9GzBWczaovx6VkO" /><figcaption>Figure 3. Left: Multi-Embedding Retrieval Model Structure. Right: Visualization results for a random user, every 2 Pins belong to the same user embedding.</figcaption></figure><p>At serving time, we only keep the first K embeddings and run ANN search, and K is determined by the length of user history. Thanks to the property of maxmin initialization, the first K embeddings are generally the most representative ones. Then the results are combined in a round robin fashion and passed to the ranking and blending layers. This new user sequence modeling technique not only hones diversity of the system but also helps increase users’ save actions, indicating that users refine their inspiration on homefeed.</p><h3>Conditional Retrieval for Homefeed</h3><p>At Pinterest, a great source of diversity comes from the interest feed candidate generator, a token-based search according to users’ explicit followed interests and inferred interests. These explicit interest signals may provide us with auxiliary information on the user’s intentions beyond user engagement history. However, due to lack of finer-grained personalization among the matched candidates, they tend to have lower engagement rate.</p><p>We utilized conditional retrieval[8], a two-tower model with a conditional input to boost personalization and engagements: at training time, we feed the target Pin’s interest id and embed it as the condition input to the user tower; when we serve the model, we feed users’ followed and inferred interests as the conditional input to fetch the candidates. The model follows an early-fusion paradigm that the conditional interest input is fed into the model at the same layer as all other features. Surprisingly, the model can learn to condition its output and produce highly relevant results, even among the long-tail interests. We further equipped the ANN search with interest filters to guarantee high relevance between the query interest and the retrieved candidates. Having better personalization and engagements at the retrieval stage helps improve recommendation funnel efficiency and improves user engagements significantly.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*_6Z092l694FTY8BS" /><figcaption>Figure 4. Left: Conditional Retrieval Model Structure. Right: Visualization results for a random user, the top figure shows retrieved candidates for iced coffee, a popular interest, and the bottom one shows retrieved candidates of friendship bracelets, which is a fine-grained tail interest.</figcaption></figure><h3>Acknowledgment</h3><p>This blog represents a variety of workstreams on embedding-based retrieval across many teams at Pinterest. We want to thank them for the valuable support and collaboration.</p><p>Home Relevance: Dafang He, Alok Malik</p><p>ATG: Yi-Ping Hsu</p><p>PADS: Lily Ling</p><p>Pinner Curation &amp; Growth: Jay Adams</p><h3>Reference</h3><p><a href="https://arxiv.org/pdf/2102.07619">[1] Wang, Zhiqiang, Qingyun She, and Junlin Zhang. “Masknet: Introducing feature-wise multiplication to CTR ranking models by instance-guided mask.” <em>arXiv preprint arXiv:2102.07619</em> (2021).</a></p><p><a href="https://arxiv.org/pdf/2203.11014">[2] Zhang, Buyun, et al. “DHEN: A deep and hierarchical ensemble network for large-scale click-through rate prediction.” <em>arXiv preprint arXiv:2203.11014</em> (2022).</a></p><p><a href="https://arxiv.org/pdf/1706.03762">[3] Vaswani, Ashish, et al. “Attention is all you need.” <em>Advances in neural information processing systems</em> 30 (2017).</a></p><p><a href="https://arxiv.org/pdf/1710.09829">[4] Sabour, Sara, Nicholas Frosst, and Geoffrey E. Hinton. “Dynamic routing between capsules.” <em>Advances in neural information processing systems</em> 30 (2017).</a></p><p><a href="https://arxiv.org/pdf/1904.08030">[5] Li, Chao, et al. “Multi-interest network with dynamic routing for recommendation at Tmall.” <em>Proceedings of the 28th ACM international conference on information and knowledge management</em>. 2019.</a></p><p><a href="https://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf">[6] Arthur, David, and Sergei Vassilvitskii. <em>k-means++: The advantages of careful seeding</em>. Stanford, 2006.</a></p><p><a href="https://dl.acm.org/doi/abs/10.1145/3640457.3688053">[7] Hsu, Yi-Ping, et al. “Taming the One-Epoch Phenomenon in Online Recommendation System by Two-stage Contrastive ID Pre-training.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>. 2024.</a></p><p><a href="https://dl.acm.org/doi/abs/10.1145/3640457.3688057">[8] Lin, Hongtao, et al. “Bootstrapping Conditional Retrieval for User-to-Item Recommendations.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>. 2024.</a></p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d7d7971a409e" width="1" /><hr /><p><a href="https://medium.com/pinterest-engineering/advancements-in-embedding-based-retrieval-at-pinterest-homefeed-d7d7971a409e">Advancements in Embedding-Based Retrieval at Pinterest Homefeed</a> was originally published in <a href="https://medium.com/pinterest-engineering">Pinterest Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>