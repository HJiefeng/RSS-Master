<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>Lyft Engineering - Medium</title>
        <link>https://eng.lyft.com?source=rss----25cd379abb8---4</link>
        
        <item>
            <id>https://medium.com/p/fa90b3f3ec24</id>
            <title>Real-Time Spatial Temporal Forecasting @ Lyft</title>
            <link>https://eng.lyft.com/real-time-spatial-temporal-forecasting-lyft-fa90b3f3ec24?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/fa90b3f3ec24</guid>
            <pubDate></pubDate>
            <updated>2025-05-05T17:50:12.462Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0eiKINrJ8k6dlZG3K4m0IQ.jpeg" /></figure><p><em>Written by </em><a href="https://www.linkedin.com/in/joshxiaominxi/"><em>Josh Xi</em></a><em> &amp; </em><a href="https://www.linkedin.com/in/rakeshkumar1007/"><em>Rakesh Kumar</em></a><em> at Lyft.</em></p><p>From real-time rider pricing and driver incentives to long-term budget allocation and strategic planning, forecasting at Lyft plays a pivotal role in providing a foresight of our market conditions for efficient operations and facilitating millions of rides daily across North America. This article explores real-time spatial temporal forecasting models and system designs used for predicting market conditions, focusing on how their complexity and rapid nature affect model performance, selection, and forecasting system design.</p><h3>Introduction: Real-Time Spatial Temporal Forecasting</h3><h4>Definition</h4><p>Real-time spatial temporal forecasting is often used to predict market signals, ranging from a few minutes to a couple of hours, at fine spatial and temporal granularity. For example, we can predict rideshare demand and supply at <a href="https://www.ibm.com/docs/en/streams/4.3.0?topic=334-geohashes">geohash-6</a> level for every 5 minute interval in the next hour for an entire city or region. The forecast also runs at a high frequency (e.g. every minute) with real-time input of the most refreshed data to capture the latest marketplace conditions in fine detail, including local spikes and dips.</p><p>Lyft currently operates in hundreds of North American cities and regions. Our spatial temporal forecasting models predict forecast values for 4M geohashes per minute, per signal.</p><h4>Use Cases</h4><p>Predictions from real-time forecasting are often used for inputs into levers that balance real-time demand and supply across space. For example:</p><ul><li><strong>Dynamic pricing:</strong> As a platform, maintaining marketplace balance is one of Lyft’s top missions. Dynamic pricing maintains balance in real-time by raising prices to dampen demand when drivers are scarce and lowering prices to encourage rides when there are more drivers.</li><li><strong>Real-time driver incentives:</strong> In contrast to dynamic pricing managing demand, real-time driver incentives achieve marketplace balance by raising drivers’ earnings to attract drivers to come online during undersupply or relocate from oversupplied to undersupplied areas. Similarly, understanding the current and near-future marketplace conditions in different locations is essential to the incentive model.</li></ul><h4>Challenges</h4><p>The high dimension, high frequency nature of real-time spatial temporal forecasting presents unique challenges compared to low dimension, low frequency time series forecasting.</p><blockquote>Large Computation Restricts Practical Model Choices and System Design</blockquote><p>More computation power is needed for heavy spatial and temporal data processing and fast online inference, which can restrict our model choices and system design in practice. Large complex models, such as deep neural networks, might give more accurate forecasts in theory, but they also cost more to run, take longer, and can be harder to keep stable and scale up. This can cancel out the accuracy gains.</p><blockquote>Noisy Signals Reduce Forecasting Accuracy</blockquote><p>When analyzing signals with greater spatial and temporal detail, such as minutely geohash-6 level rideshare demand and supply, they can become noisier compared to when viewed at a more aggregated level. Specifically, we observe:</p><ul><li>Sparser signals with many zero observations;</li><li>Increase in intermittent local spikes and dips, usually lasting from a few minutes to about 30 minutes.</li></ul><p>These local fluctuations are often due to events like concerts, sports, and community gatherings, which may not be noticeable at broader hourly, daily, or regional levels, but become significant when viewed in detail.</p><p>Modeling such impact can be challenging for multiple reasons:</p><ul><li><strong>Data availability &amp; accuracy:</strong> Getting comprehensive event data can be costly in practice. Even with access to events data, it can be hard to predict key information such as event end time or impact time at the desired accuracy level. For example, although an American football game displays 5 minutes left on the screen, the actual event end time can vary from 5 to 30 minutes.</li><li><strong>Compound effect:</strong> In our experience, the actual impact of events on rideshare demand is usually compounded with other factors; for example:<br /><strong><em>* </em></strong>Venue operation like shuttle services and designated rideshare pickups can affect both demand and supply spatially.<br /><strong><em>*</em></strong><em> </em>Availability of public transit and parking facilities can affect travelers’ mode choice.<br /><strong><em>*</em></strong><em> </em>Nearby amenities, like restaurants, can affect pre- and post-event activities and travel plans.<br /><strong><em>*</em></strong><em> </em>Time of day, day of week, and seasonality can also affect public transit, venue and business operations, affecting demand patterns.</li></ul><p>Due to this “noise”, the spatial and temporal correlation and stability at a detailed level can drop significantly from those at an aggregated level. We will discuss how these changes could affect forecast accuracy in the model performance section.</p><h3>Forecasting Models</h3><p>We have explored two distinct sets of models for real-time spatial temporal forecasting: classical time-series models and neural network models. In this section, we introduce a few representative models that we have explored at Lyft and/or have been well studied in research papers. We also compare the model performance in terms of forecast accuracy and engineering cost from our implementation. Note that although it’s mentioned in the introduction that local events can contribute to fluctuations in our signals, we will not explicitly discuss events or events modeling here as it is a complicated topic worth another discussion of its own.</p><h4>Time Series Models</h4><p>Some of the time series models we have explored include:</p><ul><li><strong>Linear regression models like auto-regression and ARIMA:</strong> These simple time series models use past data (linear combinations or weighted averages) to predict the future. To adapt single time series models on spatial data (multiple correlated time series), we can either assume the same model weights for all regional geohashes or divide the region into partitions based on signal history, assigning specific weights to each partition.</li><li><strong>Spatial temporal covariance models:</strong> Instead of treating different geohashes (or partitions) as independent time series, we can estimate and model spatial temporal correlations explicitly (see <a href="https://marcgenton.github.io/2021.CGS.ARSIA.pdf">Chen et al. [2021]</a>).</li><li><strong>Spatial temporal correlation through dimension reduction: </strong>Assuming that many geohashes are correlated, we can apply dimension reduction approaches like SVD and PCA to project thousands of time series to a few dozen, with correlation embedded in the dimension reduction process. We then apply traditional time series models on the reduced dimensions for forecasting, and finally project the forecasts back to the original geohash dimension (see <a href="https://www.sciencedirect.com/science/article/abs/pii/S0960148114002432">Skittides Fruh [2014]</a>, <a href="https://dl.acm.org/doi/abs/10.1145/3538637.3539764">Grother &amp; Reiger [2022]</a>).</li></ul><h3>Neural Net Models</h3><p>Deep neural network (DNN) models have emerged as powerful alternatives due to their capacity to automatically handle complex non-linear spatial and temporal patterns without extensive feature engineering (<a href="https://www.mdpi.com/2078-2489/14/11/598">Casolaro et al. [2023]</a>; <a href="https://link.springer.com/article/10.1007/s11831-025-10244-5">Mojtahedi et al. 2025</a>). Some of the DNNs we tested are:</p><ul><li>Recurrent neural networks (RNN) and variants like long short-term memory (LTSM): <a href="https://arxiv.org/pdf/2108.11875">He et al [2021]</a>, <a href="http://abduljabbar">Abduljabbar et al. [2021]</a>. RNNs’ fundamental architecture for sequence modeling is perfect for learning temporal correlation by memory retention from previous time steps. However, standard RNNs suffer from vanishing gradient problems when processing long sequences. LSTM networks address this limitation through gating mechanisms.</li><li>Convolutional neural networks (CNN): <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/09/DeepST-SIGSPATIAL2016_Zheng-2.pdf">Zhang et al. [2016]</a>, <a href="https://ieeexplore.ieee.org/abstract/document/8684259">Guo et al. [2019]</a>. CNNs were initially applied to image processing, and recently have been adapted for spatial temporal modeling, treating signal values at each map timestamp like a snapshot of pixels in an image.</li></ul><p>A few other emerging DNN models, which we haven’t tested yet but give similar performance in literature, are:</p><ul><li>Graphic neural networks (GNN): <a href="https://link.springer.com/article/10.1007/s10489-021-02587-w">Bui et al. [2022]</a>, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025522003978">Geng et al. [2022]</a>, <a href="https://arxiv.org/pdf/2410.22377">Corradini et al. [2024]</a>. GNNs have been well-applied to graph-structured data in applications like behavior detection, traffic control, and molecular structure study, where nodes and edges represent entities and their relationships. In spatial temporal forecasting, it is assumed that the signal value of a node depends on its own history and the history of its neighbors, whose weights are estimated by specific blocks or modules in the GNN structure.</li><li>Transformer models: <a href="https://arxiv.org/abs/2001.08317">Wu et al. [2020]</a>, <a href="https://arxiv.org/pdf/2202.07125">Wen et al. [2022]</a>. Inspired by their success in natural language processing, transformer models have been increasingly applied to time series forecasting. However, literature has shown mixed results so far (<a href="https://ojs.aaai.org/index.php/AAAI/article/view/26317">Zeng et al. [2023]</a>).</li></ul><h4>Online Implementation &amp; Refitting</h4><p>Our marketplace is constantly changing from day-to-day and minute-to-minute. In our experience, a model trained on historical data can quickly become obsolete, sometimes in a few days but sometimes in a dozen minutes, in the occurrence of some unknown or unmodeled local events. Therefore, frequently retraining the model is critical to the accuracy of the forecasts. We have implemented the following in our forecasting:</p><ul><li>Refit time series models every minute using the latest observations before running an inference.<br /><strong><em>* </em></strong>Due to the simplicity of the model structure and smaller model weights, these models are effortlessly re-trainable without causing significant latency or memory issues.<br /><strong><em>* </em></strong>It’s worth noting that some of the time series models, such as those based on dimension reduction, are only partially re-trainable in real-time. While new data samples can update model weights in the reduced dimension, this process requires a longer history, which can be time consuming and can only be completed offline at less frequent intervals.</li><li>Refit DNN models multiple times a day.<br /><strong><em>* </em></strong>While theoretically all neural network models can be refitted on new data samples, their model size can cause high latency, making real-time refitting less ideal. Instead, we refit the model separately offline multiple times daily using recent data batches.</li></ul><h3>Model Performance</h3><p>In our experience, model choice is a trade-off between forecast accuracy and engineering cost. In this section, we share some learnings from our testing and implementation of time series and DNN models. For those interested in or familiar with forecasting modes, the “Accuracy” section offers technical insights into why certain models outperform others. Alternatively, you can focus on the key learnings highlighted in <strong>bold</strong>.</p><h4>Accuracy</h4><p>While most literature finds that DNN models provide better forecast accuracy than classical time series models, we find it to be only partially true. Below are some of our learnings:</p><h4>DNNs outperform time series without latency consideration</h4><p>When latency is not considered and forecasts are simulated at the same refitting frequency (from daily, hourly, to minutely), DNN models overall generate more accurate forecasts than classical time series models. However, as the refitting frequency increases, the accuracy gap reduces significantly; and in some cases, time series models can even outperform DNN models.</p><h4>Time series outperforms DNNs with latency consideration</h4><p>When considering latency, simulating time series refitted minutely and DNN refitted hourly results in time series models having better overall accuracy than DNN. Specifically,</p><ol><li><strong>Time series models are more accurate for forecasting short-term horizons, such as the next 5 to 30 or 45 minutes. In contrast, DNN models can outperform time series in longer horizons, beyond 30 or 45 minutes. </strong><br />Our real-time signal has strong near-term autocorrelation. In other words, what will happen in the next 5 minutes can be similar to what happened in the past 5 minutes; hence, even a simple autocorrelation model can generate a good forecast.<br />This is also true during temporal local spikes caused by irregular events. To illustrate, imagine riders leaving a concert. Rideshare demand usually spikes up fast, stays high in the first 15–20 minutes, then gradually returns to normal in the next 15–20 minutes. As the peak occurs, refitting an autoregression model can quickly pick up the autocorrelation and update the forecast accordingly. Meanwhile, a local spike can temporarily change the spatial correlation across nearby geohashes, and without refitting, it can throw off the forecasts of a DNN model’s estimation of spatial correlation.<br />As the forecast moves to a longer horizon, the near-term autocorrelation gets weaker. In the concert example above, a demand spike in the past 5 minutes does not guarantee a spike in an hour. Instead, factors like seasonality, trend, and spatial correlation become more important predictors, which DNN models seem to capture better than classical time series models.</li><li><strong>Between demand and supply signals, both time series and DNN models tend to give better accuracy on supply; however, time series models are more likely to outperform DNN on demand signals. </strong><br />The rationale behind this is due to different levels of signal noise. Drivers tend to stay online for a while after logging on, resulting in smoother supply patterns than for demand, with less temporary spikes or dips. As a result, supply signals have more stable spatial and temporal correlations, making both sets of models perform better. Meanwhile, riders make on-demand requests based on their individual schedules, which tends to cause more fluctuations in demand pattern, making time series with fast refitting a better option. <br />In general, the underlying signal generation process influences the spatial temporal correlation and stability, affecting forecast accuracy. For example, average traffic speed in different locations of a city resembles a Gaussian process, and hence is likely to have a much stronger spatial correlation and more stable temporal pattern compared to a demand signal from a Poisson process.</li><li><strong>For regions with complicated terrain structures (like lots of hills and lakes), DNN models tend to perform worse than time series models</strong>.<br />Our conjecture is that complicated terrain structures can weaken spatial correlation; hence, making some of the DNN models less powerful. For example, a city with many mountains and waters can have more pockets with zero demand and supply; and a city with many venues for irregular events can cause more local spikes and dips.</li></ol><p>From our learnings, we can conclude that forecast accuracy is heavily dependent on the signal characteristics. Before choosing your model, you should evaluate signals for their spatial and temporal correlation and stability.</p><h4>Engineering Cost</h4><p>Real-time spatial temporal forecasting is big in size, and requires a large amount of memory and computation power for heavy data processing and fast online inference. Hence, scalability, stability (e.g. low latency), and computation cost affect the final model and system design.</p><p>Given the size of the DNN models, it’s no surprise that they are more costly than time series models. In particular:</p><ul><li><strong>Training cost:</strong> DNN models require training on GPU, which can be 100x more expensive than classical time series models that train on CPU. For example, training a DNN model on a few weeks of data of a single region can take a couple of hours on a 128GB GPU, while a classical time series model takes less than a minute on an 8GB CPU. These cost differences can be non-trivial when training separate models for hundreds of regions on dozens of signals.</li><li><strong>Engineering reliability:</strong> In our experience, DNN models are more prone to issues like training failures, out-of-memory errors, and high latency, incurring higher maintenance costs.</li><li><strong>Forecast interpretability &amp; debuggability:</strong> Forecasts from conventional time-series models are usually more interpretable, making it easy to debug performance issues and overwrite forecasts manually if necessary. For example, a time series can be broken down into trend and seasonality, with events and weather impacts added. Each component can be further examined and overwritten if expert knowledge or external data provides a different projection.</li></ul><h3>Forecasting Architecture &amp; Tech Stack</h3><h4>Architecture</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3vmpUOqOOaLbotj8" /><figcaption>Figure 1: Architecture diagram for forecasting pipelines</figcaption></figure><p>The current architecture uses the following technology stack:</p><ul><li><a href="https://beam.apache.org/">Apache Beam</a> (<a href="https://flink.apache.org/">Apache Flink</a> runner) streaming pipelines</li><li>Amazon Services (<a href="https://aws.amazon.com/msk/">MSK</a>, <a href="https://aws.amazon.com/kinesis/">Kinesis</a>, <a href="https://aws.amazon.com/pm/dynamodb/?gclid=Cj0KCQiA8fW9BhC8ARIsACwHqYqKfsD8rEN51OItEvKNW9k4EBaFlcQUOQJPf-XVMFXc7M5VV5kCeNAaAiXUEALw_wcB&amp;trk=94bf4df1-96e1-4046-a020-b07a2be0d712&amp;sc_channel=ps&amp;ef_id=Cj0KCQiA8fW9BhC8ARIsACwHqYqKfsD8rEN51OItEvKNW9k4EBaFlcQUOQJPf-XVMFXc7M5VV5kCeNAaAiXUEALw_wcB:G:s&amp;s_kwcid=AL!4422!3!610000101513!e!!g!!dynamodb!11205224796!115505094368">DynamoDB</a>)</li><li><a href="https://eng.lyft.com/powering-millions-of-real-time-decisions-with-lyftlearn-serving-9bb1f73318dc">Lyft Machine Learning Platform</a> (<a href="https://airflow.apache.org/">Apache Airflow</a>, <a href="https://aws.amazon.com/sagemaker">AWS SageMaker</a>)</li><li><a href="https://clickhouse.com/">ClickHouse</a></li></ul><h4><strong>Execution flow</strong></h4><ol><li>Before we can run any online inference, an offline model training pipeline needs to be executed to construct online model structure and initialize model weights. These are saved into a model artifacts database for online models to access. This pipeline can also be used to retrain model weights at a scheduled frequency or ad-hocly, and push updated weights to the artifacts database.</li><li>The system gets the data based on analytics events. These events are generated by our services and client apps.</li><li>The feature generation pipeline aggregates the events (read about them in this <a href="https://eng.lyft.com/evolution-of-streaming-pipelines-in-lyfts-marketplace-74295eaf1eba">blog post</a>) and passes them through Kafka topics.</li><li>Features are indexed in OLAP DB (ClickHouse) as short-term historical features with a one day TTL.</li><li>Our forecasting pipeline takes these features from Kafka topics and OLAP DB. This ensures data consistency across different features based on the window end time of those features. We pass them to our forecasting models hosted on Lyft’s Machine Learning Platform (MLP). Our models also use older historical features (days/weeks), refreshed daily via Airflow DAGs.</li><li>MLP is responsible for model online inference and asynchronously syncing results to a Kafka topic. If online refitting is implemented, our models can refit using the most recent input and adjust their weights before each inference.</li><li>This topic is subscribed by downstream services (Rider Pricing, Driver Earnings, etc.) who are interested in forecasted values and want to consume them in real-time. Forecasted values are stored in the Feature Store in case users want to consume them in asynchronous fashion.</li><li>The forecasted values are also indexed in the OLAP DB, so we can calculate and monitor model performance metrics in real-time.</li></ol><p>We have chosen an asynchronous design mainly for scaling and performance reasons.</p><h4>Forecasted Feature Guarantee</h4><p>It is important that features come with a quality and reliability guarantee, otherwise it would be difficult for our customers to build products with confidence. For each signal, we define a quality guarantee and measure through our internal systems. Specifically, one system generates model performance metrics (e.g. bias, mean absolute percent error) based on the forecasted and historical values stored in the OLAP DB. Another system constantly monitors these metrics, and if the metrics are outside the expected bounds, then it alerts our engineering team.</p><h3>Conclusion</h3><p>Forecasting performance is heavily dependent on the characteristics of the forecasted signals, such as spatial temporal granularity, level of noise, spatial temporal correlation, and its stability. In our experience, simple time series models with real-time refitting provide overall better accuracy when the signal is more granular with less stable spatial temporal correlations, and/or when forecasting the near-term horizons. Although a complex DNN model can improve forecasting accuracy if refitted real-time or forecasting for longer horizons, they may not be the best solution for your business due to high computation cost and latency. Businesses often prioritize simpler models with greater interpretability, lower inference latency, a simplified retraining process and, <em>crucially, </em>a lower total cost of ownership. These factors are essential for choosing cost-effective, maintainable solutions in real-world applications.</p><h3>Acknowledgements</h3><p>We would like to thank all our forecasting team members (<a href="https://www.linkedin.com/in/jimchiang/">Jim</a>, <a href="https://www.linkedin.com/in/glennazhang/">Glenna</a>, <a href="https://www.linkedin.com/in/quinn-liu/">Quinn</a>, <a href="https://www.linkedin.com/in/hongru-liu/">Hongru</a>, <a href="https://www.linkedin.com/in/binli98004/">Bin</a>, <a href="https://www.linkedin.com/in/lees28/">Soo</a>, <a href="https://www.linkedin.com/in/kyle-bilton/">Kyle</a>, <a href="https://www.linkedin.com/in/ido-bright-a1808429/">Ido</a>, <a href="https://www.linkedin.com/in/evanwils/">Evan</a>) for their contribution to model and architecture development, as well as the editing team (<a href="https://www.linkedin.com/in/jeana-choi/">Jeana</a>) for valuable suggestions and support during writing this blog post.</p><p>Want to build ML solutions that impact millions? Join Lyft! We’re leveraging machine learning to solve problems at scale. If you’re passionate about building impactful, real-world applications, explore our openings at <a href="https://www.lyft.com/careers">Lyft Careers</a>.</p><h3>Relevant Posts</h3><ul><li>Learn how we solve other forecasting problems at Lyft: <a href="https://eng.lyft.com/causal-forecasting-at-lyft-part-2-418f1febca5a">causal forecasting</a>, <a href="https://eng.lyft.com/making-long-term-forecasts-at-lyft-fac475b3ba52">cohort-based long-term forecasts</a>, <a href="https://eng.lyft.com/how-to-deal-with-the-seasonality-of-a-market-584cc94d6b75">seasonality</a>.</li><li>Check how we build <a href="https://eng.lyft.com/building-real-time-machine-learning-foundations-at-lyft-6dd99b385a4e">real-time machine learning foundations</a> and <a href="https://eng.lyft.com/ml-feature-serving-infrastructure-at-lyft-d30bf2d3c32a">ML feature service</a>.</li><li>Check out how we <a href="https://eng.lyft.com/evolution-of-streaming-pipelines-in-lyfts-marketplace-74295eaf1eba">evolved our streaming pipelines</a> for realtime ML feature generation.</li><li>Discover how <a href="https://eng.lyft.com/gotchas-of-streaming-pipelines-profiling-performance-improvements-301439f46412">Lyft identified and fixed performance issues</a> in our streaming pipelines.</li><li>Learn <a href="https://eng.lyft.com/gotchas-of-stream-processing-data-skewness-cfba58eb45d4">how data skewness</a> can affect performance of streaming pipelines.</li></ul><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fa90b3f3ec24" width="1" /><hr /><p><a href="https://eng.lyft.com/real-time-spatial-temporal-forecasting-lyft-fa90b3f3ec24">Real-Time Spatial Temporal Forecasting @ Lyft</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/74c4f9df4680</id>
            <title>From manual fixes to automatic upgrades — building the Codemod Platform at Lyft</title>
            <link>https://eng.lyft.com/from-manual-fixes-to-automatic-upgrades-building-the-codemod-platform-at-lyft-74c4f9df4680?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/74c4f9df4680</guid>
            <pubDate></pubDate>
            <updated>2025-04-30T15:24:59.151Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <h3>From manual fixes to automatic upgrades — building the Codemod Platform at Lyft</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JpuIl6aS0EAFxl75QWlnsg.jpeg" /></figure><p>In software development, keeping your code up-to-date with the latest libraries and APIs is both crucial and frustrating — especially in large-scale organizations. There’s always this trade-off between building shiny new features and slogging through tech debt to upgrade dependencies. And let’s be honest, new features almost always win. They’re the ones driving business value, after all.</p><p>What if we flipped the script? Imagine upgrading libraries, handling breaking changes, and adopting new features happening seamlessly — without pulling developers away from their real work. Not only would that be cool, but it would also help engineers stay focused on delivering business value by removing the toil that often makes reducing tech debt such a tough tradeoff. Well, that’s exactly the challenge we set out to tackle.</p><p>My team, Frontend Developer Experience, maintains the server-side rendering (SSR) web platform and a common components library used across all 100+ Lyft frontend microservices. Keeping these core tools updated was always a big effort, so we set out to build something that could automate upgrades — handling breaking changes and rolling out new features seamlessly.</p><p>To make this happen, we decided to build a Codemod Platform to handle code transformations at scale. We already had some codemod tools, but they were tied to specific libraries or versions. We wanted something better — a platform that works for any library or framework and makes updates easy and reusable.</p><p><em>If you’ve never used codemods, they’re scripts that transform code by parsing it into a tree (</em><a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree"><em>AST — Abstract Syntax Tree</em></a><em>), making changes, and converting it back.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*c5QcnGfsG3CdlmSS" /></figure><p>With the right library, you can automate almost any change, but handling all edge cases takes time and patience.</p><h4>Our Goals</h4><p>To tackle this challenge, we started with several clear goals:</p><ul><li><strong>Automate dependency upgrades: </strong>minor version updates often introduce new features, while major versions come with breaking changes that require fixes. Instead of developers manually reading docs, testing changes, and updating code, we would love codemods to handle everything automatically — seamlessly upgrading APIs and applying necessary fixes without human intervention.</li><li><strong>Make codemods easier to write: </strong>reduce the learning curve by providing helper functions and clear documentation, making it easier for developers to create their own transforms.</li><li><strong>Make it accessible to all developers:</strong> to ensure codemods could run anywhere with Node.js access, we provide our own CLI tool. By executing it with <a href="https://docs.npmjs.com/cli/v11/commands/npx">npx</a>, developers can run codemods without needing a global installation or adding them to frontend (FE) repositories.</li><li><strong>Standardize codemods across Lyft:</strong> last but not least, we aimed to unify the different codemod implementations across Lyft.</li></ul><h4>Requirements</h4><p>Choosing the right library was key. There aren’t many options for transforming code in frontend, and <a href="https://github.com/facebook/jscodeshift">jscodeshift</a> was the best fit — it provides parsing, transformation, and writing in one place. Since we needed to handle TS, TSX, JS, and JSX files, jscodeshift worked out of the box. However, it also had some limitations we wanted to address:</p><ul><li><strong>Reusing code:</strong> out of the box, jscodeshift provides a <a href="https://github.com/facebook/jscodeshift?tab=readme-ov-file#usage-cli">CLI</a> to run a single transform module. However, we wanted more flexibility — allowing one transform to execute another or run multiple transformations together.</li><li><strong>Detecting eligible services:</strong> for dependency upgrades, we needed to skip services that didn’t use the dependency to avoid wasting resources. This was especially important when running codemods across hundreds of microservices in CI.</li><li><strong>Evergreen codemods:</strong> some transforms need extra setup before running. For example, upgrading @lyft/service (core library for Lyft FE services, <a href="https://eng.lyft.com/changing-lanes-how-lyft-is-migrating-100-frontend-microservices-to-next-js-42199aaebd5f">learn more</a>) to v2 requires installing sass to avoid breaking changes. Since every @lyft/service version from v2 onward has this requirement, we needed a way to ensure it was always handled automatically. We introduced evergreen codemods — pre-checks that set up things before running the main transform. We later expanded this to include post-transform checks as well.</li><li><strong>Non-JS transforms</strong>: In addition to TS and JS files, we wanted to support transformations for YAML, JSON, and .env files. This allowed us to extend codemods beyond code changes, handling configuration updates, environment variable adjustments, and other use cases, making the platform even more powerful.</li><li><strong>Naming convention:</strong> A clear and consistent naming convention makes the CLI easier to use and understand. To keep codemods predictable and maintainable, we established a set of naming rules:<br />– Clear and descriptive — the name should clearly state what the transformation does, use a predictable format like verb-noun or verb-noun-adjective to make names easy to interpret.<br />– Versioning — if a codemod applies to a specific library update, include the version number (e.g., react-18.3.1, next-15.1) to a transform name.<br />– Allowed characters — use only lowercase letters, numbers, “.” and “-” (e.g., react-18.3.1 instead of react_1831). This prevents issues with version mix-ups, such as distinguishing 18.3.1 from 1.83.1.</li><li><strong>Run 3rd-party codemods: </strong>many Frontend open-source libraries provide their own codemods to simplify migrations. For example, Next.js has the <a href="https://nextjs.org/docs/app/building-your-application/upgrading/codemods">@next/codemod</a> CLI; React and Storybook offer similar tools. We wanted our Codemod Platform to support running these existing transforms, allowing us to reuse them instead of reinventing the wheel.</li><li><strong>Helper functions:</strong> last but not least, we aimed to minimize boilerplate. Whether adding a new import or removing a JSX prop from a React component, we wanted reusable functions to handle these common tasks automatically.</li></ul><h4>Design</h4><p>To address these challenges, we built a solution that covers all these gaps. Here’s how we did it:</p><p><strong>@lyft/codemod CLI</strong></p><p>A typical codemod execution from the terminal would look like this:</p><a href="https://medium.com/media/030c7fd847c4e2bfd1a1f95332bfb43e/href">https://medium.com/media/030c7fd847c4e2bfd1a1f95332bfb43e/href</a><p>For the CLI to find and execute the correct transform, we created a helper function called executeUpgrade:</p><a href="https://medium.com/media/e6633e16286ab9d4e2c43f8109b11161/href">https://medium.com/media/e6633e16286ab9d4e2c43f8109b11161/href</a><p>As you might have noticed, we execute the UpgradeClass, which sits at the core of our platform. This class was designed to handle all the requirements we outlined, ensuring every transformation runs smoothly and consistently. Let’s take a closer look at its implementation:</p><a href="https://medium.com/media/95726eb31fa29ecd608c44875595db83/href">https://medium.com/media/95726eb31fa29ecd608c44875595db83/href</a><p>To see how everything fits together, let’s walk through a real example.</p><p>Suppose we have a component library called core-ui, and in version 2, we removed the compact prop from the Button component. Our goal is to create a @lyft/codemod transform to automatically fix this breaking change.</p><p>To start, we create a new folder in our transforms directory:</p><a href="https://medium.com/media/a817b7e703f825b3120476f9d7825c6a/href">https://medium.com/media/a817b7e703f825b3120476f9d7825c6a/href</a><p>The index.ts file will export a class extending UpgradeBase with the following implementation:</p><a href="https://medium.com/media/5022448e2a77adb3c48b363fba5557e7/href">https://medium.com/media/5022448e2a77adb3c48b363fba5557e7/href</a><p>The transform-buttons.ts file follows the same structure as a standard <a href="https://github.com/facebook/jscodeshift?tab=readme-ov-file#transform-module">jscodeshift transform module</a>. This made migrating existing jscodeshift transforms from other projects as simple as copying and pasting them into the new platform.</p><a href="https://medium.com/media/6cb2ed24d87f282605a299239d2251ba/href">https://medium.com/media/6cb2ed24d87f282605a299239d2251ba/href</a><p>Or, even better if using helper functions to reduce boilerplate:</p><a href="https://medium.com/media/32d0475bd87d9671d2a9a7020287af6b/href">https://medium.com/media/32d0475bd87d9671d2a9a7020287af6b/href</a><p><strong>Execution flow</strong></p><ol><li>@lyft/codemod -t core-ui-2 is being executed</li><li>executeUpgrade searches for a folder named core-ui-2</li><li>core-ui-2 must contain an index.ts file exporting UpgradeBase as the default</li><li>UpgradeBase.execute is called to run the codemod</li><li>Execute checks if the service is eligible</li><li>If eligible, runUpgrade executes the transform-buttons transformation</li><li>The compact prop has been removed from the Button component across all files under the pathname</li><li>🎉 Profit</li></ol><a href="https://medium.com/media/6a0da699ce8b6d10d4028e2834b5ae51/href">https://medium.com/media/6a0da699ce8b6d10d4028e2834b5ae51/href</a><p>This example covers a single use case, but multiple transformations can be combined within one folder and executed sequentially. Now, imagine applying this across hundreds of microservices!</p><p><strong>More complex use cases</strong></p><p>Some codemods could be a lot more complex. For example, it could be running one codemod, and then executing another transform file, and then running 3rd-party codemod. All these can be easily handled by the platform. Here’s the example of the index file of the transform as well as how the whole file structure would look like:</p><a href="https://medium.com/media/9d3315d6b8ed68dfca83932bbd1ed7f7/href">https://medium.com/media/9d3315d6b8ed68dfca83932bbd1ed7f7/href</a><a href="https://medium.com/media/bae7a12dc2e9772bb34746846a8c205e/href">https://medium.com/media/bae7a12dc2e9772bb34746846a8c205e/href</a><p>It could be much more than that, but hopefully, this example gives you a sense of what codemods can achieve.</p><h4>Releasing</h4><p>The Codemod Platform is essentially a library with its own CLI. To manage different versions, we package it as an internal npm package called @lyft/codemod, which any Lyft developer can execute using:</p><a href="https://medium.com/media/7bd4a3f523899da6aafbc1bf5347ab03/href">https://medium.com/media/7bd4a3f523899da6aafbc1bf5347ab03/href</a><p>Versioning follows <a href="https://semver.org/">semver</a> to maintain consistency. Locally, developers can use any available version by specifying it explicitly:</p><a href="https://medium.com/media/76cc00fa804c90c8f610164d37f1b13e/href">https://medium.com/media/76cc00fa804c90c8f610164d37f1b13e/href</a><p>In CI, we run:</p><a href="https://medium.com/media/99bd2ef0767844893b01c4cfe4c25608/href">https://medium.com/media/99bd2ef0767844893b01c4cfe4c25608/href</a><p>This ensures that CI always uses the latest version of codemods, instantly applying all new transforms and bug fixes.</p><p>Keeping the @lyft/codemod package separate from our internal libraries has also been key. It allowed us to develop and iterate quickly, without being blocked by or introducing changes to library code.</p><h4>Testing</h4><p>To test our changes before it goes out we use <a href="https://github.com/facebook/jscodeshift?tab=readme-ov-file#definetest">defineTest</a> from jscodeshift because it makes testing codemods simple and readable. It compares two fixture files — one before and one after the transform — to verify that the changes work as expected.</p><p>For example, transform-buttons fixtures structure and content would look like this</p><a href="https://medium.com/media/c9d0a5e6c3810ebaf99eea317626e157/href">https://medium.com/media/c9d0a5e6c3810ebaf99eea317626e157/href</a><a href="https://medium.com/media/1336d832b3ca3c601b1e5ea54fe47b30/href">https://medium.com/media/1336d832b3ca3c601b1e5ea54fe47b30/href</a><a href="https://medium.com/media/e9d250763f0b3dc90e7c81163bd3746f/href">https://medium.com/media/e9d250763f0b3dc90e7c81163bd3746f/href</a><p>Making it very easy to maintain and read by developers!</p><p>Another useful tool for testing and writing codemods is <a href="https://astexplorer.net/">AST Explorer</a>, which we use frequently. Since jscodeshift transforms code into an AST before modifying it, using the right API and node types is crucial.</p><p>AST Explorer makes this easy by providing a visual representation of the AST and a real-time editor to experiment with transformations. For example, here’s how console.log(‘Hello, World’) looks in AST world:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*PL_VLsl2-3Jg1RnIVltZNQ.png" /></figure><p>Even though the AST might look complicated at first, it is a powerful tool that can help you analyze and transform the source code in a structured way. As you can see, each node has a type, and you can traverse the tree to find the nodes and methods you are interested in.</p><h4>Outcome</h4><p>So, what did we actually get out of this? Was all the effort really worth it? Let’s break it down!</p><p><strong>Converted multiple Web Platform releases from major to minor</strong></p><p>By fully automating breaking changes, we turned what would have been major releases into minor ones. Even better, for minor updates, we made Refactorator pull requests (PRs) fully auto-mergeable, meaning engineers no longer had to review PRs, read docs, or manually test changes.</p><p><em>At Lyft, Refactorator is our internal tool that automates large-scale code changes by generating and managing PRs across all projects.</em></p><p>With 100+ FE microservices and multiple releases per year, this cut down on tedious manual work, saved thousands of developer hours, and made upgrades a lot smoother.</p><p><strong>Integrated codemods into automatic dependencies upgrade</strong></p><p>At Lyft, we already had a system for automating npm dependency upgrades, but version bumps alone weren’t enough — breaking changes still needed manual fixes. By integrating the Codemods CLI into the process, upgrade PRs now do more than just update dependencies. They fix breaking changes and adopt new features automatically, all in a single PR.</p><p>For example, automating 80% of breaking changes in our components library significantly boosted adoption, leading to ~30% of microservices migrating within 2 weeks — something that would have previously taken months. This not only made upgrades smoother but also gave developers a way to write their own codemods, making it easier to contribute meaningful changes and grow their impact.</p><p>Codemods have now been executed in thousands of dependency upgrades to fix breaking changes and support new feature adoption. This automated process has helped reduce the total number of outdated dependencies across microservices by over 1,000. While this number is dynamic — since new library versions are released daily — the combination of automated package upgrades and codemods has had a lasting impact on keeping our ecosystem up to date.</p><h4>Future plans</h4><p>To make an even bigger impact, we’ve built a set of cleanup codemods that run on a schedule across FE services. These handle tasks like removing duplicate TypeScript compiler options, cleaning up redundant ESLint rules already covered by our base config, or eliminating unexpected duplicate dependencies in FE services.</p><p>We’re expanding codemods into this space as well. So far, we’ve created 40+ transforms in just a year, and we’re just getting started.</p><p>Looking ahead, we plan to make codemods even more accessible by integrating them into local development workflows and CI pipelines, giving engineers early feedback. We also plan to explore AI-assisted codemods that can suggest or even generate code transformations based on diff patterns, upgrade guides, or documentation. This could further reduce engineering effort and unlock new levels of automation in how we maintain and modernize our codebase.</p><h4>Acknowledgments</h4><p>Many thanks to my team — Diana Cubas, Jonatan Santa Cruz, Betsabe Ortegon, Alfredo Campos Tams, Allen Arturo Jimenez — for continuously improving this platform and adding more transforms to automate manual work.</p><p>Additionally, thanks to Glen Cheney and Mario Garcia for automating breaking changes in some libraries and creating new helper functions used across transformations!</p><p><em>Lyft is hiring! If you’re passionate about developer tooling and automation, visit </em><a href="https://www.lyft.com/careers"><em>Lyft Careers</em></a><em> to see our openings.</em></p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=74c4f9df4680" width="1" /><hr /><p><a href="https://eng.lyft.com/from-manual-fixes-to-automatic-upgrades-building-the-codemod-platform-at-lyft-74c4f9df4680">From manual fixes to automatic upgrades — building the Codemod Platform at Lyft</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/dab49f5b27c7</id>
            <title>FacetController: How we made infrastructure changes at Lyft simple</title>
            <link>https://eng.lyft.com/facetcontroller-how-we-made-infrastructure-changes-at-lyft-simple-dab49f5b27c7?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/dab49f5b27c7</guid>
            <pubDate></pubDate>
            <updated>2025-02-24T15:48:48.773Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><em>Written by </em><a href="https://www.linkedin.com/in/miguel-molina-594475120/">Miguel Molina</a> and <a href="https://www.linkedin.com/in/avsubramanian">Arvind Subramanian</a></p><figure><img alt="Logo for FacetController" src="https://cdn-images-1.medium.com/max/1024/0*UL3sTV2r4p46S2zQ" /><figcaption>FacetController</figcaption></figure><p>If you are curious about Lyft’s automatic deployment process on a higher level, please read our blog post on <a href="https://medium.com/lyft-engineering/continuous-deployment-at-lyft-9b457314771a">Continuous Deployment</a>.</p><p>In this post, we will go a little deeper into the deployment stack and how we leverage Kubernetes <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resource Definitions</a> (CRDs) to create an abstraction on top of Kubernetes native resources, known at Lyft as facets. Additionally, we will discuss the new controller we developed to manage these facets and to streamline infrastructure rollouts across the company.</p><h3>What are facets?</h3><p>When deploying code, each Lyft microservice is composed of smaller deployable Kubernetes components called facets. There are several facet types representing different deployable Kubernetes objects, and they are defined in a generic manifest.yaml file within each project’s repository.</p><p>The following are some of the facet types we have at Lyft:</p><p><strong>Service facets</strong></p><p>These facets receive and send traffic, typically web servers containing APIs for a microservice. In this example the service facet will have different autoscaling min and max sizes per environment, and the HPA will scale up when CPU utilization reaches 70%.</p><pre>- name: webservice<br />  container_command: go run main.go <br />  type: service<br />  autoscaling:<br />     criteria:<br />         cpu_target: 70<br />     environment:<br />         staging:<br />             min_size: 5<br />             max_size: 20<br />         production:<br />             min_size: 5<br />             max_size: 200</pre><p>This metadata ensures that the Kubernetes resources for a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>, <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a>, <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a>, <a href="https://kubernetes.io/docs/concepts/configuration/configmap/">Configmap</a> and an <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">HPA</a> are created.</p><p><strong>Worker facets</strong></p><p>These facets only send traffic and typically do some offline processing of work, like taking items from a queue and performing some action.</p><pre>- name: offlineworker<br />  container_command: somestartupcommand.sh<br />  type: worker<br />  autoscaling:<br />     min_size: 1<br />     max_size: 1</pre><p>This metadata ensures that the Kubernetes resources for a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>, <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a>, <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a>, <a href="https://kubernetes.io/docs/concepts/configuration/configmap/">Configmap</a> and an <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">HPA</a> are created.</p><p><strong>Cron facets</strong></p><p>These facets run workloads on a schedule. For example, once a week on Sunday.</p><pre>- name: mycron<br />  container_command: somestartupcommand.sh<br />  type: cron<br />  schedule: 0 0 * * SUN</pre><p>This metadata ensures that the Kubernetes resources for a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJob</a> are created.</p><p><strong>Job facets</strong></p><p>These facets run workflows once at deployment time and then gets terminated and deleted.</p><pre>- name: s3uploadjob<br />  container_command: upload_data_to_s3.py<br />  type: job</pre><p>This metadata ensures that the Kubernetes resources for a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Job</a> are created.</p><p><strong>Batch facets</strong></p><p>These facets contain code for a workflow that can be invoked by the user whenever an action is needed. For example, running a DB migration.</p><pre>- name: dbmigrationbatch<br />  container_command: migration.py<br />  type: batch</pre><p>This metadata ensures that the Kubernetes resources for a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Job</a> are created.</p><p><strong>Deploying Facets</strong></p><p>Lyft developers can reference and target facets in their microservice’s deploy pipeline with a controlled rollout. This allows a deploy step to target an environment or a specific percentage of that environment as well as target individual facets of the service using the target_facets field. For more details on pipeline structures, refer to the <a href="https://eng.lyft.com/continuous-deployment-at-lyft-9b457314771a">Continuous Deployment</a> blog.</p><p>For example, a deploy pipeline might look like this:</p><pre>deploy:<br /> - name: staging<br />   automatic: true<br />   environment: staging<br />   target_facets: [webservice, offlineworker, dbmigrationbatch]<br /> - name: canary<br />   environment: production<br />   bake_time_minutes: 10<br />   automatic: true<br />   target_facets: [webservicecanary]<br /> - name: one-third-of-production<br />   environment: production<br />   bake_time_minutes: 10<br />   automatic: true<br />   target_facets: [webservice]<br /> - name: production<br />   environment: production<br />   automatic: true<br />   target_facets: [webservice, offlineworker, dbmigrationbatch, s3uploadjob, mycron]</pre><h3>Problems</h3><p>Early on during Lyft’s migration to Kubernetes (2019–2020), the infrastructure was rapidly evolving how Kubernetes deployments and manifests were configured (the templates defining deployments changed <em>a lot!</em>). At the time, any updates to these templates could only be propagated when a new deployment was triggered.</p><p>At deployment time, the system reads the project manifest, translates it into relevant Kubernetes objects (Deployments, ConfigMaps, HPAs, RoleBindings, ServiceAccounts, etc.), and applies these objects to the relevant Kubernetes clusters. We used to template Kubernetes files at deploy time, where user defined configuration was used in combination with some static logic, similar to helm-style deployments.</p><p>With over one thousand microservices, each containing a number of facets, this meant thousands of deployments / redeployments were needed to update facet objects with any template change. Hence, any major changes to environment variables, scaling, or other configurations were difficult to track rollout of and fully converge across all services, and also equally difficult to roll back in an emergency.</p><p>Each time we needed to add a field to a facet type or have any infrastructure wide migration, this required heavy manual tracking, like using a spreadsheet to know what still needed deployment, and required lots of coordination with every service team. This process would typically take many weeks or months to make any change to a template, like simply adding or renaming a field.</p><p>These problems all highlighted that we lacked a high level Custom Resource Definition (CRD) for deployable objects and a way to manage them. So we introduced FacetController.</p><h3>Solution: FacetController</h3><p>FacetController manages the lifecycle of facets. Instead of applying all the Kubernetes objects mentioned above, the deploy process will now create or update a singular facet (ex. ServiceFacet, WorkerFacet) resource, configured as a Custom Resource Definition, on a Kubernetes cluster. The facet resource closely resembles the metadata that is exposed to our developers in deployment manifests. When facet specs are updated or during a regular deployment on the cluster, FacetController will pick up this change, create/update the associated child resources (ex. Deployments, ConfigMaps), and delete resources that are no longer required. This allows changes on <em>how</em> these child resources are defined to be quickly and easily propagated to all services at Lyft.</p><p>Now all that is required when changing the templates is a deployment of FacetController instead of individually deploying each service at Lyft. Facecontroller effectively saved every infrastructure team from spending multiple quarters on migrations that now only take a few weeks to fully rollout and test safely.</p><figure><img alt="FacetController’s Sync loops architecture design" src="https://cdn-images-1.medium.com/max/960/0*hsoK0RpYe_DwXs-F" /><figcaption>Design of FacetController</figcaption></figure><h3>Infrastructure Management is way easier</h3><p>The biggest benefit of FacetController is that it has given us a way to drive sweeping changes to user services safely and ensures the changes happen in an automated fashion. Some examples:</p><h3>Changes to Underlying Infrastructure</h3><p><em>Autoscaling Changes (</em><a href="https://github.com/kubernetes/autoscaler"><em>Kubernetes/autoscaler</em></a><em> to </em><a href="https://karpenter.sh/"><em>Karpenter</em></a><em>)</em></p><p>FacetController enabled our migration to use Karpenter instead of Cluster AutoScaler to manage how our nodes get packed with pods and balanced over time. It allowed us to slowly and safely select projects for deployment to Karpenter-managed nodes by using labels added through FacetController.</p><p><em>Kubernetes</em> <em>Upgrades</em></p><p>As Lyft’s infrastructure has evolved, some Kubernetes clusters are pending deprecation and are running older versions of Kubernetes while other clusters are running newer versions. Even though older clusters may rely on deprecated APIs, FacetController allows for managing different cluster versions by generating the appropriate resources based on each cluster’s specific API version.</p><h3>Changes to Developer Experience</h3><p><em>CPU limits removal</em></p><p>Removal of CPU limits allowed Lyft services to eliminate CPU throttling and let our most critical services burst when needed. The benefits of this has been extensively talked about by others, so here are some articles that explore this topic in more detail: <a href="https://medium.com/@jettycloud/making-sense-of-kubernetes-cpu-requests-and-limits-390bbb5b7c92">Making Sense of Kubernetes CPU Requests And Limits | by JettyCloud | Medium</a>, <a href="https://medium.com/directeam/kubernetes-resources-under-the-hood-part-3-6ee7d6015965">Remove your CPU Limits | by Shon Lev-Ran | Directeam</a>, and <a href="https://danluu.com/cgroup-throttling/">The container throttling problem | Dan Luu</a>.</p><p>Stay tuned for a future blog post on how removing CPU limits unblocked many cost savings initiatives.</p><p><em>Scaling on service container CPU</em></p><p>At Lyft we run many sidecar containers (<a href="https://www.envoyproxy.io/">envoyproxy</a>, stats, logging, etc.) on each pod. CPU for the pod can sometimes be deceiving as the sidecars can skew the average CPU utilization of the pod but the application might be running hotter. This made us realize the importance of also scaling on application container CPU, and we now use the max of the application container CPU and the pod’s overall CPU to have more accurate scaling.</p><h3>FacetController’s Net Benefits</h3><p><em>Proper abstraction for facets and their templates</em></p><p>With FacetController, we now have one unified codebase to manage the lifecycle of facets instead of disparate systems that require individual updates. This consolidation means we now we have one resource to interact with for tools that modify facets (ex. our internal developer platform, command line tools) instead of multiple resources that could diverge between the old tools.</p><p><em>Automatic Garbage Collection (GC) of resources</em></p><p>Before, when deprecating a facet for a service, we would have to manually delete all the objects from that facet, such as the ConfigMap, K8s service, K8s Deployment, etc. Now with FacetController, because each facet has their standard interface/template and management, all of these are automatically GC’d when a facet is removed from a project’s manifest.</p><p><em>No need for en-masse redeploys of services for an infrastructure-level change</em></p><p>This process used to require coordination with service owners and having to re-deploy thousands of services, which would take multiple months for a change to the facet spec. Now most infrastructure-level changes can take minutes to take effect but can still be done in a controlled manner with rollout flags when percentage based rollout is required. This has saved infrastructure engineers many months of work.</p><p><em>Safe rollout of infrastructure-level changes</em></p><p>Despite having changes applied outside of the service’s deployment pipeline, we kept safety as a top priority in the design of how to deploy FacetController. Changes get rolled out on a per-cluster basis and can even be done to select services within a cluster given that we run multiple Kubernetes clusters at Lyft for availability.</p><p>Another safeguard we implemented is that concurrent updates to facets are limited to reduce the impact of problematic changes and being able to throttle updates.</p><p><strong>Future work</strong></p><p>We have fully adopted the controller pattern in different areas of our Kubernetes platform, creating others that use FacetController as an example to design controllers that manage and automate other parts of our infrastructure.</p><p>Some services at Lyft require additional resources and configurations outside of the provided templates, often for reasons such as using open source configuration. We refer to these as Direct Facets because they directly apply template files to Kubernetes. These exempt services do not use FacetController and therefore do not get the benefits mentioned above. However, we are actively working on adding generic support for these services so that they can leverage the platform.</p><p>….</p><p>Special thanks to all the people that contributed to the blog post and FacetController over the last few years: <a href="https://www.linkedin.com/in/mikecutalo/">Mike Cutalo</a>, <a href="https://www.linkedin.com/in/tuong-la">Tuong La</a>, <a href="https://www.linkedin.com/in/daniel-metz-3079b248/">Daniel Metz</a>, <a href="https://www.linkedin.com/in/frank-porco-1617b827">Frank Porco</a>, <a href="https://www.linkedin.com/in/tomwans/">Tom Wanielista</a>, and <a href="https://www.linkedin.com/in/yannramin/">Yann Ramin</a>.</p><p><em>Lyft is hiring! If you’re passionate about Kubernetes and building new controllers or using FacetController, visit </em><a href="https://www.lyft.com/careers"><em>Lyft Careers</em></a><em> to see our openings.</em></p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=dab49f5b27c7" width="1" /><hr /><p><a href="https://eng.lyft.com/facetcontroller-how-we-made-infrastructure-changes-at-lyft-simple-dab49f5b27c7">FacetController: How we made infrastructure changes at Lyft simple</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/a11aff6e670f</id>
            <title>Using Marketplace Marginal Values to Address Interference Bias</title>
            <link>https://eng.lyft.com/using-marketplace-marginal-values-to-address-interference-bias-a11aff6e670f?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/a11aff6e670f</guid>
            <pubDate></pubDate>
            <updated>2025-01-27T18:49:49.995Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><em>Written by </em><a href="https://www.linkedin.com/in/shima-nassiri-phd-7a030826/"><em>Shima Nassiri</em></a><em> and </em><a href="https://www.linkedin.com/in/ido-bright-a1808429/"><em>Ido Bright</em></a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*JWRGG-Tys_wnbNot" /></figure><h4><strong>Network Effect</strong></h4><p>At Lyft, we run various randomized experiments to tackle different measurement needs. User-split experiments account for 90% of the randomized studies due to the higher power and fit for most use cases. However, they are prone to interference or network bias. In a multi-sided marketplace, there is no such thing as a <em>perfect balance</em> of supply and demand and one side of the market is congested: if we have oversupply, we can run rider-split experiments without interference concerns. If we are undersupplied, however, interference in a rider-split experiment can severely bias the results. Same goes for under or over-demand situations and driver-split experiments. For example, in a supply constrained situation, not enough drivers are available to address the demand. As illustrated in Figure 1, in such an environment, if the treatment in an A/B experiment incentivises higher convergence of riders to complete their intended rides, there will be fewer resources available for the riders in the control group. Hence, the outcomes of the control group will be negatively impacted by the treatment through the congested resource (i.e., the drivers) and the impact of treatment can be overestimated — this is known as interference bias or network effect. This situation violates the <a href="https://blogs.iq.harvard.edu/violations_of_s">Stable Unit Treatment Value Assumption (SUTVA)</a> which indicates that the control group should not be affected by the treatment to keep the results unbiased.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/666/0*Hhe-N6iUBGcQIfx4" /><figcaption><strong>Figure 1: </strong>Network effect in an undersupply situation</figcaption></figure><p>It’s important to recognize that interference doesn’t always lead to an overestimation of the treatment effect. For instance, in social networks, treating units in the treatment group can positively influence the outcomes for control units who are friends with those treated, boosting control outcomes and thus reducing the perceived treatment effect. Similarly, in a retail setting, with complementary products, treating units in the treatment group might positively impact complementary products often purchased together, inflating control outcomes and underestimating the treatment effect. Conversely, for substitutable products, the opposite occurs, where the treatment effect may be overestimated.</p><h4><strong>Possible Solutions to Interference</strong></h4><p>Much of the literature on interference focuses on modifying classical experimental designs to mitigate its effects. Cluster randomization is a popular method for addressing interference. For instance, at Amazon, cluster randomization is explored to tackle interference issues among substitutable products. In Section 4 of <a href="https://link.springer.com/epdf/10.1057/s11369-023-00303-9?sharing_token=sqt919p0mbXyzpY-5ZFkUVxOt48VBPO10Uv7D6sAgHtEG2O9voG9dfzv7n7ZMm6_MRamE4DqbUZCAqlQ9PkTEiUYGvozBcKwMGKY-8gB4O4IflB15aojB9iX8W_GeqZKW_TgvQsJr94TxT-cPzM79PUoWaQAimTlxuy1fo5cW3A%3D">Cooperider and Nassiri (2023)</a>, the authors also address the challenge of low power resulting from such clustering and discuss how power can be improved through better cluster balancing.</p><p>Other alternative designs like time-split or region-split experiments can also be used to address interference. In a time-split experiment <em>all</em> units are exposed to a single treatment at any given time or time-location combination, which helps prevent the interference effect. (This type of experiment is also known as <a href="https://www.hbs.edu/ris/Publication%20Files/WP21-034_20160b13-a86c-4a0d-b6e9-bbae288486c5_c93009c0-8003-43fd-bb1a-012c02d33b98.pdf"><em>switchback</em></a>). However, this approach can affect the user experience for user-facing changes. For example, if we frequently toggle a UI feature that provides the driver with more rider information, it might disrupt the user experience. Additionally, time-split experiments are inherently suited for scenarios where the focus is on the overall marketplace impact. They are designed to capture short-term marketplace behavior, as users experience different treatments throughout the experiment. However, it’s not possible to include a holdout group in a time-split experiment, making them unsuitable for assessing long-term impacts. Therefore, time-split experiments are suitable only for a limited range of use cases. Experimenters might opt to run a combination of a time-split experiment followed by a user-split experiment to leverage the strengths of both approaches. This strategy allows them to accurately gauge marketplace-level effects without interference concerns through the time-split, while also assessing user-level, long-term impacts via the user-split. However, this approach is costly to implement and can delay decision making by several weeks.</p><p>On the other hand, <a href="https://artofmarketingscience.github.io/what-are-geo-experiments/">region-split or geo experiments</a> apply a treatment across an entire region or region-time bucket, effectively eliminating interference bias since significant interference across different regions is unlikely. Additionally, they don’t impact user experience. However, region-split experiments often suffer from low statistical power due to smaller effective sample sizes, which limits their large-scale adoption.</p><p>Another method to obtain unbiased treatment effect estimates despite interference is by modeling interference. Interference can be a challenge in two types of marketplaces: choice-based (e.g., Airbnb and Amazon) and match-based (e.g., Lyft and Doordash). In choice-based marketplaces, customers select from multiple options, making it more complex to model the interference. In contrast, match-based marketplaces assign customers to a single option, which simplifies the modeling of interference. At Lyft, we use a Marketplace Marginal Values (MMV) approach for modeling interference. You can find the theoretical details of this approach in <a href="https://arxiv.org/pdf/2205.02274">Bright et al. (2024)</a>. Essentially, MMV represents the change in the gain (which can be whatever you are optimizing for, e.g., more profit or rides) as a result of changing the resource (additional supply/demand) by one unit. This concept is commonly known as the <em>shadow price</em> in the operations research literature.</p><h4><strong>Why MMV?</strong></h4><p>In the paper, the authors present technical proofs demonstrating how marginal values can help significantly reduce the estimator bias of the treatment effect. Essentially, the primary source of interference bias as previously mentioned, is the competition for limited resources. Marginal values effectively capture this resource contention. Consider the following situations:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/226/0*BGmJLZLA-S-kNlW8" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/220/0*2-0ryrE--K5DAQIN" /><figcaption><strong>Figure 2: </strong>marginal vs. face value of a rider</figcaption></figure><p>As illustrated in Figure 2, when supply is abundant, the marginal value of having rider R1 matches its face value, which is $6. However, in a low supply scenario where resources are limited, the resource is allocated to rider R2. Consequently, both the marginal and face values for R1 become zero. For rider R2, the face value is $10, but its marginal value is only the additional $4 gained by having rider R2. This demonstrates how the marginal value inherently accounts for resource contention. By aggregating the marginal values across both the treatment and control groups and calculating the difference, one can derive an unbiased estimator of the average treatment effect.</p><h4><strong>How to compute MMVs?</strong></h4><p>As previously mentioned, shadow prices in the dispatch optimization problem can be used to obtain the MMVs. The primal dispatch problem can be described as follows:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/578/1*rsMnV1aaHOfDavoX6zMiLA.png" /></figure><p>Where <em>xᵢⱼ </em>is a variable that takes the value of 1 if the driver <em>j</em> got matched to that rider <em>i</em>, and 0 otherwise. <em>πᵢⱼ</em> represents the score (e.g., profit) of matching driver <em>j</em> to rider <em>i</em>. The first constraint ensures that a driver is matched with at most one ride per a matching cycle (more on this later), and the second constraint indicates that a rider can have at most one driver. Solving this optimization gives the optimal matching of drivers to riders. We can relax the last constraint into <em>xᵢⱼ ≥ 0</em>, and obtain a linear relaxation of the above problem for which we can compute the dual as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/546/1*iWfKDkLNaQjy_n_ND_S7eQ.png" /></figure><p>The dual variable <em>μⱼ</em> is associated to the driver constraint (first primal constraint), and <em>λᵢ</em> is associated with the rider constraint. This means that for each driver <em>j</em>, there is an associated dual variable <em>μⱼ</em> (same is true for riders). More on duality can be found <a href="https://en.wikipedia.org/wiki/Duality_(optimization)">here</a>. To find the MMV values, we aim at generating a matching cycle dispatch graph, solve it, and then efficiently compute the incremental values via the duals. Consider the objective function, denoted as <em>Π(d,s)</em>, where <em>d</em> and <em>s</em> represent the demand and supply respectively. Assume that the treatment effect results in increasing the demand by <em>e</em>. Then the global effect of such treatment can be presented as follows:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/456/1*Ebhi4iuPIuZB_1E_EL7XRg.png" /></figure><p>Now to estimate <em>Δ</em>, we can do a rider-split 50/50 A/B test where each group serves half the demand. Consider the demand in each group being presented by <em>dₑ</em>. We then have</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*f31qkY3fGl1_qfRNp60Tdg.png" /></figure><p>The global average treatment effect can then be estimated as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-KJr-iOId9TPVQYWZQYqzQ.png" /></figure><p>where <em>λ*</em> is the optimal rider dual or shadow price. Here, the analysis provides the first order Taylor approximation results — for more details see <a href="https://arxiv.org/pdf/2205.02274">Proposition 5 of Bright et al. (2024)</a>. We can observe that the difference in the objective function outcomes across treatment and control groups can be presented by the <a href="https://en.wikipedia.org/wiki/Shadow_price"><em>shadow price</em></a>. In the paper, the authors further did a simulation and showed this shadow price estimator will hamper the overestimation of the default estimates from standard A/B tests while lowering the noise level (refer to <a href="https://arxiv.org/pdf/2205.02274">Figure 8 in Bright et al. (2024)</a>).</p><h4><strong>Matching Cycle</strong></h4><p>Next, we need to decide how often to solve these optimization problems, essentially determining the length of the matching cycle. If the matching cycle is too short, contention can occur between cycles. For instance, a driver who isn’t matched in Cycle 1 might be available in the next cycle, or a rider choosing the wait-and-save option might wait several cycles before being matched. At Lyft, we use a 1-hour <em>mega cycle</em> to solve the dispatch optimization problem for all eligible riders and drivers within that period. This cycle length helps significantly reduce concerns about contention between cycles.</p><h4><strong>Secondary Metrics</strong></h4><p>Finally, if we want to assess the MMV-corrected impact of a treatment on metrics beyond those defined by the dispatch objective function, we can compute the edge or ride cost for each completed ride (e.g., <em>νᵢⱼ</em>). Considering a linear relaxation of the primal problem and applying complementary slackness, we have:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Nz51nLNFd3Nq7SsoPVm2LQ.png" /></figure><p>Assuming non-degeneracy, we can then solve the above system of equations to find the optimal dual values and use them to estimate the average treatment effect same as before.</p><h4><strong>Productionalizing MMV in an Experimentation Platform</strong></h4><p>To implement MMV in an experimentation platform, we solve the matching optimization problem for passenger and driver duals on an hourly basis, as previously described, and store these values in a table. This data is then used to calculate MMV-corrected values for drivers and riders. These metrics are included in experiment reports alongside other metrics, with standard computations like <a href="https://amplitude.com/explore/experiment/cuped-explanation-guide">CUPED</a> applied to them. Below is an example of how an MMV-corrected metric might appear in a driver randomized experiment. Here, riders are the congested resource contributing to the interference bias, and over-estimation of the results as discussed earlier. The MMV correction would hamper the effect by accounting for the contention over the limited resource (in this case pool of riders).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*yAbxwJoKV0eG_rKY" /></figure><p>It’s important to note that there are limitations to the use cases for MMV. For instance, MMV cannot be applied in situations where the target population for randomization is not drivers or passengers. An example of this would be mapping experiments where the route is associated with the ride itself, rather than being specific to drivers or passengers, making MMV-corrected metrics unsuitable.</p><h4><strong>MMVs in Practice</strong></h4><p>Other experimental designs like time-split, region-split, or a combination of time- and user-splits often fall short in addressing the majority of experimentation needs. They tend to lack sufficient power, are costly to implement, and can take several weeks to execute. In contrast, the MMV approach can adjust the effect sizes in user-split experiments where interference is a concern. This is particularly important in cases with significant network effects, as the change in effect magnitude can be substantial, potentially altering launch decisions under MMV correction.</p><p>At Lyft, we’ve had instances where both time- and user-split experiments were conducted for the same initiative to capture both market-level and long-term effects. We compared the MMV-corrected user-split outcomes with the time-split outcomes in three historical cases where a time-split counterpart was available. After applying MMV correction, we observed greater alignment with the time-split results.</p><p>Additionally, a comprehensive backtest across various user-split experiments was conducted, comparing MMV-corrected completed rides with traditional metrics. In 10% of these comparisons, the launch decision could change when using MMV-corrected values. These cases were evenly split between false positives (launching based on traditional values when MMV-corrected values didn’t meet launch criteria) and false negatives.</p><p>Moreover, when MMV results show a lower magnitude compared to naive user-split results, particularly in resource-constrained experiments, we anticipate an average 45% reduction in outcome magnitude based on this numerical study. This decrease occurs because the contribution of each ride is divided between the rider and the driver when calculating marginal values, thus correcting for the overestimation of effects due to interference bias.</p><h4>Acknowledgements</h4><p>We would like to thank Anahita Hassanzadeh and Thu Le for helpful discussions and suggestions.</p><p><em>Lyft is hiring! If you’re passionate about experimentation and measurement, visit </em><a href="https://www.lyft.com/careers"><em>Lyft Careers</em></a><em> to see our openings.</em></p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a11aff6e670f" width="1" /><hr /><p><a href="https://eng.lyft.com/using-marketplace-marginal-values-to-address-interference-bias-a11aff6e670f">Using Marketplace Marginal Values to Address Interference Bias</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/6f6b7be099a7</id>
            <title>Cartography joins the CNCF</title>
            <link>https://eng.lyft.com/cartography-joins-the-cncf-6f6b7be099a7?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/6f6b7be099a7</guid>
            <pubDate></pubDate>
            <updated>2024-12-18T17:48:35.207Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*d-AY3SOlTPlB9gfl" /></figure><p><em>Written by </em><a href="https://www.linkedin.com/in/alexchantavy/"><em>Alex Chantavy</em></a></p><p>Today we’re thrilled to announce that Lyft has donated <a href="https://github.com/cartography-cncf/cartography">Cartography</a> to the Cloud Native Computing Foundation (<a href="https://www.cncf.io/">CNCF</a>). Since Lyft open sourced it in 2019, it’s been rewarding to see the project grow from an experimental tool to a solution that’s been battle-tested in production by multiple companies. In this post, we’ll reflect on our learnings from Cartography’s open source journey and discuss where it’s going next.</p><p><strong>Origins and growth</strong></p><p>Cartography was first created to find attack paths that a malicious actor could take to compromise data in cloud environments. We first used it to understand <a href="https://eng.lyft.com/iam-whatever-you-say-iam-febce59d1e3b">complicated IAM permissions</a> so that we could think like an attacker and identify the shortest path to administrator privileges.</p><p>We soon realized that this graph capability was equally valuable for defenders. It also allowed us to quickly answer questions like, “Which of my services are internet-facing and running a vulnerable version of a software library?” or “Which directors at the company own the most security risk?”</p><p>In 2020, we chose to use Cartography as the backbone for our <a href="https://eng.lyft.com/vulnerability-management-at-lyft-enforcing-the-cascade-part-1-234d1561b994">vulnerability management program</a> because it helped us contextualize risks across our infrastructure in a way that no other tools did. As I talked about <a href="https://www.youtube.com/watch?v=F4EFHK21Et0">at BSidesSF</a>, this was not an easy or smooth journey but it forced us to quickly mature the tool and improve our correctness, stability, and performance.</p><p><strong>Lessons learned from open source</strong></p><p>Through all this, we built a community and got to meet many of you. <a href="https://x.com/mattklein123">Matt Klein</a>’s advice on managing an open source project (<a href="https://www.youtube.com/watch?v=8laijEpZuo8">1</a>, <a href="https://mattklein123.dev/2021/09/14/5-years-envoy-oss/#End-user-driven-OSS-is-a-structural-advantage">2</a>) was extremely helpful, and I’ll summarize some of my own recommendations for those considering open sourcing a project:</p><ul><li><strong>Open source is a big commitment.</strong> It’s like building a global engineering team. Define clear goals, set expectations for support, and remember that open source is usually a <a href="https://www.youtube.com/watch?v=EWTvfEF55wo">net negative</a> for companies unless there’s large adoption.</li><li><strong>Communicate and make decisions openly</strong>. Let the community have buy-in and understand your project’s direction. Start a public chat channel and hold regular community video calls.</li><li><strong>Appoint external maintainers</strong>. To avoid burnout, find regular contributors who align with your vision. Allow them to review and merge PRs (we actually made one of our key hires for Lyft’s security team this way!).</li><li><strong>Documentation is everything</strong>. Let others self serve and unblock themselves to avoid friction when trying out and learning your project.</li><li><strong>Set a clear standard: passing tests = mergeable PR.</strong> Encourage contributions by setting clear expectations. Include checklist templates, use linters, and implement robust automated tests. Do what you can to avoid PR authors becoming frustrated and leaving the project behind.</li><li><strong>Understand your project’s niche</strong>. Focus on its comparative advantages and avoid trying to be everything for everyone.</li></ul><p><strong>The impact of open sourcing Cartography</strong></p><p>Cartography has grown to over 300 Slack members, 90 committers to the main source branch, and over a dozen companies adopting it (that we know of). It’s been incredibly rewarding to see this growth, and it was cool to see how community members <a href="https://blog.marcolancini.it/2020/blog-tracking-moving-clouds-with-cartography/">built alerting</a> around it or tried to experiment with <a href="https://gigi.nullneuron.net/gigilabs/migrating-cartography-to-memgraph/">other backend tech</a>. Having a good open source presence helped Lyft source candidates, and as mentioned above, we were even able to make a key hire from the community. Cartography’s open source status enabled our vuln management and auditing programs to run smoother internally at Lyft as the community often encountered and fixed bugs before we did. We also benefited from dozens of community-contributed modules, many of which were from former Lyft employees and it was nice being able to collaborate with them even after they had changed companies.</p><p><strong>The path to the CNCF</strong></p><p>Over the past nearly 6 years working on Cartography, I believe more and more that having a self-maintaining map of your infra is a superpower and I can’t imagine working anywhere without it. I’d like to see a world where having a “Cartography-like” graph representation of infra assets becomes something of an open standard, especially since modern companies must maintain visibility over an ever-growing plethora of providers and tools.</p><p>However, one of the realities of running an open source project is that contributors (understandably) come and go with time. A successful project needs a steady stream of people discovering it, making contributions, and becoming maintainers. It became clear that growing the project wasn’t going to be possible over the long run if its steward was just one company.</p><p>In August 2023, we <a href="https://github.com/cncf/sandbox/issues/58#issue-1870643644">applied</a> to donate Cartography to the CNCF. We pursued the CNCF in particular because Cartography was built to solve security problems that are uniquely complicated in cloud-native environments.</p><p>By donating the project we hope to:</p><ul><li>Demonstrate Cartography’s commitment to being fully open source and supported over the long term.</li><li>Improve its reach by showing it has achieved a high level of maturity and can be trusted in production.</li><li>Receive logistics help in hosting the project. The foundation provides resources such as web hosting, video conferencing, Slack, GitHub continuous integration services, and others.</li></ul><p>After a long, thorough review, Cartography was finally <a href="https://github.com/cncf/sandbox/issues/58#issuecomment-2318558538">accepted</a> by the foundation in August 2024 — big thanks to the Technical Oversight Committee and CNCF staff for shepherding the project through the vote and onboarding it!</p><p><strong>The future</strong></p><p>Now that Cartography is a CNCF project, what does this mean? The only practical differences are that our Slack channel is now hosted by CNCF instead of Lyft, and our GitHub URL is now slightly different: <a href="https://github.com/cartography-cncf/cartography">https://github.com/cartography-cncf/cartography</a>. Cartography will still be developed and led by those who are interested, i.e. its community members. If this project seems useful to you, please try it out and say hi in the #cartography channel on the CNCF <a href="https://communityinviter.com/apps/cloud-native/cncf">Slack</a> — we’d love to hear your feedback. If you think someone else would find Cartography useful, please also share it with them. There are lots of new technical directions I’d like to explore in the future but we can only do this if the community continues to grow and be supported — future blog post to come!</p><p>Working on open source has been a career highlight for me, and I like to think that we’ve done at least a little to help the information security industry <a href="https://x.com/JohnLaTwC/status/1059841882086232065">think in graphs and not lists</a>.</p><p><strong>Thank you</strong></p><p>Special thanks to the leadership of Lyft’s security team who have been instrumental in their support of Cartography in this multi-year open source journey: Sacha Faust (Cartography’s original creator), Chaim Sanders, Nico Waisman, Matthew Webber, Ben Stewart, Martin Conte Mac Donnell, Samantha Davison, and Jason Vogrinec.</p><p>Thanks to Andrew Johnson, Taya Steere, and Evan Davis for taking Cartography from 0 to 1.</p><p>Thanks to those who helped take the project to the next level in production through vuln management and infra scenarios: Eryx Paredes, Zoe Longo, Jason Foote, Sergio Franco, Khanh Le Do, Aneesh Agrawal, Leif Raptis-Firth, Hans Wernetti, Fernando Zarate, Kunaal Sikka, Gaston Kleiman, and Lynx Lean.</p><p>Thanks to the maintainers and friends of Cartography: Ramon Petgrave, Chandan Chowdhury, Jeremy Chapeau, Marco Lancini, Ryan Lane, Kedar Ghule, Purusottam Mupunu, Daniel D’Agostino, Ashley Lowde, and Daniel Brauer.</p><p>Additional thanks to Matt Klein for mentorship in managing an open source project.</p><p>Finally, thank you to everyone who has tried out Cartography, raised an issue, shared code in a pull request, provided feedback, or otherwise interacted with the community or project in any way. There have been so many people involved in this journey — thank you for your contributions.</p><p><em>If you think in graphs and not lists, you should apply to work on Lyft’s security team. We’re a small team that absolutely punches above our weight in solving big engineering problems. Visit </em><a href="https://www.lyft.com/careers"><em>Lyft Careers</em></a><em> to see our openings.</em></p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6f6b7be099a7" width="1" /><hr /><p><a href="https://eng.lyft.com/cartography-joins-the-cncf-6f6b7be099a7">Cartography joins the CNCF</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/8f15dc5f3be9</id>
            <title>Integrating Extensions into Large-Scale iOS apps</title>
            <link>https://eng.lyft.com/integrating-extensions-into-large-scale-ios-apps-8f15dc5f3be9?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/8f15dc5f3be9</guid>
            <pubDate></pubDate>
            <updated>2024-12-11T19:48:18.995Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mpvsDeOyT_or7ABfwCl1Qw.png" /></figure><p><em>Written by </em><a href="https://linkedin.com/in/artur-stepaniuk-453a3a191">Artur Stepaniuk</a> <em>and</em> <a href="https://www.linkedin.com/in/maxhusar/">Max Husar</a></p><p>Today, when you open Apple Maps and choose a destination, you are able to see a list of available Lyft offers, seamlessly routing you to the Lyft app to book your next ride. To create this fluid and user-friendly experience across the iOS ecosystem, however, engineers must tackle a range of technical challenges, from managing dependencies in a highly modular application to optimizing performance while maintaining a high quality user experience.</p><p><em>Disclaimer:</em> For the purpose of this deep dive, we assume that you have a general understanding of what a build system is, including concepts like modules, static/dynamic frameworks, dependency graphs and build settings customization via flags.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/300/1*TOp73pX6VcBl8duNE-F3KQ.gif" /><figcaption>Demo of Lyft’s integration to Apple Maps</figcaption></figure><p>From an implementation perspective, this involves creating a separate application extension. Detailed descriptions of API for booking rides can be found in the relevant <a href="https://developer.apple.com/documentation/sirikit/booking-rides-with-sirikit">Apple documentation</a>, which includes multiple code examples.</p><p>Let’s explore the architectural nuances, challenges faced during development, and solutions implemented to overcome these obstacles:</p><ul><li><strong>The Dependency Jungle</strong> and how to manage constraints in a highly modular application, complying with RAM and binary size limitations;</li><li><strong>Development process caveats &amp; SiriKit integration tips</strong> to maintain a consistent development user experience.</li></ul><p>Most of the faced complications in this specific use case can be generalized to integrations with different parts of the iOS system or other applications.</p><h3>Dependency Jungle</h3><p>Lyft applications are built using the <a href="https://bazel.build/">Bazel build system</a> (check out our Lyft Mobile <a href="https://lyftmobilepodcast.libsyn.com/bazel-with-brentley-jones-and-keith-smiley">podcast</a> with Keith Smiley). Our codebase is highly modular, with each business feature consisting of several separate blocks/modules. This modularity forces the use of static linking for dependencies to avoid long app start times, among other benefits.</p><p>However, the downside of static linking is that each linked dependency is <em>copied</em> to the extension, which is a separate target. With many small modules, this leads to numerous connections between them, potentially causing issues and inevitably increasing the dependency graph complexity.</p><p>The advantage of a highly modular codebase is that it simplifies adding corresponding modules and avoids code duplication or significant refactoring. However, after the initial setup, the dependency tree of the extension looks as follows:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KL-DAp5V-rFd7L07YQicMA.jpeg" /><figcaption>Initial dependency tree of the extension module. Powered by Gephi, a visualization tool.</figcaption></figure><p>There are dozens of modules linked to the extension module and hundreds of dependencies between them, making the extension’s dependency tree immensely complex.</p><p>While a large dependency graph isn’t inherently problematic, it does contribute significantly to the Extension’s memory footprint.</p><p>As mentioned above, with static linking each dependency is copied. It implies that every module from the image above will be added to the application package twice, increasing its binary size.</p><p>At the same time, loading these modules during extension’s work increases its overall runtime memory consumption.</p><p><strong>Tooling tea break #1</strong></p><p>At Lyft, we utilize Bazel to analyze module dependency graphs together with open-source <a href="https://graphviz.org/">Graphviz</a> visualization software. It enables us to determine which application modules depend on others, such as checking if module A depends on module B.</p><p>Additionally, <a href="https://gephi.org/">Gephi</a> visualization software is used for demonstration purposes &amp; as a more enhanced graph analysis tool.</p><p>An example query might look like this and can be executed against any module or the root of the app:</p><pre>bazel query 'kind(swift_library, deps(MODULE_PATH:MODULE_NAME))'</pre><p>The output is a list of MODULE_NAME dependencies. Different parameters allow you to create simple files with dependencies lists or build a graph representation of ones. This tool is essential for our instrumentation, particularly in addressing the extension memory footprint issue.</p><p>The next section describes the limitations related to the extension’s available memory and our way of handling them.</p><h4>Memory footprint</h4><p>App extensions are designed to extend existing applications’ functionality, so their resource needs are limited to avoid impacting the overall user experience within the main application or the iOS system.</p><p>For an extension’s runtime memory, there is no fixed limit on the RAM extension, as it depends on the iOS version, device model, OS environment, and other factors. Our explorations indicate that this limit can vary roughly between 20 to 50 MB.</p><p>Regarding binary size, it’s generally understood that smaller is better. A larger binary size can lead to longer download and install times, potentially reducing the number of installs. The worst-case scenario is hitting the 200 MB download size limit, which triggers an additional confirmation dialog during app download when using cellular data.</p><p>In our case, the initially created extension’s RAM footprint is around <strong>21 MB</strong> which is considered safe within the explored boundaries. However, the initial binary size increase of <strong>45 MB</strong> is a critical issue, as the extension itself would take almost ¼ of the 200MB size limit.</p><p>To address this, the following steps can be taken:</p><ol><li>Analyze Business Logic Blocks: Break down the three main business logic blocks — authorization, available offers, and additional offers’ data. Identify the components that contribute most to the binary and runtime memory footprint.</li><li>Eliminate Major Contributors: Investigate and implement strategies to reduce or eliminate these major contributors.</li></ol><p>To facilitate this analysis, a graph visualization tool is used. For example:</p><pre>bazel query --output=graph [other omitted parameters] module=ExploredModule</pre><p>The command above generates a detailed dependency graph file, which can be then visualized using Graphviz, Gephi or other software:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*t7EnkxEJNpriVAe9THWjZQ.jpeg" /><figcaption>ExploredModule’s graph tree with most significant dependencies highlighted</figcaption></figure><p>The next step is to analyze the graph to identify any suspicious dependencies that might include unnecessary resources beyond the source code.</p><h4>In-depth modules analysis</h4><p>To measure the binary size impact in detail, each module can be added as the only dependency to the Apple Maps extension and analyzed using the `binary-size-diff` tool (explained in section below).</p><p>By repeating this process for each necessary business logic feature and its dependencies, we can identify the main contributors to the binary size:</p><ul><li>UI module: ~15MB</li><li>Networking layer: ~7MB</li><li>Interface Description Language (IDL)* imports: ~6MB</li><li>Maps/ML stack: ~8MB</li></ul><p><em>*Note: IDL modules in the context of the Lyft iOS applications are code-generated modules with DTO models and simple API clients to communicate with backend based on the contracts predefined using </em><a href="https://protobuf.dev/"><em>protocol buffers</em></a><em>. The usage of this concept is explained in detail in our other </em><a href="https://eng.lyft.com/protocol-buffer-design-principles-and-practices-for-collaborative-development-8f5aa7e6ed85"><em>article</em></a><em>.</em></p><p>To understand how to remove these elements or limit their impact, we can use Bazel again to show the transitive dependencies (the path) between two modules. For example:</p><pre>bazel query 'allpaths(INITIAL_MODULE_PATH:INITIAL_MODULE_NAME, TARGET_MODULE_PATH:TARGET_MODULE_NAME)' --output=graph | grep -v '  node \[shape=box\];' &gt; relations.dot</pre><p>As we can see on the graph shown below, the result is a much more readable graph compared to other methods.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HvNqvAYmdpqcZrTFWYtCPA.jpeg" /><figcaption>Graph showcasing the dependencies that create Initial-Target modules connection</figcaption></figure><p>It allows us to identify what modules are linking our Initial module to the Target one and understand how to remove it from our dependency graph to unlink the unnecessary source code.</p><p>In our case there are 6 dependencies of the Initial module that need to be addressed to remove the biggest binary size contributor: the Target (CoreUI)* dependency.</p><p><em>*Note: CoreUI is our main module containing all UI components and related resources. While it’s widely used in the main app, it is not needed for the extension and only adds a significant increase in binary size.</em></p><p>The next step for this Initial module would be to either remove all 6 dependencies that lead to the Target module or to eliminate their connection to the Target module.</p><p>Separately, while some adjustments can be made to create a lighter version of the networking layer and to strip some of the IDL imports, the main issue is a singular module which fetches available ride offers (part of its dependencies is shown on the graph above) and its broad dependency list.</p><p>Two approaches are considered:</p><p>1. Extracting a core submodule to be used in both the main app’s Offers service and directly in the Apple Maps extension.</p><p>2. Creating a small, separate module containing only the functionality required for the extension.</p><p>In this case we are choosing the second approach as it allows us to have full control over added dependencies, thereby minimizing any unnecessary imports. The downside is the need to keep both the original and the new services in sync if any relevant parts in the extension are changed.</p><p><strong>Tooling tea break #2</strong></p><p>One of the essential tools for managing the dependency tree is the binary-size-diff script. This CI bash script allows you to compare the binary size differences between the base branch and the created Pull Request. Essentially, it compares the .ipa file sizes in both compressed and uncompressed states, enabling you to see changes in the app’s install and download sizes.</p><p>The workflow looks like:</p><ol><li>Create a draft Pull Request (PR).</li><li>Modify the extension’s BUILD file* to include only the dependencies you want to measure.</li><li>Commit and push the changes.</li><li>Invoke the CI with the command `<em>/test diff-app-sizes`</em> to run the bash script described above.</li><li>Iterate this process for each individual dependency you want to measure.</li></ol><p><em>*Note: </em><a href="https://bazel.build/reference/glossary#build-file"><em>BUILD file</em></a><em> is the main configuration file that tells Bazel what software outputs to build, what their dependencies are, and how to build them.</em></p><h4>Optimization results</h4><p>The culmination of our efforts is resulting in a significantly streamlined dependency tree for the extension, leading to a substantial reduction in its binary size — from <strong>45MB</strong> to <strong>15MB</strong>.</p><p>Below is a visualization of the extension’s resulting dependency graph. Although it may still appear chaotic, most of the remaining modules are IDL imports. These imports are highly atomic and are all transitively linked to a networking base layer, creating some “dependency noise.”</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FOs55y5Xm1caXzxE1RpTww.jpeg" /><figcaption>Extension module dependency tree after performed optimizations</figcaption></figure><h3>Development process caveats &amp; SiriKit integration tips</h3><p>While the optimization of the dependency tree marks a significant milestone in enhancing the extension’s efficiency, the journey towards a successful release doesn’t end here. Attention must now shift to the finer details of the development process. Successfully releasing the Apple Maps extension requires addressing the following small but crucial details.</p><h4>Region Availability</h4><p>Ensure your extension is available in the regions where your platform provides rides. This is done by creating a GeoJSON file listing the supported regions and uploading it as your app’s Routing App Coverage File (<a href="https://developer.apple.com/documentation/sirikit/inlistrideoptionsintent">documentation</a>). The extension card will only be displayed to users trying to book a ride within the specified regions.</p><p>Caveats with GeoJSON:</p><p>- While you can debug the GeoJSON file itself to check its correctness (<a href="https://developer.apple.com/library/archive/documentation/UserExperience/Conceptual/LocationAwarenessPG/ProvidingDirections/ProvidingDirections.html">developer docs</a>), there is no direct way to test its integration with the Maps extension. Therefore, testing must be done in production after release.</p><p>- The GeoJSON file adds maintenance overhead. Whenever your service area changes, the configuration must be manually updated and uploaded.</p><h4>Third-Party dependencies and the APPLICATION_EXTENSION_API_ONLY</h4><p>According to the <a href="https://developer.apple.com/documentation/xcode/build-settings-reference#Require-Only-App-Extension-Safe-API">documentation</a>, the build setting flag APPLICATION_EXTENSION_API_ONLY “when enabled, causes the compiler and linker to disallow use of APIs that are not available to app extensions and <em>to disallow linking to frameworks that have not been built with this setting enabled.</em>”</p><p>Implications:</p><ul><li>If any of your application’s modules <em>or any of its dependencies (direct or indirect)</em> are built with this flag set to TRUE, they cannot be linked to the extension.</li><li>While you can control this flag in your own modules, dealing with third-party dependencies may require stripping them out (in cases when it is impossible to rebuild them from the source code). This issue is closely related to efforts to reduce the extension’s memory footprint (discussed in the previous section). Fortunately in our case no critical functionality depended on these third-party frameworks.</li></ul><p>This highlights the risks associated with relying on third-party dependencies — adding one can lead to unexpected limitations in the future.</p><p>For more on this topic, check out <a href="https://www.scottberrevoets.com/2022/07/15/third-party-libraries-are-no-party-at-all/">our article</a> about the risks and evaluation of adding third-party dependencies.</p><h4>Encountered Developer experience issues</h4><p>In the process of developing extension for Apple Maps, you may face several challenges that can impact workflow and efficiency.</p><p>The first one arises during the installation of your extension on a device or simulator for the first time, SiriKit may not immediately recognize it. This results in the Apple Maps application not displaying the added extension. You may need to wait several minutes before issuing any relevant commands to the Apple Maps (or simply try to reinstall &amp; rerun the extension).</p><p>Similarly, when updating your extension’s Info.plist file or making any code updates to the extension’s business logic, it may take several minutes for SiriKit to recognize the changes. This is especially important during the develop-run-debug cycle as some unexpected behaviors might get unnoticed.</p><h3>Conclusions</h3><p>Integrating Lyft’s ride booking functionality with Apple Maps is a rewarding effort — this journey has underscored the critical importance of precise dependency management and efficient memory usage, even with the substantial computational resources available on modern mobile devices.</p><p>Key Takeaways:</p><ul><li><strong>Dependency Management:</strong> Effective management of a large dependency tree is essential to minimize the memory footprint and binary size of the extension. A highly modular codebase is key for success.</li><li><strong>Tooling:</strong> Leveraging tools such as dependency graph visualization and binary size comparison can significantly aid in identifying and resolving issues related to memory and binary size.</li><li><strong>Third-Party Dependencies:</strong> Relying on third-party dependencies can introduce unexpected limitations, highlighting the need for careful consideration and potential alternatives.</li></ul><p>As we continue to refine our integration with Apple Maps, the lessons learned from this experience will guide us in overcoming new challenges and can be extrapolated to other initiatives related to utilizing various Apple’s App Extensions.</p><p>And one more thing: Lyft is hiring! If you’re passionate about developing complex systems using state-of-the-art technologies or building the infrastructure that powers them, consider <a href="https://www.lyft.com/careers">joining our team</a>.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8f15dc5f3be9" width="1" /><hr /><p><a href="https://eng.lyft.com/integrating-extensions-into-large-scale-ios-apps-8f15dc5f3be9">Integrating Extensions into Large-Scale iOS apps</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/8f5aa7e6ed85</id>
            <title>Protocol Buffer Design: Principles and Practices for Collaborative Development</title>
            <link>https://eng.lyft.com/protocol-buffer-design-principles-and-practices-for-collaborative-development-8f5aa7e6ed85?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/8f5aa7e6ed85</guid>
            <pubDate></pubDate>
            <updated>2024-08-19T16:40:51.543Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*3EBqdIfG2h7wVybLc0Ayrg.png" /></figure><p>At Lyft Media, we’re obsessed with building flexible and highly reliable native ad products. Since our technical stack encompasses mobile clients on both iOS and Android, as well as multiple backend services, it is crucial to ensure robust and efficient communication between all involved entities. For this task we are leveraging <a href="https://protobuf.dev/">Protocol Buffers</a>, and we would like to share the best practices that are helping us achieve this goal. This article focuses on our experience addressing the challenges that come with collaborating on shared protocols in teams where people with different levels of familiarity and historical context, or even people outside the team, get to contribute. The problem of development process quality is prioritized over raw efficiency optimizations.</p><p><strong>Note:</strong> This article focuses on the proto3 specification of the Protocol Buffers (protobuf) language. Code snippets illustrating the handling of generated protobuf messages are provided in Python.</p><h3>Why Protocol Buffers?</h3><ol><li>In comparison to text-based data serialization formats like JSON, protobufs offer both higher serialization efficiency &amp; performance, and better backwards compatibility.</li><li>Our team works with Python, Swift, and Kotlin codebases extensively, and all of these languages have extensive protobuf support.</li><li>Protobufs are extensible with rich validation capabilities that help us reach our reliability goals while avoiding writing boilerplate code across platforms.</li><li>Protobufs boast rich internal tooling at Lyft, and widespread use in both mobile-to-server and server-to-server domains. For additional context on the decision to adopt protobufs for mobile development at Lyft and the process of achieving this change, refer to <a href="https://eng.lyft.com/lyfts-journey-through-mobile-networking-d8e13c938166">this 2020 article</a> by Michael Rebello in the Lyft Engineering Blog.</li></ol><h3>Key Principles</h3><p>Protocol design is different from typical coding &amp; implementation work in a couple of crucial aspects. To illustrate the rather abstract arguments presented here, let’s imagine we’re designing a message conveying a certain event, containing typical fields like event kind, id, timestamp, etc. With no prior protocol design experience, one might be tempted to approach it like writing code and use the familiar concept of <em>enum</em> to distinguish between different kinds of events:</p><pre>message Event {<br />    enum Kind {<br />        EVENT_KIND_A = 0;<br />        EVENT_KIND_B = 1;<br />    }<br /><br />    uint64 id = 1;<br />    uint64 timestamp = 2;<br />    Kind kind = 3;<br />}</pre><p>Indeed, this setup will work well — until new events are added that are required to carry additional data. Let’s say, now a new rich event type appears, and we add a new enum value and the new associated field like so:</p><pre>message Event {<br />    enum Kind {<br />        EVENT_KIND_A = 0;<br />        EVENT_KIND_B = 1;<br />        EVENT_KIND_C = 2;<br />    }<br /><br />    uint64 id = 1;<br />    uint64 timestamp = 2;<br />    Kind kind = 3;<br />    uint32 payload_size = 4;  // Specific to EVENT_C.<br />}</pre><p>The problem with this setup is that it is implicit about the correctness of various combinations of its fields being set — if <em>kind</em> equals <em>EVENT_C</em>, should <em>payload_size</em>’s presence be enforced? What about if <em>kind</em> is <em>EVENT_A</em>? Of course it can be implemented semantically in the logic handling these values, but with each new implicit relationship, like this one, the code becomes more convoluted and can quickly grow unmaintainable. Avoiding such a pitfall brings us to our first principle: <strong>clarity</strong>. It’s nice to work with a protocol that’s structured in a way where it explains itself; both as perceived immediately and as proven by iterating on it long term.</p><p>Clarity relates not only to semantic relationships between fields like we just illustrated, it also applies to individual fields in their own context. Take for example the <em>payload_size</em> field that was just added: what is the unit specifying this value? One may assume bytes, but one shouldn’t have to — and this makes the difference between a good protocol and a lacking one. What about, besides the time unit, assuming the timezone of <em>timestamp</em>? It proves much more practical to name these fields appropriately with these considerations in mind, e.g. <em>payload_size_bytes</em> and <em>timestamp_ms_utc</em>.</p><p>One other common point of ambiguity is fields being required versus being optional. The proto3 standard has deprecated marking fields as required for backward compatibility reasons, effectively allowing any field to be left unset and interpreting such cases as carrying the default value for its given type. Some practices will be covered later in this article to help make your protocols more explicit and more appreciated by engineers who will be using it.</p><p>To zoom out again, a more correct way to go about structuring this message is by using <em>oneof</em>, the protobuf version of the familiar concept of a union. A protobuf oneof unites a set of fields to ensure that no more than one of them is set at the same time, and also to improve data transfer efficiency by saving on the payload size (since unset oneof fields do not get serialized):</p><pre>message Event {<br />    uint64 id = 1;<br />    uint64 timestamp_ms_utc = 2;<br />    oneof data_kind {<br />        EventDataA data_a = 3;<br />        EventDataB data_b = 4;<br />        EventDataC data_c = 5;<br />    }<br />}<br /><br />message EventDataA {}<br />message EventDataB {}<br />message EventDataC {<br />    uint32 payload_size_bytes = 1;<br />}</pre><p>Now the <em>EventData</em> messages serve both to distinguish between event kinds, and to contain the respective fields specific to a certain kind but not others. Even if some of them are going to share a common subset of fields, the duplication of declaring them individually is worth it compared to the ambiguity in the other approach. Unlike with enums, the oneof approach is self-documenting, and it doesn’t leave much room for error interpreting how the message should be formed.</p><p>Note, however, that the structure of our message had to change in a major way to allow this upgrade, which is another key thing to keep in mind — let’s call this the principle of <strong>extensibility</strong>. This doesn’t only apply to avoiding convoluted field semantics. Let’s say at some point the system is migrated from using integer IDs to UUIDs. It becomes a problem since the <em>id</em> field is already locked in as <em>uint64</em>, and we are forced to deprecate and declare a new one; whereas having it as <em>string</em> from the beginning would allow a smooth transition to virtually any ID scheme. While it’s impossible to predict and stay safe from all potential breaking changes, there’s a few common pitfalls in protobuf, which often revolve around changing the type of a field and rearranging oneof groupings.</p><p>To recap, the key principles of protocol design that we just outlined are:</p><ul><li><strong>Clarity</strong>: A well-designed protocol should define its messages in a way where it’s not only explicit about which fields must be set. This prevents missetting any of the messages during implementation. In other words, <strong>good protocols leave no ambiguity for its implementers</strong>.</li><li><strong>Extensibility</strong>: It is crucial that protocol structure is built with future vision and potential roadmap in mind. This way, some foreseeable additions and breaking changes can be accounted for in advance.</li></ul><p>These ideas are quite applicable to classic software development. However, protocol design features greater constraints in comparison and therefore puts greater emphasis on the above principles.</p><h3>Best Practices</h3><p>Besides the broad principles, let’s go over some practices that help avoid typical pitfalls in protocol design.</p><h4>Unknown enum values</h4><p>It’s always a good idea to declare the 0-th element of an enum as “unknown” to ensure backward compatibility. When an enum without one is added to a message definition, earlier implementations that came before this addition will produce messages for which the fields of its type will be interpreted by newer implementations as 0. To use the <em>Event.Kind</em> example from earlier:</p><pre>enum Kind {<br />    EVENT_KIND_A = 0;<br />    EVENT_KIND_B = 1;<br />}</pre><p>The above definition should become:</p><pre>enum Kind {<br />    EVENT_KIND_UNKNOWN = 0;<br />    EVENT_KIND_A = 1;<br />    EVENT_KIND_B = 2;<br />}</pre><p>This way, coming back to the clarity principle, it’s unambiguous and implementation-agnostic when the enum value is set.</p><h4>Well-known types</h4><p>Once your team has used protobufs for some time, you might notice that some field types commonly pop up across your protocol surface. There are some that are commonplace enough that the Protocol Buffers development team made them part of the language itself, such as <em>Duration</em> and <em>Timestamp</em> among other, more specific ones. Indeed, going back to the event message example, the <em>uint64</em> timestamp field can — and should — be replaced with a <em>google.protobuf.Timestamp</em>, fitting right in line with our clarity principle. Some might not be available out of the box, and it’ll be at your team’s discretion to add and standardize your usage of them, for example a reusable <em>LatLng</em> type for geospatial coordinates.</p><p>The full list of default well-known protobuf types is available <a href="https://protobuf.dev/reference/protobuf/google.protobuf">here in the official documentation</a>.</p><h4>Explicit optional fields</h4><p>A bit of historical context: in the proto2 protobuf standard, both required and optional fields could be marked with a namesake label. The <em>required</em> label was enforced strictly by the compiler which proved hugely problematic in the long run, because it was nearly impossible to safely change a required field to be optional. In the proto3 standard, the <em>required</em> label was dropped entirely and starting with protobuf 3.15 (2021), the <em>optional</em> label was added. The distinction shifted to fields being explicitly optional vs. implicitly required (having no explicit label). The value of marking optional fields is in the ability to check them for presence in a serialized message. Let’s say that, in the event message example above, we need to distinguish between the payload size value being 0, and being absent. With its current state:</p><pre>message EventDataC {<br />    uint32 payload_size_bytes = 1;<br />}</pre><p>Querying from a formed message like this:</p><pre>if event_pb.WhichOneof('data_kind') == 'data_c':<br />    # ...<br />    if not event_pb.data_c.payload_size_bytes:  # Error-prone!<br />        handle_payload_size_absent()</pre><p>Is error-prone, and can be quite misleading — since primitive types get initialized to a default value, it’s impossible to tell whether a field was absent or equal to default value. However, with an optional label like so:</p><pre>message EventDataC {<br />    optional uint32 payload_size_bytes = 1;<br />}</pre><p>The <em>.HasField</em> method can then be used on the <em>EventDataC</em> instance:</p><pre>if event_pb.WhichOneof('data_kind') == 'data_c':<br />    # ...<br />    if not event_pb.data_c.HasField('payload_size_bytes'):<br />        handle_payload_size_absent()</pre><p><strong>Note:</strong> Since protobufs were adopted at Lyft prior to the introduction of optionals to the language specification, our convention for optional primitive types is to use wrappers from the <em>google.protobuf</em> package.</p><h4>Validation rules</h4><p>When considering principles of protocol design, we are big fans of explicitly stating a field’s constraints within its message’s broader context. For this we’re taking advantage of the <a href="https://github.com/bufbuild/protoc-gen-validate">protoc-gen-validate</a> plugin (PGV).</p><p><strong>Note:</strong> Since recently, PGV has reached a stable state and has been succeeded by <a href="https://github.com/bufbuild/protovalidate">protovalidate</a>. While the general idea remains the same, consider using the modernized solution when getting started with validation.</p><p>A list of useful validation rules for common types is provided in the bulleted section below. Please note that some level of familiarity with protobufs is assumed.</p><ul><li><strong><em>oneof</em> validation</strong>: By default, and somewhat counterintuitively, none of the fields declared under a oneof have to be set. A neat validation rule exists to enforce one of the fields to be present in a formed message: option (validate.required) = true; and needs to be declared alongside with the oneof members.</li><li><strong>Generic <em>message</em> validation</strong>: (validate.rules).message = { … } <br />· required with a boolean value is self-explanatory and extremely useful.</li><li><strong><em>enum</em> validation</strong>: (validate.rules).enum = { … }<br />· Prior to proto3, enums were treated as “closed” — meaning that fields of their type could only store the defined values. This produced undefined behaviors, and “open” enum behavior was introduced with proto3, making it valid for fields to be set to values other than the ones listed in the enum definition. defined_only is useful for enforcing that an enum field is effectively “closed” and will only carry expected values.<br />· in allows you to specify the collection of acceptable values for the given field.<br />· not_in is also extremely handy. The obvious example is to set it to [0] — enforcing cases when the unknown value is not acceptable.</li><li><strong><em>string</em> validation</strong>: (validate.rules).string = { … } <br />· min_len with value <em>1</em> is great for enforcing a non-empty value to be set for the field.<br />· Well-known string formats are handily available for validation, including email, ip (and ipv4 and ipv6), uri, uuid, among other ones.<br />· pattern allows you to define a bespoke regex to fit your validation needs.</li><li><strong><em>repeated</em> validation</strong>: (validate.rules).repeated = { … } <br />· min_len with value <em>1</em> is great for enforcing that the collection is not empty.<br />· items allows individual values to be validated against their given type, e.g. items: {enum: {not_in: [0]}}<br />· unique set to <em>true</em> is useful for validating set-like collections.</li><li><strong><em>map</em> validation</strong>: (validate.rules).map = { … }<br />· min_pairs and keys &amp; values work exactly like min_len and items for <em>repeated</em> fields, respectively.<br />· no_sparse is good for validating that, for maps with non-primitive value type, values must be set.</li></ul><p><strong>Pro tip:</strong> Validation also works on the wrapper types with the same rules as for their respective wrapped types, e.g. a <em>google.protobuf.StringValue</em> field can be validated with (validate.rules).string = { … }.</p><p>An exhaustive definition of all validation rules (declared in protobuf syntax themselves!) is available in <a href="https://github.com/bufbuild/protoc-gen-validate/blob/main/validate/validate.proto">the validate.proto source</a>.</p><p><strong>Note:</strong> It is important to understand that the generated validation methods still need to be called manually — <strong>if a message is formed in violation of the stated rules, nothing will fail until its validator is invoked!</strong> A snippet for validating a formed protobuf message will look like this:</p><pre>import protoc_gen_validate.validator<br /># It's handy to explicitly distinguish between protobuf entities<br /># and natively defined models, by e.g. appending PB as applicable<br /># or importing the whole package.<br />from your_protobuf_namespace_path.event_pb2 import Event as EventPB<br /><br />event_pb = EventPB(...)<br /><br />try:<br />    protoc_gen_validate.validator.validate(event_pb)  <br />except protoc_gen_validate.validator.ValidationFailed as ex:<br />    raise ValueError(f'Protobuf validation error: {ex}')</pre><h4>Cross-entity constants</h4><p>In some cases, various code points across different services or even domains (e.g. client app and server) may need to refer to the same constants. Protobuf definitions can lend great help in aligning these constants across all entities. Although it’s not an explicit feature of the language, this effect can be achieved using custom options:</p><pre>import &quot;google/protobuf/descriptor.proto&quot;;<br /><br />extend google.protobuf.EnumValueOptions {<br />    // Use a distant number to avoid accidental collisions.<br />    // For a small project, picking an arbitrary large prime number<br />    // should be safe enough.<br />    // For larger projects, tooling can be built to manage field numbers<br />    // with safety guarantees.<br />    string const_value = 11117;<br />}<br /><br />enum EventTag {<br />    // The unknown value might not be necessary depending on whether<br />    // you intend to pass values of this type in actual proto messages,<br />    // or just reference their const values statically.<br />    EVENT_TAG_UNKNOWN = 0 [(const_value) = &quot;&quot;];<br />    EVENT_TAG_1 = 1 [(const_value) = &quot;#tag1&quot;];<br />    EVENT_TAG_2 = 2 [(const_value) = &quot;#tag2&quot;];<br />}</pre><p>Then the values can be accessed through the enum descriptor:</p><pre>from your_protobuf_namespace_path import event_pb2<br /><br />tag_name = event_pb2.EventTag.Name(event_pb2.EVENT_TAG_1)<br />tag_descriptor = event_pb2.EventTag.DESCRIPTOR.values_by_name[tag_name]<br />tag_options = tag_descriptor.GetOptions()<br />tag_value = tag_options.Extensions[event_pb2.const_value]</pre><p>Or, compacted:</p><pre>tag_value = event_pb2.EventTag.DESCRIPTOR \<br />    .values_by_name[event_pb2.EventTag.Name(event_pb2.EVENT_TAG_1)] \<br />    .GetOptions() \<br />    .Extensions[event_pb2.const_value]</pre><p><strong>Note:</strong> It’s recommended to exercise caution when using this technique. It is most suitable for cases where the constant values are never expected to change, or where you have complete control over deployment of entities that will be consuming the protocol.</p><h4>Language-dependent behaviors</h4><p>The <a href="https://protobuf.dev/getting-started">“Getting started”</a> section in the official documentation is a good entry point to language-specific protobuf work, covering the basic setup as well as more nuanced details like exact type mapping, ways of parsing messages, properties of the entities generated from the protocol definition, etc. This is important because certain behaviors differ across languages (from namespace structuring and naming to implementation details, i.e. when a key in a <em>map</em> field has no value, it being serialized with the default value in some languages and omitted in others), so knowing your target language stack you can always find the right steps to ensure correct behavior.</p><h3>Conclusion</h3><p>In this article, we’ve explored the intricacies of working with Protocol Buffers from a collaboration standpoint. In the end, our protocol might end up looking like this:</p><pre>syntax = &quot;proto3&quot;;<br /><br />import &quot;google/protobuf/descriptor.proto&quot;;<br />import &quot;google/protobuf/timestamp.proto&quot;;<br />import &quot;validate/validate.proto&quot;;<br /><br />extend google.protobuf.EnumValueOptions {<br />    string const_value = 11117;<br />}<br /><br />enum EventTag {<br />    EVENT_TAG_UNKNOWN = 0 [(const_value) = &quot;&quot;];<br />    EVENT_TAG_1 = 1 [(const_value) = &quot;#tag1&quot;];<br />    EVENT_TAG_2 = 2 [(const_value) = &quot;#tag2&quot;];<br />}<br /><br />message Event {<br />    string id = 1 [(validate.rules).string = {min_len: 1}];<br />    google.protobuf.Timestamp timestamp_utc = 2 [(validate.rules).timestamp = {required: true}];<br />    oneof data_kind {<br />        option (validate.required) = true;<br />        EventDataA data_a = 3;<br />        EventDataB data_b = 4;<br />        EventDataC data_c = 5;<br />    }<br />}<br /><br />message EventDataA {}<br />message EventDataB {}<br />message EventDataC {<br />    optional uint32 payload_size_bytes = 1;<br />}</pre><p>To recap the key takeaways:</p><ul><li><strong>Clarity and Extensibility</strong>: We’ve emphasized the importance of designing protocols that are self-explanatory and flexible enough to accommodate future changes. This approach minimizes ambiguity for implementers and reduces the likelihood of breaking changes.</li><li><strong>Best Practices</strong>: We’ve covered several useful practices, including:<br />· Using unknown default enum values<br />· Leveraging standard well-known types<br />· Setting optional fields intentionally and explicitly<br />· Implementing validation rules<br />· Declaring cross-entity constants when appropriate</li></ul><p>There are many other useful practices that aren’t mentioned in this article, that may or may not apply to your team depending on the given use case and language stack. For an extensive list, refer to <a href="https://protobuf.dev/programming-guides/dos-donts">Proto Best Practices</a> and <a href="https://protobuf.dev/programming-guides/api">API Best Practices</a> from the official documentation.</p><p>And one more thing: <strong>Lyft is hiring!</strong> If you’re passionate about developing complex systems using state-of-the-art technologies or building the infrastructure that powers them, consider <a href="https://www.lyft.com/careers">joining our team</a>.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8f5aa7e6ed85" width="1" /><hr /><p><a href="https://eng.lyft.com/protocol-buffer-design-principles-and-practices-for-collaborative-development-8f5aa7e6ed85">Protocol Buffer Design: Principles and Practices for Collaborative Development</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/60ceb460dfea</id>
            <title>Building Lyft’s Next Emblem — Glow</title>
            <link>https://eng.lyft.com/building-lyfts-next-emblem-glow-60ceb460dfea?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/60ceb460dfea</guid>
            <pubDate></pubDate>
            <updated>2024-07-29T22:43:03.688Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <h3><strong>Building Lyft’s Next Emblem — Glow</strong></h3><p><em>By: </em><a href="https://www.linkedin.com/in/avneetoberoi/"><em>Avneet Oberoi</em></a><em>, </em><a href="https://www.linkedin.com/in/michael-vernier-a898226/"><em>Michael Vernier</em></a><em>, </em><a href="https://www.linkedin.com/in/yanrong-phoenix-li-14119173/"><em>Phoenix Li</em></a><em>, </em><a href="https://www.linkedin.com/in/khajamasroorahmed/"><em>Masroor Ahmed</em></a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*dNJM0P5RBdb6THJx" /></figure><h3>Introduction</h3><p>Long time riders might remember the original fuzzy, pink Carstache emblem that made Lyft universally recognizable. Over the years, the emblem dropped the fuzz for pink lights in the <a href="https://www.youtube.com/watch?v=mxQgABWfLkg">Glowstache</a> and later evolved with more colors as the beloved <a href="https://lyft.com/amp">Amp</a>, which has been in active use for over seven years! Recently, Lyft has introduced its brighter, bolder next generation emblem — <a href="https://www.lyft.com/driver/glow">Glow</a>. Glow provides a daytime visible, auto-dimmable display showing rider customizable colors and new animations to help them find their ride faster. Glow also has enhanced GPS and IMU sensors for improved driver location accuracy.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/716/1*E-t0meafr_J11PRk007YPA.png" /><figcaption>Figure 1: In app rider experience for Glow ride</figcaption></figure><p>Prior to Glow, Lyft has IoT development experience with not only the Amp but also with <a href="https://www.lyft.com/bikes">Bikes</a> and <a href="https://www.lyft.com/scooters">Scooters</a>, <a href="https://media.lyft.com/lyft-halo">Halo ad displays</a>, and even Autonomous research vehicles. Each of these were built with bespoke solutions for their use case, which were hard to retrofit for new device types. Observing similar functionality across these siloed systems motivated us to collaborate with these teams, where feasible, to build new IoT middleware services which could provide a unified framework for managing a variety of devices.</p><h3>High Level Overview</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fwLJIQDLlh9-JWSK" /><figcaption>Figure 2: Simplified high level overview</figcaption></figure><p>Just like for the Amp, we continue to use Bluetooth Low Energy (BLE) as the communication mechanism between the Glow device and the driver’s smartphone, instead of opting for a cellular chip in the device. We leverage the driver’s phone as an “IoT gateway” which serves as a communication link between the Glow device and the Lyft backend.</p><p>The Lyft backend consists of services which act as the brain of the whole operation, determining everything from whether the driver is eligible to use a Glow, to controlling every aspect of the driver’s Glow.</p><p>The Glow device itself is controllable via a well defined request-response based command framework that was developed in-house based on our requirements.</p><p>This post will detail a few simplified foundational components that make up the IoT system for the Glow including:</p><ul><li>Provisioning and Authentication</li><li>Control and Communication</li><li>State Management including the firmware update process</li></ul><p>Lastly, we’ll briefly discuss what’s next for the Glow program.</p><h3>Provisioning and Authentication</h3><p>For any IoT device, provisioning refers to the process of creating a unique identifier for each device and registering the device in a central Device Registry, so that the state of each one in the fleet can be tracked and managed.</p><p>Common provisioning processes typically involve:</p><ul><li>Flashing a bootstrap URL onto the device during manufacturing and then letting the device self-identify by connecting to the URL when activated.</li><li>Having the end user manually register their device through an app or website when they start using the device.</li></ul><p>After a device has been provisioned into the Device Registry, future communication with the device requires authentication to verify the device is genuine before it engages with the IoT system. Authentication mechanisms such as verification of on-device certificates, symmetric or asymmetric key cryptography, or more sophisticated hardware solutions can be employed depending on the sensitivity of the data being transferred to and from the device.</p><p>For the Amp, we simply relied on treating the <a href="https://en.wikipedia.org/wiki/MAC_address">MAC address</a> of the Amp device as its unique identifier, which would be read by the driver’s phone and relayed to the backend. Phone manufacturers though can choose to anonymize MAC addresses, providing an obfuscated and random hash of the MAC address every time it is read. Additionally, we had no concept of a dedicated Device Registry then and were simply associating each device’s MAC address with a driver record. Since each device could report multiple identities, we faced obvious challenges with device state management and accurate user-to-device association and tracking.</p><p>In order to circumvent the problems faced with the Amp, we designed the following on-the-fly provisioning process for the Glow:</p><ol><li>Each Glow device is assigned a unique serial number during the manufacturing process. This number is stored within the Glow’s internal flash storage and on a barcode which is affixed to the device packaging.</li><li>As part of shipping a device to a driver, the above barcode is scanned and mapped to a specific Lyft driver in the Device Registry service. Once this is complete, the Glow is shipped out to its new owner.</li><li>When the driver receives their Glow device and tries connecting it to their phone, the Lyft Driver app will leverage a pre-shared key and the serial number of the Glow to authenticate the device before any communication can begin.</li></ol><p>The diagram below summarizes the different states that a Glow device transitions through, as described above.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*TsN1yO6i_bjvewZ-" /><figcaption>Figure 3: Glow device state transitions</figcaption></figure><p>The above flow not only helps provide detailed device metadata and accountability for devices issued to drivers, but is also meant as a security measure. Unlike the Amp, a Glow device is made to only illuminate when it has successfully authenticated with the Lyft backend services and when the driver is actively driving for Lyft. This prevents malicious parties from impersonating Lyft drivers with a Glow device obtained from someone or somewhere other than Lyft.</p><h3>Device Control and Communication</h3><h4>Device Command Framework</h4><p>Each IoT device needs to be controllable by means of pre-determined operating instructions. These can be as trivial as turning the device on or off, or more involved, like playing a certain animation file on the LED display, such as for the Glow. A device is made remotely controllable through its firmware i.e. the on-device software responsible for deciphering and executing instructions to make the device operate as required.</p><p>The instruction set aka the “command set” built for the Glow is simple, yet powerful. It supports instructions to individually control various functionalities such as animation file playback, adjusting device brightness, uploading file to device storage and even monitoring on-device sensors. These types of fine grained device commands are also made highly configurable which enables robust, predictable, and easily extensible behavior from the Glow device.</p><p>As an example, the instruction for file uploads to the device unlocks the ability to add more animation options for riders to choose from in the future. The alternative would have been to bundle the additional files as part of a new firmware update, which would be much slower to build and roll out. Additionally, firmware resourcing is often more limited, delaying launch of even simple functionality like the above.</p><h4>Device Control Flow</h4><p>The Lyft backend is responsible for controlling the device using the command set described above. We built a backend service, “Device Controller”, which listens for different types of relevant event triggers to determine appropriate instructions for the driver’s Glow device. For example, when the service receives a trigger indicating that the driver is in close proximity to the rider’s pickup location, it issues an instruction for the Glow to play the rider selected pickup animation.</p><p>The mobile client acts as the communication gateway, helping with essential flows such as command relay and BLE protocol translation as well as buffering for data transfers when needed.</p><p>Each server generated command is delivered to the mobile client by means of a server streaming component which exists in Lyft’s infrastructure. The client simply needs to subscribe for the commands once a Glow is successfully connected to it, in order to receive real-time data from the server.</p><p>The entire command set has been divided into two categories, based on how the command is to be handled by the client:</p><ul><li>Passthrough commands: These are simple commands which do not require any pre-processing or state management by the mobile client and can be forwarded directly to the Glow device via BLE. Examples include updating the device’s brightness or updating the animation being played on the device.</li><li>Complex commands: These typically require the client to pre-process payload data, download files from the Lyft backend, buffer data, or maintain some sort of state. Transferring files to the Glow is an action that requires complex commands. A single command is sent from the backend to the client. The client is responsible for first downloading the new files to internal storage, splitting the file into smaller chunks, and sending each chunk to the Glow in a separate command.</li></ul><p>Some of the more nuanced details about command ordering/prioritization, data encoding, retry mechanism and encryption mechanism between the client and Glow have been omitted in order to avoid complicating the overall flow we’re trying to depict here.</p><p>All commands include a unique identifier to enable tracking of the control flow at any given time. The Glow will send a response for every command message it receives. This response will include whether the action succeeded or failed, an error message describing the failure, or any data that might have been requested by the command. All responses are transmitted to the Lyft backend so that they can be used to diagnose or debug issues.</p><p>The end to end flow has been summarized in the image below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*oFRHvg7vW0GteKUZ" /><figcaption>Figure 4: End to end device control flow</figcaption></figure><h4>Device Data Streams</h4><p>While the above section details communication directed to the Glow, we talk briefly here about data originating from the device itself.</p><p>The Glow streams data to the client so that neither the server nor the client has to continuously request new data from it. There are 4 types of messages that the Glow generates:</p><ul><li>Sensor readings: Data generated from on device sensors which includes internal device temperature, display brightness, inertial measurements from gyroscopes and accelerometers, position and speed of the vehicle.</li><li>Diagnostic information: The Glow provides information regarding its performance and any errors it encounters during its normal operation. This information is necessary for software maintenance and to keep the Glow device running in optimal condition.</li><li>Warning messages: These are triggered by the Glow when it is operating beyond the bounds of its normal operation. For example, a temperature sensor on the Glow warns the client that the Glow device is running abnormally hot. In such cases, the device will self trigger a sleep command, which will render it inoperable for a duration of time before it can be restarted again.</li><li>Heartbeat messages: This periodic stream of data reflects the current device state such as the installed firmware version, device assets, etc. Such state is explicitly managed as is detailed in the section below.</li></ul><h3>State Management</h3><p>An IoT device’s state is a set of properties that represent its current configuration. This state should be updatable for device management and readable by the backend for monitoring and debugging issues with the device. A device can either report its state asynchronously by pushing periodic updates to the backend or synchronously by responding to a request for it.</p><p>A “Device Shadow” service maintains a mirror of each device’s current state using the information received from the device. The service also manages the desired state of the device, which can be updated by systems integrated with the service. The Device Shadow service will detect the difference between the current and desired state state and issue appropriate commands required to transition the device to the desired state. This is done in an eventually consistent manner, thereby guaranteeing synchronizing a device to its desired state irrespective of its current connectivity status.</p><p>The state information for the Glow includes the current firmware version, a list of files stored on the device, the display brightness and configuration parameters for various device functions. The Glow broadcasts its state information asynchronously to the client to forward to the backend. A simplified flow depicting this process is shown below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hS6mKk9401OjNOLD" /><figcaption>Figure 4: Device Shadow determining commands</figcaption></figure><p>Lastly, we discuss how device firmware is updated via the Device Shadow service.</p><h4>Firmware Update Process</h4><p>During the lifetime of an IoT device, it’s almost guaranteed that the firmware will need to be upgraded to add new functionality, address bugs, or patch security vulnerabilities. Some devices can be manually updated by means of a human operator, but most need to support remote over-the-air (OTA) updates. Without a human operator, the firmware update strategy needs to be robust to internal and external problems such as firmware not initializing properly or an update that was only partially installed due to a power interruption. If the system is not designed properly, a device can become inoperable, often referred to as being “bricked.”</p><p>A new “OTA Manager” service was created to manage the firmware update lifecycle of devices, including Glow as well as other IoT devices developed by teams within Lyft. The OTA Manager service is wired to an internal developer UI which allows configuring the desired firmware version, the S3 location of the firmware file, the rollout percentage and a set of attribute filters which allow fine grained control of which devices the firmware update should be applied to.</p><p>The Device Shadow service interacts with OTA Manager to check for any differences between the installed firmware image on the device, as read from the most recent heartbeat message, and compares it with the desired firmware version. If a difference is detected, a new complex command for file upload will be issued by the server and managed by the client as described earlier in the post, to send the new firmware file to the Glow device.</p><p>As the Glow receives the new firmware image, the contents are saved to a different memory location than the currently running version. Once the full image is received, the Device Shadow service commands the device to reboot.</p><p>On power on, the Glow runs a small piece of software called a Boot Manager to detect a new pending firmware update. When one is detected, the following happens:</p><ul><li>The previous firmware image is copied from internal to external storage and vice versa for the new firmware image.</li><li>The new firmware image is booted and several checks are performed before confirming that the new firmware image is functioning properly. For example, the device needs to connect to the mobile client via Bluetooth and successfully authenticate with Lyft backend services.</li><li>If any fault occurs, the device will reboot and revert back to the previously working firmware image. The Glow will also attempt to transmit the reason for failure in its next heartbeat message.</li><li>If anything goes wrong during this revert process, a recovery firmware image will be used, which was flashed onto the device as part of the manufacturing process and cannot be changed.</li></ul><p>All of these mechanisms combine to help mitigate the risk of bricking a Glow device.</p><h3>What’s Next ?</h3><p>The Glow is actively rolling out in markets across the US, with over 30,000 devices already live!</p><p>The systems built above are standing strong, but we’re keeping an eye out for any room for improvements in our systems and are already sharing the knowledge gained with other teams at Lyft.</p><p><em>Lyft is hiring! If you’re passionate about building software, visit </em><a href="https://www.lyft.com/careers"><em>Lyft Careers</em></a><em> to see our openings.</em></p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=60ceb460dfea" width="1" /><hr /><p><a href="https://eng.lyft.com/building-lyfts-next-emblem-glow-60ceb460dfea">Building Lyft’s Next Emblem — Glow</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/fddc42f24096</id>
            <title>FAQ: Common Questions from Candidates During Lyft Data Science Interviews</title>
            <link>https://eng.lyft.com/faq-common-questions-from-candidates-during-lyft-data-science-interviews-fddc42f24096?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/fddc42f24096</guid>
            <pubDate></pubDate>
            <updated>2024-06-25T21:13:31.737Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xsvmCSLmhSjtfKFfa5Gnsg@2x.jpeg" /></figure><p><em>Written by</em> <a href="https://www.linkedin.com/in/nada-sarsour-700abb24/"><em>Nada Sarsour</em></a><em>,</em> <a href="https://www.linkedin.com/in/mariaf-ochoa/"><em>Maria Rice</em></a><em>, and</em> <a href="https://www.linkedin.com/in/kellyhaberl/"><em>Kelly Haberl</em></a></p><p>Interested in applying to Lyft Data Science or currently in the interview process? This article helps answer questions commonly asked by Data Science candidates looking to learn more about the Lyft application process, our Data Science teams, and overall life at Lyft!</p><h3><strong>The Application &amp; Interview Process</strong></h3><p><strong>Which Data Science role should you apply for?</strong></p><p>Lyft Data Science is split between Decisions and Algorithms Scientists, which can be defined by the typical output of their work (although there is overlap between the 2 roles). Decision Scientists tend to focus more on helping humans make decisions by utilizing a deep understanding of the business to develop decision frameworks that drive alignment on the most impactful solutions. Algorithm Scientists tend to focus more on helping machines make decisions by developing models that power internal and external production systems. Both types of scientists leverage a suite of data science skills, from A/B experimentation to machine learning and operations research.</p><p>When you are deciding what Data Science role to apply for at Lyft, we recommend reviewing <a href="https://eng.lyft.com/what-is-data-science-at-lyft-4101a69be028">this article by Simran and Thibault </a>to learn more about different focus areas and responsibilities. Your recruiter can also help determine what role makes the most sense with your experience after you submit your application.</p><p><strong>How is the interview process structured?</strong></p><p>The Data Science interview process at Lyft is similar for both Decisions and Algorithms Scientists. Even though this <a href="https://eng.lyft.com/what-to-expect-when-interviewing-as-a-data-scientist-intern-at-lyft-42a6d475cb81">previous article</a> is focused on the process for Lyft Science Interns, it provides a great overview of the interview process that’s also relevant to full time hires (note: the interview process differs slightly if you are interviewing for a staff or manager role).</p><p>Overall, there are 3 stages to interviewing:</p><p><strong>Recruiter Screen (30 minutes):</strong> A recruiter reviews your resume and will reach out if they determine you are a good match for an opening. It is important for your resume to align with the role you are interested in and to include experience that matches the requirements of the role. The recruiter call is a chance for you to talk through your experience and learn more about the different tracks/interview processes.</p><p><strong>Technical Phone Screen (60 minutes):</strong> This interview step is the same for all Data Science tracks. It focuses on statistics, probability, experimentation, and business acumen, and does not include live coding or modeling. After the phone screen and based on interviewer feedback, there is an opportunity for candidates to be redirected to another track (i.e. Decisions or Algorithms) if you would perform better in another space.</p><p><strong>Virtual Onsite Interviews:</strong> The final round consists of 4 or 5 virtual interviews where candidates speak with a Data Scientist or Data Science Manager. These rounds can be completed in 1 day or split across 2 days. These interviews are broken down into the following areas:</p><ul><li><strong>Business Case Interview (45 minutes):</strong> work through a technical business problem that’s an example of the problems you would solve in this DS role at Lyft; this interview requires an evaluation of a business case and Lyft metrics but won’t include live coding.</li><li><strong>Experience Interview (45 minutes):</strong> in this behavior interview, candidates could be asked to give examples of previous projects, describe how you reacted in specific work-related situations, and discuss your background and interests</li><li><strong>Coding Interview (45 minutes):</strong> in this technical interview, candidates complete a live coding challenge in the language of their choice; the interview various depending on the Data Science path:<br />- <strong>Algorithms</strong>: a technical problem related to programming fundamentals used in everyday practical algorithm development, machine learning implementation, or data processing; this interview is meant to test the candidate’s technical coding abilities and communication skills, through discussing potential approaches and pitfalls, and implementing a working end-to-end solution to a practical problem (no esoteric pointer techniques or brain-teasers!<br />- <strong>Decisions: </strong>a series of analytical coding questions that require manipulation of an example data set containing rideshare data, to assess coding and communication skills</li><li><strong>Technical Interview (45 minutes): </strong>work through a case study, which varies based on the Data Science path:<br />- <strong>Algorithms</strong>:<em> </em>a technical discussion on how you would approach and solve a problem end-to-end that you would encounter as a data scientist at Lyft, using tools from your track expertise related to either statistics, optimization, or machine learning<br />- <strong>Decisions</strong>: a diagnosis of a product problem end-to-end, including sections on experimentation, probability, and product intuition</li></ul><p><strong>How long does the interview process take from start to finish?</strong></p><p>The time it takes to complete our interview process depends on your availability and how quickly you are willing and able to schedule interviews. On average, it takes about 3–4 weeks to complete the entire science interview process. If a candidate does not pass the interviews, they are eligible to reapply after 6 months.</p><p><strong>Are any interviews conducted in person in a Lyft office?</strong></p><p>All science interviews are being conducted virtually so you will not be required to come into an office for any interviews at this time!</p><h3><strong>Lyft’s Data Science Teams</strong></h3><p><strong>What type of work will I do?</strong></p><p>At Lyft, Data Scientists are embedded into a specific business function or focus area. This allows our scientists to become experts in an area of the Lyft business and drive long-term impact. The team working on one business function will consist of not only Data Scientists, but cross-functional teammates from Product Management, Software Engineering, Design, Marketing, Operations, etc.</p><p>There are Scientists working on every aspect of the Lyft business — here are some examples of focus areas our scientists are aligned to (more information on focus areas can be found in this <a href="https://eng.lyft.com/what-is-data-science-at-lyft-4101a69be028">article</a>!):</p><ol><li><a href="https://eng.lyft.com/pricing-at-lyft-8a4022065f8b">Rideshare’s Pricing</a></li><li><a href="https://eng.lyft.com/data-science-on-lyfts-fleet-team-141c594f656b">Lyft’s Fleet Growth</a></li><li><a href="https://eng.lyft.com/a-b-tests-for-lyft-hardware-570330b488d4">Bike &amp; Scooter Hardware</a></li></ol><p><strong>How do I get matched to a team?</strong></p><p>Your interview process will be focused on interviewing for a specific team, which will be outlined in the job description that you are applying for. If an interviewer sees you may be a better fit on another team, they will let the recruiter know so you can discuss which is the best team match. Given this set up, it is preferable that the candidate apply to only one type of role, Decisions or Algorithms, during the initial application to not delay the interview process (selecting a type of role can also be discussed with a recruiter during the initial recruiter screen).</p><p><strong>Are specific teams based in one location? Is traveling common?</strong></p><p>Teams are not based in one specific office location, but rather one team can be located across a few of our offices. Our corporate offices are in San Francisco, New York, Seattle, Montreal, and Toronto. You will report to the nearest location per your residence for our hybrid work schedule.</p><h3><strong>Life at Lyft</strong></h3><p><strong>Tell me more about Lyft’s in-office policy!</strong></p><p>Lyft’s workplace strategy categorizes most of our roles as hybrid roles. Team members in hybrid roles will work from their nearest Lyft office and report to the office 3 times a week. Additionally, hybrid roles have the flexibility to work from anywhere for up to 4 weeks per year.</p><p>In the office, Lyft provides free breakfast, lunch, and snack options daily. At the Lyft headquarters in San Francisco, in addition to the salad, soup, and pizza options, there is a rotational buffet lunch each day including menu items like pastas, empanadas, tostadas, and poke!</p><p><strong>How would you describe Lyft’s culture?</strong></p><p>Lyft’s culture revolves around a sense of collaboration and belonging. Team members at Lyft proactively support one another, whether it’s discussing ideas on Slack, reviewing code, or whiteboarding solutions together. We frequently celebrate each other, whether it be nominating others for ‘Employee of the Month’ or discussing our hobbies &amp; passions! Teams typically have events or offsites at least once a quarter and cross-team socials occur in or around the office even more often than that.</p><p>It’s also a culture of fun! The Diversity &amp; Inclusion group at Lyft has a Culture workstream dedicated to bringing people together. We host various events such as social meet ups (from a gingerbread house competition to a Texas Hold ’em poker tournament with prizes to an in-office Boba social — pictured below!) and lean-in circles where we can support and learn from each other on what it’s like day-to-day at Lyft.</p><figure><img alt="Picture: An in-office Data Science social event" src="https://cdn-images-1.medium.com/max/848/1*ciW4-7jfRYx0dsb_uFImcg.png" /></figure><p><strong>How would you describe work-life balance at Lyft?</strong></p><p>Lyft promotes work-life balance in many different ways. In addition to generous personal time off and company-wide holidays, Lyft sets recharge time around the November/December holidays for all employees to be offline. Scientists are encouraged to take this time to do whatever helps them recharge, whether that be spending time with family, traveling, or just relaxing!</p><p>Another favorite benefit of Lyft employees is the sabbatical! After hitting your 5 year mark at Lyft, we encourage team members to take a multi-week sabbatical to enjoy some extended time off after their hard work.</p><p>Many employees also participate in extracurricular activities together after work. One great example is the Lyft soccer team, which is a community of Lyft team members who enjoy playing soccer on weekday nights. The team participates in a recurring quarterly corporate league in San Francisco with weekly games, featuring other Bay Area tech companies like LinkedIn, Cruise, and Plaid.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/980/1*ohGkEL6SwkNOESjgxUHA5A.png" /></figure><p>Similarly, the Lyft Chess Team competes in an online corporate league against 40+ companies in sectors ranging from tech, finance, to consulting. The team also likes to play in the office whenever they get a chance!</p><figure><img alt="Picture: Two chess players in the middle of a chess game" src="https://cdn-images-1.medium.com/max/970/1*Ny8uztjYa_coNFGu-i1l3Q.png" /></figure><p><strong>What is diversity at Lyft like?</strong></p><p>Diversity is a large focus for Lyft Science with many ongoing efforts to maintain and improve the diversity and inclusion among our scientists. A key example of these efforts is DIG, which is Lyft Science’s ‘Diversity &amp; Inclusion Group’. Founded in 2018, this internal council of over 50 scientists focuses on cross-organizational efforts to ensure diversity and inclusivity throughout a scientist’s entire lifecycle at Lyft: from attracting diverse talent, to ensuring our work reaches a broad audience, to building a welcoming community and culture within. DIG volunteers work on a variety of projects such as leading internal discussions on diversity topics (e.g. being a parent at Lyft), creating blog posts to share recent Science work or increase transparency on our interview process, and planning/hosting meet-ups with external DS/ML organizations. Read more information on DIG’s efforts &amp; mission <a href="https://eng.lyft.com/how-to-build-a-diversity-inclusion-program-that-lasts-12d7c1e42445">here</a>!</p><p>Additionally, there are several specific focus groups within Lyft that foster communities for support and discussion. For example, members of the Women in Science group have periodic meet ups and discussion groups on topics such as imposter syndrome and how to approach promotion / salary discussions.</p><figure><img alt="Picture: Lyft DIG members hosting a Women in Science cross-company conference at the Lyft office, including technical talks and a panel discussion" src="https://cdn-images-1.medium.com/max/758/1*jRw1xRAVYYT1xmfxNIhTHQ.png" /></figure><p><strong>What does growth look like at Lyft? How would my job change in 1 year? 5 years?</strong></p><p>At Lyft, there are ample resources to support career development. There are multiple levels of Data Scientists at Lyft, ranging from junior to senior roles. Each level has clearly laid-out performance expectations in a rubric format, which are shared with employees from their first day at Lyft to make promotions fair and predictable. Managers conduct frequent check-ins with employees on their performance vs these rubrics and can provide feedback and advice for future projects to develop new skills or strengthen current weaknesses. Formal reviews are conducted bi-annually, which allow scientists to reflect on their own work, compare their performance to the expectations rubric, and receive written feedback from their manager and peers.</p><p>Lyft provides the option to advance to higher Data Science roles either as a manager or an individual contributor in order to support the varying career goals across our scientists. The individual contributor track means the scientist will not take on any people-management responsibilities and will continue to focus on technical work as they progress to higher Data Science levels, while they take on a larger technical / product scope.</p><p>To help Data Scientists maximize their career growth especially early on in their career, Lyft offers a mentorship program many employees have elected to participate in. Mentorship at Lyft is a formalized program where employees are matched to mentors, typically more senior scientists, who can help provide informal guidance on a specific issue such as how to prioritize workloads or a more general topic like working towards a promotion. Additional offerings at Lyft for career development include participating in internal technical education courses (recent courses focused on optimization, causal inference, and reinforcement learning), cutting-edge research seminars by academic guests, or attending external conferences to learn about industry trends — read more about these initiatives in this <a href="https://eng.lyft.com/technical-learning-at-lyft-build-a-strong-data-science-team-a6628215513c">blog post</a>!</p><p><em>If you are interested in joining our incredible team of Data Scientists and improving people’s lives with the world’s best transportation, check out our </em><a href="https://www.lyft.com/careers"><em>careers page</em></a><em>!</em></p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fddc42f24096" width="1" /><hr /><p><a href="https://eng.lyft.com/faq-common-questions-from-candidates-during-lyft-data-science-interviews-fddc42f24096">FAQ: Common Questions from Candidates During Lyft Data Science Interviews</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/d4ca2720bda8</id>
            <title>ETA (Estimated Time of Arrival) Reliability at Lyft</title>
            <link>https://eng.lyft.com/eta-estimated-time-of-arrival-reliability-at-lyft-d4ca2720bda8?source=rss----25cd379abb8---4</link>
            <guid isPermaLink="false">https://medium.com/p/d4ca2720bda8</guid>
            <pubDate></pubDate>
            <updated>2024-06-20T15:41:12.690Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*_U9AeqAeVYhr2FxCfZbNPw.png" /></figure><h3>The ETA Conundrum: Speed vs Accuracy</h3><p>Imagine this: You’ve got a crucial early morning flight to catch — you open the Lyft app, and see an estimated pickup time on the screen. But here’s the million-dollar question: Will the driver you are paired with, arrive at the estimated time? One of Lyft’s most simple, yet profound goals is to ensure we provide riders with the most accurate ETAs we can.</p><p>Before you even hit ‘request’ and summon a ride, there are complex algorithms that sift through historical and real-time data, leveraging machine learning alongside traffic and weather insights to predict the ETA (or pickup time) to display on the rider’s screen based on the destination they input. This got us thinking — how do we determine ETA estimates as accurately as possible, before a rider requests a ride?</p><p>Enter: <strong><em>Reliability</em></strong></p><h3>What does ‘<strong><em>reliable</em>’</strong> really mean in this context?</h3><p>In the realm of ridesharing, ‘reliability’ takes on different dimensions depending on the ride’s phase — be it prior to a rider requesting a ride, after the ride has been requested, or after the details of the driver who has accepted the rider’s offer, emerge. For instance, once a driver’s information is visible, reliability translates to the accuracy of the predicted driver arrival time (displayed on the screen) compared to the driver’s factual arrival time.</p><p>So, what encompasses <strong><em>reliability</em></strong> before a ride is even requested?</p><p>Simply put — Given an ETA before a ride request, reliability is the likelihood that a driver will arrive within a reasonable timeframe around that ETA, should the ride be booked.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/732/1*LJqHAs9HBLv-fAHlnKCieA.png" /><figcaption>Rider Screen pre-request</figcaption></figure><p>Understanding and estimating this aspect of reliability is crucial for setting accurate ETAs, as it has a direct impact on the likelihood of riders canceling their bookings. Illustrated below is a simulated graph depicting the relationship between reliability and rider cancellation rates — demonstrating a higher reliability % results in lower % of cancellations.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/616/1*xhzbryDhuxTQh7EwKlkHJw.png" /><figcaption>Reliability vs Cancels graph</figcaption></figure><p>Our objective, for each ride option presented to our users, is to showcase an accurate ETA with a strong likelihood of a swift, reliable pickup.</p><h3>Unpacking ETA Uncertainties</h3><p>Before we set out to tackle this problem, it is important to understand the reasons for unreliability — i.e, why estimated times of arrival (ETAs) might differ from the actual arrival times.</p><ol><li><strong>Unpredictability of Driver Availability: </strong>At the heart of the ETA challenge is the inherent uncertainty around driver availability at the time a rider requests a ride. Our system endeavors to predict the closest and most suitable drivers who may choose to accept a ride request. Yet, there are still many variables at play: <br />– <strong>Driver Preference:</strong> Drivers have the autonomy to reject or cancel a ride based on personal preferences, impacting the ETA estimate.<br />– <strong>Driver Contention:</strong> The occurrence of several requests simultaneously vying for the same driver complicates the process of matching each ride request with a driver.<br />– <strong>Changes in Driver Status:</strong> Drivers may choose to log off unexpectedly which could alter ETAs.</li><li><strong>Organic ETA uncertainty/mapping volatility:</strong> Beyond the unpredictability of driver availability, there are other factors at play that can skew ETA accuracy:<br />– <strong>Traffic Conditions:</strong> Traffic can unpredictably affect travel times.<br />– <strong>Navigation Challenges</strong>: Unexpected detours like missed turns/ road closures can add time to the journey.<br />– <strong>GPS Volatility:</strong> GPS inaccuracies can affect the exact location of a driver or rider, impacting ETA predictions prior to request.</li><li><strong>Marketplace Dynamics:</strong> Another layer of complexity is the supply and demand dynamics within specific neighborhoods. There are instances where requests are made from areas with a lower density of available drivers. Additionally, marketplace conditions are in constant flux, with the balance of supply and demand shifting within minutes, further impacting ETA reliability.</li></ol><h3>Harnessing Machine Learning (ML) for Reliability Prediction</h3><p><em>The rest of the article delves into some technical aspects of ML and familiarity with fundamental concepts is recommended!</em></p><p>To predict the effect of selecting a certain ETA on ride reliability, we could potentially use <a href="https://en.wikipedia.org/wiki/Causal_inference">Causal Inference</a> methods leveraging historical data to predict the causal effect of different ETA settings on reliability, given actual arrival times, rider cancellations and other relevant metrics.</p><p>However, in order to automatically detect complex interactions between multiple variables (like driver behavior, ETA patterns, demand and supply conditions) without explicit specifications, we decided to harness ML. This approach enhances our ability to accurately predict ride fulfillment reliability by analyzing rich datasets, while also ensuring scalability and efficiency in our processes.</p><p>We started with the objective of developing a classification model, capable of predicting the reliability probability of ETA estimates. The goal? To arm downstream services with reliability scores for all possible ETA brackets, enabling the selection of the most accurate ETAs for our riders.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nwkrrTI8Qs6LlO_vKt9VCA.png" /><figcaption><em>Fig 1. Example classification model prototype</em></figcaption></figure><p><strong><em>But how are these reliability estimates used for ETA selection?</em></strong></p><p>Using product requirements, data insights and UX research, we set a stringent reliability Service Level Agreement (SLA) for every possible ETA estimate that we can present to our users (possible ETA brackets are pre-determined per ride type). This ensures we hit reliability targets by only selecting ETAs that meet the desired SLAs. Fig 1 illustrates a model prototype — sample inputs include possible ETAs (ranging from 1 to 10 minutes) and ride-level and marketplace features. The outputs are ETAs enhanced with model-based reliability estimates. Finally, the ETA with reliability greater than SLA is selected. In this narrative, we will focus on our approach towards reliability estimation.</p><h3>The Model</h3><p>At the core of our solution lies a tree-based classification model. While deep learning models have their advantages and are increasingly used in ridesharing for tasks like demand forecasting, route optimization, and image recognition (e.g. identifying road conditions), they are often an overkill for everyday lightweight business classification tasks.</p><p>Gradient boosting tree-based models have been a historic choice at Lyft for these purposes due to their clear interpretability, efficiency with smaller datasets, and robustness to outliers and missing values. These models excel in handling structured tabular data common in ridesharing, capturing complex, non-linear relationships and feature interactions without extensive feature preprocessing/ scaling. They require less computational resources and are straightforward to implement and maintain, facilitating rapid deployment in production environments.</p><h3>Features and Training</h3><p>Along with the ETA estimate we want to predict reliability for, we need features that would help us capture as much of the marketplace uncertainty as possible at prediction time itself -</p><ul><li><strong>Nearby Available Drivers:</strong> We identify a list of the closest drivers to a ride request and use their characteristics, such as estimated driving time, distance, and driver status (online, offline, or completing a trip) as model features. This data helps the model gauge the likelihood of each driver matching with the ride, should the ride be requested in the future.</li><li><strong>Harnessing Historical Insights: </strong>Our model integrates historical data at the regional and granular geohash level to offer a broader perspective on performance trends. Recent driver ETA estimates and match times, and number of completed and canceled rides establish historical benchmarks that help adjust predictions based on recent performance.</li><li><strong>Marketplace Features: Capturing the Pulse of Demand and Supply: </strong>Realtime neighborhood-level demand and supply indicators, such as the number of app opens, unassigned rides and driver pool counts offer a granular view of the market conditions.</li><li>To further refine model predictions, we incorporate features such as pickup/ dropoff location, temporal elements, and categorical data like which region the ride was requested.</li></ul><p><strong>Innovative Training Approach:</strong> Our training label is generated by comparing the actual request-to-driver arrival time against the ETA to produce a binary label for reliability. A unique aspect of our approach is the decision to train the model on all possible ETA estimates for each ride, rather than just the factual ETA estimates shown to riders (prior to request), i.e, each ride in the training data is duplicated n times — n = number of possible ETA estimates (eg — 1, 2, 3, … 10 minutes). This strategy helps us -</p><ol><li>Avoid negative feedback loops during training — a model trained on only factuals could progressively degrade over time.</li><li>Ensures equal representation of all possible ETA estimates which could be seen during inference.</li><li>Allows the model to learn variances in driver ETA estimation (driver ETAs which are used as model features are generated upstream by another service and may not always be accurate).</li></ol><p><strong>Evaluating Model Performance: </strong>We use Area Under the Curve (AUC) metric to evaluate model performance since it evaluates performance across all thresholds and not just a single one (which is useful since we utilize raw probabilities for our use case).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/514/1*tCMryj0DKimz9SDmJCBMuw.png" /><figcaption>AUC Curve for the Reliability Model</figcaption></figure><p>We also look at performance per ETA bracket — the model bias is generally small but increases for larger ETAs (owing to smaller % of rides with say ETA &gt; 15 minutes).</p><h3>Beyond Prediction: Ensuring Sustained Performance</h3><p>Ensuring that a model meets our use case upon training and deployment is crucial, but maintaining its performance over time in the dynamic rideshare environment presents a unique challenge. Consider how a model trained before significant societal shifts, such as the pandemic, would struggle as commuting patterns evolve dramatically. Similarly, updates to the Lyft app itself can impact the functionality of its components, including predictive models. This necessitates a robust system for continuous monitoring of features and performance to identify and address any degradation promptly.</p><p>It’s often hard to pinpoint the root cause of the model degradation, but most times, a simple retrain on fresh data can often mitigate performance declines. Thankfully, Lyft’s advanced ML Platform called LyftLearn composed of <a href="https://eng.lyft.com/powering-millions-of-real-time-decisions-with-lyftlearn-serving-9bb1f73318dc">model serving</a>, <a href="https://eng.lyft.com/lyftlearn-ml-model-training-infrastructure-built-on-kubernetes-aef8218842bb">training</a>, CI/CD, <a href="https://eng.lyft.com/ml-feature-serving-infrastructure-at-lyft-d30bf2d3c32a">feature serving</a>, and <a href="https://eng.lyft.com/full-spectrum-ml-model-monitoring-at-lyft-a4cdaf828e8f">model monitoring</a> functionality equips us with necessary tools to establish drift detection alarms and automated retraining pipelines seamlessly.</p><h3>What’s Next?</h3><p>In addition to focusing on our primary regions, we are broadening our analysis to include unique markets (e.g. complex marketplaces like airports) to incorporate more nuanced signals into the model. We are also integrating more real-time signals to better capture dynamic marketplace conditions. As we continue to refine our predictive models and strategies, our goal remains clear: to enhance the reliability of our service and uphold our commitment to providing riders with accurate and trustworthy information.</p><h3>Acknowledgements</h3><p>A huge thank you to all the members of the Offer Generation and Offer Selection teams at Lyft for making this happen!</p><p>If you are excited to be part of the Lyft team, explore opportunities on our <a href="https://www.lyft.com/careers">careers page</a>. Join us on our journey to improve people’s lives with the world’s best transportation!</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d4ca2720bda8" width="1" /><hr /><p><a href="https://eng.lyft.com/eta-estimated-time-of-arrival-reliability-at-lyft-d4ca2720bda8">ETA (Estimated Time of Arrival) Reliability at Lyft</a> was originally published in <a href="https://eng.lyft.com">Lyft Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>