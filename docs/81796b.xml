<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>Latest Results</title>
        <link>http://link.springer.com</link>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-025-00919-7</id>
            <title>HMI: hierarchical knowledge management for efficient multi-tenant inference in pretrained language models</title>
            <link>https://link.springer.com/article/10.1007/s00778-025-00919-7</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-025-00919-7</guid>
            <pubDate></pubDate>
            <updated>2025-05-14</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>The significant computational demands of pretrained language models (PLMs), which often require dedicated hardware, present a substantial challenge in serving them efficiently, especially in multi-tenant environments. To address this, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant Inference system, designed to manage tenants with distinct PLMs resource-efficiently. Our approach is three-fold: Firstly, we categorize PLM knowledge into general, domain-specific, and task-specific. Leveraging insights on knowledge acquisition across different model layers, we construct hierarchical PLMs (hPLMs) by extracting and storing knowledge at different levels, significantly reducing GPU memory usage per tenant. Secondly, we establish hierarchical knowledge management for hPLMs generated by various tenants in HMI. We manage domain-specific knowledge with acceptable storage increases by constructing and updating domain-specific knowledge trees based on frequency. We manage task-specific knowledge within limited GPU memory through parameter swapping. Finally, we propose system optimizations to enhance resource utilization and inference throughput. These include fine-grained pipelining via hierarchical knowledge prefetching to overlap CPU and I/O operations with GPU computations, and optimizing parallel implementations with batched matrix multiplications. Our experimental results demonstrate that the proposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a single GPU, with only a negligible compromise in accuracy.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-025-00926-8</id>
            <title>Efficiently Counting Four-Node Motifs in Large-Scale Temporal Graphs</title>
            <link>https://link.springer.com/article/10.1007/s00778-025-00926-8</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-025-00926-8</guid>
            <pubDate></pubDate>
            <updated>2025-05-14</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Temporal motifs are compact subgraph patterns that recur frequently within a sequence of timestamps. They reveal implicit insights in the graph data and guide informed decision-making. However, current methods for exactly counting temporal motifs face challenges of high time complexity and inapplicability when motifs involve four nodes and struggle to scale to larger temporal graphs. In this paper, we propose a novel and exact counting framework tailored to 4-node, 3-edge, and 4-edge single-interaction temporal motifs whose time window size is constrained in a fixed interval. To speed up the counting process, we begin by categorizing all 4-node temporal motifs based on their structural characteristics. Subsequently, we present three rapid and precise sub-algorithms, each dedicated to counting motifs within its category. To expedite the counting process, we implement a series of straightforward and highly effective counters. Our algorithm cleverly uses these counters to identify and record all temporal motif instances based on the information and interrelationships of edges, significantly enhancing counting efficiency, especially for large-scale temporal graphs. Our extensive experiments on 14 large-scale real-world temporal graphs demonstrate the superiority of our work in terms of efficiency. Results show that our work significantly outperforms all state-of-the-art baselines and achieves a remarkable speedup of up to 25,816-fold.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-025-00920-0</id>
            <title>Model reusability in Reinforcement Learning</title>
            <link>https://link.springer.com/article/10.1007/s00778-025-00920-0</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-025-00920-0</guid>
            <pubDate></pubDate>
            <updated>2025-05-12</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>The ability to reuse trained models in Reinforcement Learning (RL) holds substantial practical value in particular for complex tasks. While model reusability is widely studied for supervised models in data management, to the best of our knowledge, this is the first ever principled study that is proposed for RL. To capture trained policies, we develop a framework based on an expressive and lossless graph data model that accommodates Temporal Difference Learning and Deep-RL based RL algorithms. Our framework is able to capture arbitrary reward functions that can be composed at inference time. The framework comes with theoretical guarantees and shows that it yields the same result as policies trained from scratch. We design a parameterized algorithm that strikes a balance between efficiency and quality w.r.t cumulative reward. Our experiments with two common RL tasks (query refinement and robot movement) corroborate our theory and show the effectiveness and efficiency of our algorithms.
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-025-00913-z</id>
            <title>Oltp in the cloud: architectures, tradeoffs, and cost</title>
            <link>https://link.springer.com/article/10.1007/s00778-025-00913-z</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-025-00913-z</guid>
            <pubDate></pubDate>
            <updated>2025-05-12</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>What is the best architecture for cloud OLTP systems? How costly is it to run a specific workload? Which and how many hardware instances should be provisioned? To answer such questions systematically, we develop an analytical model framework for cloud OLTP. It enables the analysis of a wide variety of workloads and determines the cost-optimal architecture and hardware configuration for each. Workloads are specified in terms of dataset size, performance, latency, availability and durability requirements. System designs are evaluated based on the CPU, memory, storage, and network resources they require. We study a concrete model instance that is calibrated with the LeanStore storage engine and real-world hardware/service options and prices from AWS, one of the major cloud providers. Our analysis yields several observations on how to achieve fast, durable and cost-efficient OLTP in the cloud.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-025-00922-y</id>
            <title>DBSP: automatic incremental view maintenance for rich query languages</title>
            <link>https://link.springer.com/article/10.1007/s00778-025-00922-y</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-025-00922-y</guid>
            <pubDate></pubDate>
            <updated>2025-05-08</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Incremental view maintenance (IVM) has long been a central problem in database theory and practice. Many solutions have been proposed for restricted classes of database languages (such as the relational algebra or Datalog), restricted classes of queries, and restricted classes of database changes. In this paper we give a general, heuristic-free solution to this problem in 4 steps: (1) we describe a simple but expressive language called DBSP for describing computations over data streams; (2) we give a new mathematical definition of IVM using DBSP; (3) we give an algorithm for converting any DBSP program into an incremental program; this algorithm reduces the problem of incrementalizing a complex query to the problem of incrementalizing the primitive operations that compose the query. Finally, (4) we show that practical database query languages, such as SQL and Datalog, can be directly implemented on top of DBSP, using primitives that have efficient incremental implementations. As a consequence, we obtain a general recipe for efficient IVM for essentially arbitrary queries written in all these languages.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-025-00923-x</id>
            <title>Transactional panorama: a conceptual framework for user perception in analytical visual interfaces (extended version)</title>
            <link>https://link.springer.com/article/10.1007/s00778-025-00923-x</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-025-00923-x</guid>
            <pubDate></pubDate>
            <updated>2025-05-08</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Many tools empower analysts and data scientists to consume analysis results in a visual interface. When the underlying data changes, these results need to be updated, but this update can take a long time—all while the user continues to explore the results. Tools can either (i) hide away results that haven’t been updated, hindering exploration; (ii) make the updated results immediately available to the user (on the same screen as old results), leading to confusion and incorrect insights; or (iii) present old—and therefore stale—results to the user during the update. To help users reason about these options and others, and make appropriate trade-offs, we introduce Transactional Panorama, a formal framework that adopts transaction s to jointly model the system refreshing the analysis results and the user interacting with them. We introduce three key properties that are important for user perception in this context: visibility (allowing users to continuously explore results), consistency (ensuring that results presented are from the same version of the data), and monotonicity (making sure that results don’t “go back in time”). Within transactional panorama, we characterize all feasible property combinations, design new mechanisms (that we call <i>lenses</i>) for presenting analysis results to the user while preserving a given property combination, formally prove their relative orderings for various performance criteria, and discuss their use cases. We propose novel algorithms to preserve each property combination and efficiently present fresh analysis results. We implement our framework into a popular, open-source BI tool, illustrate the relative performance implications of different lens es, and demonstrate the benefits of the novel lens es and our optimizations.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-025-00912-0</id>
            <title><span>prompt4vis</span>: prompting large language models with example mining for tabular data visualization</title>
            <link>https://link.springer.com/article/10.1007/s00778-025-00912-0</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-025-00912-0</guid>
            <pubDate></pubDate>
            <updated>2025-05-02</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>We are currently in the epoch of Large Language Models (LLMs), which have transformed numerous technological domains within the database community. In this paper, we examine the application of LLMs in text-to-visualization (text-to-vis). The advancement of natural language processing technologies has made natural language interfaces more accessible and intuitive for visualizing tabular data. However, despite utilizing advanced neural network architectures, current methods such as Seq2Vis, ncNet, and RGVisNet for transforming natural language queries into DV commands still underperform, indicating significant room for improvement. In this paper, we introduce <span>Prompt4Vis</span>, a novel framework that leverages LLMs and In-context learning to enhance the generation of data visualizations from natural language. Given that In-context learning’s effectiveness is highly dependent on the selection of examples, it is critical to optimize this aspect. Additionally, encoding the full database schema of a query is not only costly but can also lead to inaccuracies. This framework includes two main components: (1) an example mining module that identifies highly effective examples to enhance In-context learning capabilities for text-to-vis applications, and (2) a schema filtering module designed to streamline database schemas. Comprehensive testing on the <i>NVBench</i> dataset has shown that <span>Prompt4Vis</span> significantly outperforms the current state-of-the-art model, RGVisNet, by approximately 35.9% on development sets and 71.3% on test sets. To the best of our knowledge, <span>Prompt4Vis</span> is the first framework to incorporate In-context learning for enhancing text-to-vis, marking a pioneering step in the domain.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-024-00899-0</id>
            <title>DBOS: three years later</title>
            <link>https://link.springer.com/article/10.1007/s00778-024-00899-0</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-024-00899-0</guid>
            <pubDate></pubDate>
            <updated>2025-04-29</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>In our VLDB 2022 publication (Skiadopoulos, 2022), we presented the rationale for a DBMS-oriented operating system and reported a series of experiments supporting this approach. This paper provides a comprehensive update on the project, which evolved as a research initiative for two additional years before transitioning into a venture-capital-backed startup over the past 18 months. During this period, we developed a provenance system and a programming environment integrating Python and TypeScript, accompanied by detailed performance evaluations. Furthermore, we outline the modifications made to the research prototype to support the demands of commercialization.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-025-00917-9</id>
            <title>Table integration in data lakes unleashed: pairwise integrability judgment, integrable set discovery, and multi-tuple conflict resolution</title>
            <link>https://link.springer.com/article/10.1007/s00778-025-00917-9</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-025-00917-9</guid>
            <pubDate></pubDate>
            <updated>2025-04-28</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Table integration aims to create a comprehensive table by consolidating tuples containing relevant information. In this work, we investigate the challenge of integrating multiple tables from a data lake, focusing on three core tasks: (1) <i>pairwise integrability judgment</i>, which determines whether a tuple pair is integrable, accounting for any occurrences of semantic equivalence or typographical errors; (2) <i>integrable set discovery</i>, which identifies all integrable sets in a table based on pairwise integrability judgments established in the first task; (3) <i>multi-tuple conflict resolution</i>, which resolves conflicts between multiple tuples during integration. To this end, we train a binary classifier to address the task of pairwise integrability judgment. Given the scarcity of labeled data in data lakes, we propose a self-supervised adversarial contrastive learning algorithm to perform classification, which incorporates data augmentation methods and adversarial examples to autonomously generate new training data. Upon the output of pairwise integrability judgment, each integrable set can be considered as a community—a densely connected sub-graph where nodes and edges correspond to tuples in the table and their pairwise integrability, respectively—we proceed to investigate various community detection algorithms to address the integrable set discovery objective. Moving forward to tackle <i>multi-tuple conflict resolution</i>, we introduce an innovative in-context learning methodology. This approach capitalizes on the knowledge embedded within large language models to effectively resolve conflicts that arise when integrating multiple tuples. Notably, our method minimizes the need for annotated data, making it particularly suited for scenarios where labeled datasets are scarce. Since no suitable test collections are available for our tasks, we develop our own benchmarks using two real-world dataset repositories: <i>Real</i> and <i>Join</i>. We conduct extensive experiments on these benchmarks to validate the robustness and applicability of our methodologies in the context of integrating tables within data lakes.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-025-00914-y</id>
            <title>GRELA: Exploiting graph representation learning in effective approximate query processing</title>
            <link>https://link.springer.com/article/10.1007/s00778-025-00914-y</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-025-00914-y</guid>
            <pubDate></pubDate>
            <updated>2025-04-24</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Approximate query processing (<b>AQP</b>) plays a critical role in modern data analytics. Although machine learning models are used for AQP, existing methods fail to uncover implicit relationships among the underlying data, the aggregate functions in queries, and the query predicates. In this work, we propose a Graph REpresentation Learning-based AQP model (<b>GRELA</b> for short) for answering queries with multiple aggregate functions. GRELA models the aggregate functions and the query predicates as task and clause nodes respectively in a graph and then learns appropriate node representations via its two modules. In particular, the $$\texttt {Encoder}$$ module coalesces query predicates and underlying data into the representations of clause nodes. The $$\mathbf {\texttt {Graph}}$$ module bridges task nodes and clause nodes such that each task node can aggregate the information from its neighborhood into its representation. Through the inner products of clause and task representations, GRELA is able to make accurate estimates for queries with multiple aggregate functions. Extensive experimental results verify that GRELA outperforms the state-of-the-art AQP methods on different kinds of datasets.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-025-00915-x</id>
            <title>Efficient indexing and searching of constrained core in hypergraphs</title>
            <link>https://link.springer.com/article/10.1007/s00778-025-00915-x</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-025-00915-x</guid>
            <pubDate></pubDate>
            <updated>2025-04-22</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Hypergraphs are generalized graph models that capture high-order relationships through hyperedges that contain arbitrary vertices. In large-scale hypergraphs, it is common to observe global sparsity and local density, which makes identifying the dense substructures a fundamental task in graph mining. This paper introduces a framework called (<i>k</i>, <i>p</i>)-PoolCore for searching constrained cohesive subgraphs in hypergraphs, with <i>k</i> representing the degree constraint of vertices and <i>p</i> indicating the size constraint of hyperedges. We theoretically analyze the monotonicity and hierarchical properties of (<i>k</i>, <i>p</i>)-PoolCore. To capitalize on these properties, we propose a tree-based PoolCore index that organizes all (<i>k</i>, <i>p</i>)-PoolCores within trees to facilitate efficient queries. Furthermore, we optimize this index to establish a list-based PoolCore index, which offers <i>O</i>(1) query time complexity in most cases without additional space complexity in large-scale hypergraphs. Our extensive experiments and case studies on real-world hypergraphs demonstrate the advantages of (<i>k</i>, <i>p</i>)-PoolCore in hypergraph decomposition and modeling cohesiveness substructures. The proposed tree-based PoolCore index achieves a remarkable $$10^6$$ speedup compared to the basic computation method. Additionally, the optimized list-based PoolCore index further accelerates querying by at least 100x while maintaining space-complexity efficiency and scalability in real-world hypergraphs.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-024-00886-5</id>
            <title>LIST: learning to index spatio-textual data for embedding based spatial keyword queries</title>
            <link>https://link.springer.com/article/10.1007/s00778-024-00886-5</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-024-00886-5</guid>
            <pubDate></pubDate>
            <updated>2025-04-07</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>With the proliferation of spatio-textual data, Top-k KNN spatial keyword queries (TkQs), which return a list of objects based on a ranking function that considers both spatial and textual relevance, have found many real-life applications. To efficiently handle TkQs, many indexes have been developed, but the effectiveness of TkQ is limited. To improve effectiveness, several deep learning models have recently been proposed, but they suffer severe efficiency issues and there are no efficient indexes specifically designed to accelerate the top-k search process for these deep learning models. To tackle these issues, we consider embedding based spatial keyword queries, which capture the semantic meaning of query keywords and object descriptions in two separate embeddings to evaluate textual relevance. Although various models can be used to generate these embeddings, no indexes have been specifically designed for such queries. To fill this gap, we propose LIST, a novel machine learning based Approximate Nearest Neighbor Search index that Learns to Index the Spatio-Textual data. LIST utilizes a new learning-to-cluster technique to group relevant queries and objects together while separating irrelevant queries and objects. There are two key challenges in building an effective and efficient index, i.e., the absence of high-quality labels and the unbalanced clustering results. We develop a novel pseudo-label generation technique to address the two challenges. Additionally, we introduce a learning based spatial relevance model that can integrate with various text relevance models to form a lightweight yet effective relevance for reranking objects retrieved by LIST. Experimental results show that (1) our lightweight embedding based relevance model significantly outperforms state-of-the-art relevance models; (2) LIST outperforms state-of-the-art indexes, providing a better trade-off between effectiveness and efficiency.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-025-00907-x</id>
            <title>VUS: effective and efficient accuracy measures for time-series anomaly detection</title>
            <link>https://link.springer.com/article/10.1007/s00778-025-00907-x</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-025-00907-x</guid>
            <pubDate></pubDate>
            <updated>2025-03-27</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Anomaly detection (AD) is a fundamental task for time-series analytics with important implications for the downstream performance of many applications. In contrast to other domains where AD mainly focuses on point-based anomalies (i.e., outliers in standalone observations), AD for time series is also concerned with range-based anomalies (i.e., outliers spanning multiple observations). Nevertheless, it is common to use traditional point-based information retrieval measures, such as Precision, Recall, and F-score, to assess the quality of methods by thresholding the anomaly score to mark each point as an anomaly or not. However, mapping discrete labels into continuous data introduces unavoidable shortcomings, complicating the evaluation of range-based anomalies. Notably, the choice of evaluation measure may significantly bias the experimental outcome. Despite over six decades of attention, there has never been a large-scale systematic quantitative and qualitative analysis of time-series AD evaluation measures. This paper extensively evaluates quality measures for time-series AD to assess their robustness under noise, misalignments, and different anomaly cardinality ratios. Our results indicate that measures producing quality values independently of a threshold (i.e., AUC-ROC and AUC-PR) are more suitable for time-series AD. Motivated by this observation, we first extend the AUC-based measures to account for range-based anomalies. Then, we introduce a new family of parameter-free and threshold-independent measures, volume under the surface (VUS), to evaluate methods while varying parameters. We also introduce two optimized implementations for VUS that reduce significantly the execution time of the initial implementation. Our findings demonstrate that our four measures are significantly more robust in assessing the quality of time-series AD methods.
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-025-00906-y</id>
            <title>Join optimization revisited: a novel DP algorithm for join&amp;sort order selection</title>
            <link>https://link.springer.com/article/10.1007/s00778-025-00906-y</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-025-00906-y</guid>
            <pubDate></pubDate>
            <updated>2025-03-26</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Join order selection has been widely studied, and the widely used algorithm to find the optimal join order is Dynamic Programming (DP). However, it is also known that the existing DP algorithms cannot deal with the so-called interesting order (e.g., sort order), or the algorithm to consider sort order together with joins will violate the optimal substructure behind DP. As a result, it is difficult for DBMSs to find the optimal join order given sort orders, as it comes with extremely high overhead. In this paper, we study a novel DP algorithm to find the optimal join order by taking sort orders into consideration. We call it a join&amp;sort orders selection problem, which is to minimize the total join&amp;sort cost to process a join query. This problem is challenging, because both the join order selection and the sort order selection for a given join-tree are known to be NP-hard. In addition, join&amp;sort orders are dependent in the sense that the change of one order affects the selection of the other. We show that the optimal substructure exists in dealing with join&amp;sort orders selection by DP under some simple condition, which we call $$\varOmega $$-condition. The $$\varOmega $$-Condition is not a condition to restrict join queries to optimize, but is a condition that allows us to find the optimal for any join queries. We present DP algorithms for bushy and linear join trees, discussing the pruning techniques and the complexity of the algorithms. We conduct extensive experimental studies to show the efficiency and robustness of our approach.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-025-00911-1</id>
            <title>Data formats in analytical DBMSs: performance trade-offs and future directions</title>
            <link>https://link.springer.com/article/10.1007/s00778-025-00911-1</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-025-00911-1</guid>
            <pubDate></pubDate>
            <updated>2025-03-19</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>This paper evaluates the suitability of Apache Arrow, Parquet, and ORC as formats for subsumption in an analytical DBMS. We systematically identify and explore the high-level features that are important to support efficient querying in modern OLAP DBMSs and evaluate the ability of each format to support these features. We find that each format has trade-offs that make it more or less suitable for use as a format in a DBMS and identify opportunities to more holistically co-design a unified in-memory and on-disk data representation. Notably, for certain popular machine learning tasks, none of these formats perform optimally, highlighting significant opportunities for advancing format design. Our hope is that this study can be used as a guide for system developers designing and using these formats, as well as provide the community with directions to pursue for improving these common open formats.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-025-00910-2</id>
            <title>Mixed covers: optimizing updates and queries using minimal keys and functional dependencies</title>
            <link>https://link.springer.com/article/10.1007/s00778-025-00910-2</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-025-00910-2</guid>
            <pubDate></pubDate>
            <updated>2025-03-17</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Covers for a set of functional dependencies (FDs) are fundamental for many areas of data management, such as integrity maintenance, query optimization, database design, and data cleaning. When declaring integrity constraints, keys enjoy native support in database systems while FDs need to be enforced by triggers or at application level. Consequently, maximizing the use of keys will provide the best support. We propose the new notion of mixed cover for a set of FDs, comprising the set of minimal keys together with a cover for the set of non-key FDs implied by the FD set. We establish sequential and parallel algorithms for computing mixed covers from a given set of FDs, and illustrate that they complement each other in terms of their performance. Even though FD covers are typically smaller in number or size than their corresponding mixed cover, the latter generate orders of magnitude lower overheads during integrity maintenance. We also quantify how mixed covers improve the performance of query, refresh and insert operations on the TPC-H benchmark under different constraint workloads and scaling factors. Finally, we illustrate the growth of performance improvement for optimal over minimal-reduced covers, justifying the significant time required for computing the former.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-025-00909-9</id>
            <title>Cost-aware prediction service pricing with incomplete information</title>
            <link>https://link.springer.com/article/10.1007/s00778-025-00909-9</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-025-00909-9</guid>
            <pubDate></pubDate>
            <updated>2025-03-06</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Trading the machine learning-based prediction services has been up-and-coming for individuals and small companies. It serves to directly provide the predictions, e.g., classifications, for consumers without domain knowledge. Existing prediction service pricing methods closely rely on the strong assumption of <i>completely known</i> information on service quality and consumers’ valuations. In this paper, we study the <i>profit maximization problem of pricing prediction services under incomplete information for the first time</i>. We propose a novel Service Market model, named SMELT, considering <i>multiple types of customers</i> with dEmand and quaLity-aware valuaTions. We first derive the <i>theoretical optimal</i> solution to maximize service profit with complete information. Then, we develop an effective framework PSPricer under the <i>profit ratio guarantee</i> to solve the profit maximization problem with incomplete information. It is capable of not only efficiently getting the sub-optimal service price with <i>bounded</i> revenue loss, but also effectively learning the service quality function with the maximum likelihood estimation. Moreover, due to the resource-intensive and costly characteristic of machine learning model inference, we further extend the SMELT model to consider the inevitable <i>inference cost</i> in service trading. We formulate a novel <i>cost-aware</i> profit maximization problem and derive the general optimal solution. The PSPricer framework is <i>tailored</i> with an effective <i>heuristic</i> to maximize the cost-aware profit with the theoretical profit ratio guarantee. Extensive experiments on real-life datasets demonstrate our theoretical findings and the effectiveness and efficiency of PSPricer in various settings, compared with the state-of-the-art approaches.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-025-00908-w</id>
            <title>Efficient and scalable huge embedding model training via distributed cache management</title>
            <link>https://link.springer.com/article/10.1007/s00778-025-00908-w</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-025-00908-w</guid>
            <pubDate></pubDate>
            <updated>2025-03-05</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Embedding models have been an effective learning paradigm for high-dimensional data. However, one open issue of embedding models is that their representations (latent factors) often result in large parameter space. We observe that existing distributed training frameworks face a scalability issue of embedding models since updating and retrieving the shared embedding parameters from servers usually dominates the training cycle. In this paper, we propose HET, a new system framework that significantly improves the scalability of huge embedding model training. We embrace skewed popularity distributions of embeddings as a performance opportunity and leverage it to address the communication bottleneck with an <i>embedding cache</i>. To ensure consistency across the caches, we incorporate a new consistency model into HET design, which provides fine-grained consistency guarantees on a per-embedding basis. Compared to previous work that only allows staleness for read operations, HET also utilizes staleness for write operations. Evaluations on six representative tasks show that HET achieves up to 88% embedding communication reductions and up to $$20.68\times $$ performance speedup over the state-of-the-art baselines.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-025-00903-1</id>
            <title>Querying historical K-cores in large temporal graphs</title>
            <link>https://link.springer.com/article/10.1007/s00778-025-00903-1</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-025-00903-1</guid>
            <pubDate></pubDate>
            <updated>2025-02-24</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Many real-world relationships between entities can be modeled as temporal graphs, where each edge is associated with a timestamp or a time interval representing its occurrence. The <i>k</i>-core is a fundamental model used to capture cohesive subgraphs in a simple graph and have drawn much research attention over the last decade. Despite widespread research, none of the existing works support the efficient querying of historical <i>k</i>-cores in temporal graphs. In this paper, given an integer <i>k</i> and a time window, we study the problem of computing all the nodes belonging to the k-core in the graph snapshot over the time window. We propose an index-based solution and several pruning strategies to reduce the index size. We design a novel algorithm to construct this index, whose running time is linear to the final index size. We also propose algorithms to maintain our index given the continuous arrival of new edges. Lastly, we conducted extensive experiments on several real-world temporal graphs to show the high effectiveness of our index-based solution.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://link.springer.com/10.1007/s00778-024-00898-1</id>
            <title>Netherite: efficient execution of serverless workflows</title>
            <link>https://link.springer.com/article/10.1007/s00778-024-00898-1</link>
            <guid isPermaLink="false">https://link.springer.com/10.1007/s00778-024-00898-1</guid>
            <pubDate></pubDate>
            <updated>2025-02-21</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Serverless applications are popular because they can provide scalability and load-based billing with minimal developer effort. Beyond simple stateless functions (FaaS), modern serverless programming models such as Durable Functions (DF) now provide stateful abstractions, such as workflows, tasks and actors, whose state and progress is implicitly and continuously persisted. This automatic resilience greatly simplifies development, but also creates performance challenges, such as a large number of IOps. To address these challenges, we introduce Netherite, a novel architecture for executing serverless workflows on an elastic cluster. Netherite groups the numerous application objects into a smaller number of partitions, and pipelines the state persistence of each partition. This improves latency and throughput, as it enables workflow steps to group commit, even if causally dependent. Moreover, Netherite leverages FASTER’s hybrid log approach to support larger-than-memory application state, and to enable efficient partition movement between compute hosts. Our evaluation shows that (a) Netherite achieves lower latency and higher throughput than the original DF engine, by more than an order of magnitude in some cases, and (b) that Netherite has lower latency than some commonly used alternatives, like AWS Step Functions or cloud storage triggers.</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>