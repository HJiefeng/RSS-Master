<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>PyTorch - Medium</title>
        <link>https://medium.com/pytorch?source=rss----512b8efdf2e7---4</link>
        
        <item>
            <id>https://medium.com/p/9fda42d6c6a0</id>
            <title>Exploring scientific machine learning pipelines through the SimulAI toolkit</title>
            <link>https://medium.com/pytorch/exploring-scientific-machine-learning-pipelines-through-the-simulai-toolkit-9fda42d6c6a0?source=rss----512b8efdf2e7---4</link>
            <guid isPermaLink="false">https://medium.com/p/9fda42d6c6a0</guid>
            <pubDate></pubDate>
            <updated>2024-02-19T14:41:15.413Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <h3>Overview</h3><p>SciML, short for Scientific Machine Learning, encompasses work that merges quantitative sciences with machine learning. It has gained significant traction over the past decade, driven by the widespread availability of specialized hardware (such as GPUs and TPUs) and datasets. Additionally, it has been propelled by the overarching influence of the machine learning wave, now ingrained in the zeitgeist of our times. In this context, we’d like to introduce <a href="https://github.com/IBM/simulai">SimulAI</a>, an open-source toolkit under the Apache 2.0 license. SimulAI is designed to be user-friendly, providing a high-level Python interface for managing scientific machine learning pipelines. This article aims to showcase its current workflow and utility in constructing scientific experiments. We encourage feedback and potential contributions from the interested community, with plans to delve into more advanced topics in future articles.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*27jUhQaA_obydtDTFP7xNw.png" /><figcaption>Figure 1. Overall workflow of the instantiation and execution of pipelines in SimulAI</figcaption></figure><h3>SciML</h3><p>The past decade has seen the emergence of a groundbreaking field known as <a href="https://www.sciencedirect.com/science/article/abs/pii/S0021999118307125">Physics-Informed Neural Networks</a> (PINNs). These have reignited the interest of the research community in leveraging neural networks to solve ordinary and partial differential equations, commonly encountered in physics. Following PINNs, a new wave of innovation brought forth the concept of <a href="https://en.wikipedia.org/wiki/Neural_operators">neural operators</a> . This includes advancements like <a href="https://www.nature.com/articles/s42256-021-00302-5">Deep Operator Networks</a> (DeepONets) and <a href="https://arxiv.org/abs/2010.08895">Fourier Neural Operators</a> (FNO). These developments have enabled the modeling of intricate mathematical operations, expanding the versatility of neural networks to address multi-scenario problems. Subsequent to these milestones, numerous enhancements have been made across various auxiliary algorithms. These include the introduction of new loss functions, <a href="https://iopscience.iop.org/article/10.1088/2632-2153/ac3712/pdf">penalization methods</a>, and even <a href="https://global-sci.org/intro/article_detail/cicp/18403.html">domain decomposition techniques</a>.</p><p>The realm of scientific applications for deep learning continues to witness a surge in both the development of new applications and the creation of deep learning architectures. These are tailored to address scientific problems by incorporating both data and mathematical expressions into the training loop. The spectrum of application areas has expanded significantly, ranging from <a href="https://arxiv.org/pdf/2107.10711.pdf">turbulence modeling</a> and <a href="https://arxiv.org/abs/2306.04096v1">earthquake hypocenter localization</a> to <a href="https://arxiv.org/pdf/2106.05384.pdf">chemical kinetics</a>. This new wave of machine learning has not only fostered innovation in addressing specific challenges but has also spurred the creation of dedicated libraries and packages for constructing SciML workloads. Notable mentions include <a href="https://github.com/nvidia/modulus">Modulus</a> by NVIDIA, <a href="https://github.com/lululxvi/deepxde">DeepXDE</a> from Brown University, and more recently, the SimulAI toolkit, sponsored by IBM.</p><p>SimulAI aims to be a versatile and user-friendly Python package, designed to empower both research and industry communities by integrating machine learning models into their existing work. Leveraging the robust foundations of PyTorch, NumPy, and SciPy as its mathematical engines, SimulAI provides access to a diverse array of state-of-the-art models and architectures. These include <a href="https://ibm.github.io/simulai/simulai_models/simulai_models_deeponet/">DeepONets</a>, <a href="https://ibm.github.io/simulai/simulai_models/simulai_models_autoencoder/">Variational AutoEncoders</a> (VAEs), <a href="https://ibm.github.io/simulai/simulai_regression/simulai_opinf/">Operator Inference</a> (OpInf), and now extends its offerings to include <a href="https://ibm.github.io/simulai/simulai_models/simulai_models_transformer/">transformers</a> and <a href="https://ibm.github.io/simulai/simulai_models/simulai_models_unet/">U-Net</a>.</p><p>In this context, SimulAI’s current workflow will be showcased by solving a canonical problem within the SciML domain — creating a Physics-Informed Neural Network (PINN) to approximate the solution of a Partial Differential Equation (PDE). This demonstration underscores SimulAI’s capability to handle complex scientific challenges through its accessible and comprehensive set of tools and models.</p><h3>Physics-Informed Neural Networks in a Nutshell</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1GovRVIswBuWoMhY1qLI1g.png" /><figcaption>Figure 2. A general view of the PINN workflow</figcaption></figure><p>PINNs can be categorized as supervised or unsupervised learning architectures. They incorporate physics principles, expressed as governing equations, into their loss functions as penalization terms. The inclusion of these equations in the loss functions aims to enforce conservation laws since PDEs are the foundation of mathematical models that represents the physical universe. In order to illustrate it, let us consider <a href="https://arxiv.org/pdf/2203.07404.pdf">a specific case</a> of the canonical <a href="https://en.wikipedia.org/wiki/Allen%E2%80%93Cahn_equation">Allen-Cahn</a> PDE, which models phase-separation in multi-component systems. Let us consider the following PDE equation defined over a 1D domain of size <em>L</em> and a time interval <em>[0, T]</em>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/424/1*EQNJ4M8c9A0oE8Prcn40zg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/243/1*01klh-H9KQm1PA5wiTcwXw.png" /></figure><p>The PDE is constrained by the following boundary and initial conditions:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/335/1*qiHuad1Jhr7q9_wn2xPMyQ.png" /></figure><p>Considering that we can evaluate the expressions above for any sample of spatiotemporal positions within the domain, we can easily assess root mean-squared metrics (RMSE) for them. In this way, we can rewrite the mathematical expressions in the form of a loss function, as seen as following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/797/1*YuemiGq3mZDDS1z6cf94LQ.png" /></figure><p>The boundary conditions are written as subtrations between the domain extremes since they are periodic for this problem.</p><p>The lambda-penalties in the aforementioned expression play a crucial role in balancing various terms, considering potential differences in their orders of magnitude. This strategic use aims to exert control over and enhance the convergence speed during training. Once the loss function is defined, the next step involves minimizing it through an optimization algorithm. Subsequently, the model coefficients are updated accordingly. Despite the apparent simplicity of PINNs, there are intricacies in their convergence dynamics, presenting an ongoing research challenge. However, for the specific case we are addressing, a solution can be achieved without delving into more sophisticated algorithms.</p><h3>Defining PINN expressions</h3><p>The Allen-Cahn PDE, along with its corresponding initial and boundary conditions, can be succinctly expressed in Python using the SimulAI syntax.</p><pre> f = 'D(u, t) - mu*D(D(u, x), x) + alpha*(u**3) + beta*u'<br />  g_u = 'u' <br />  g_ux = 'D(u, x)'<br />  input_labels = ['x', 't']<br />  output_labels = ['u']</pre><p>Utilizing strings to represent the equations for minimization provides a clear and concise approach to express physics. However, for cases requiring increased flexibility in input representation, SimulAI supports alternative methods such as Python functions or SymPy objects within its symbolic framework.</p><h3>Instantiating a neural network</h3><p>In our experiment, we aim to train a neural network parameterized by a set of coefficients (weights and biases). This network receives coordinates (representing time and one space dimension) and predicts the corresponding value for the Allen-Cahn state variable . SimulAI offers a versatile range of pre-implemented neural network models designed to function as Physics-Informed Neural Networks (PINNs). Additionally, users can explore combinations of these models. A prime example is the special class of Multilayer Perceptron (MLP) networks, exemplified in the following template function:</p><pre>def model():<br /><br />   from simulai.models import ImprovedDenseNetwork from simulai.regression<br />   import SLFNN, ConvexDenseNetwork<br /><br />   n_inputs = len(input_labels) n_outputs = len(output_labels)<br />   <br />   # Configuration for the fully-connected network<br />   config = { &quot;layers_units&quot;: 6*[128], <br />              &quot;activations&quot;: &quot;tanh&quot;,<br />              &quot;input_size&quot;: n_inputs,<br />              &quot;output_size&quot;: n_outputs,<br />              &quot;name&quot;: &quot;net&quot;,<br />            }<br /><br />   # Instantiating the subnetworks<br />   densenet = ConvexDenseNetwork(**config)<br />   encoder_u = SLFNN(input_size=n_inputs, output_size=128, activation=&quot;tanh&quot;)<br />   encoder_v = SLFNN(input_size=n_inputs, output_size=128, activation=&quot;tanh&quot;)<br /><br />   net = ImprovedDenseNetwork(network=densenet,<br />                              encoder_u=encoder_u,<br />                              encoder_v=encoder_v,<br />                              devices=&quot;gpu&quot;)<br /><br />   return net<br /><br />net = model()<br /></pre><p>For a general overview of currently available models on the repository, take a look at <a href="https://ibm.github.io/simulai/">documentation main page</a>.</p><h3>Datasets</h3><p>The datasets used to train the PINN, as shown in the code blocks below (<em>data</em>, <em>data_boudary_xL</em>, <em>data_boundary_x0</em>, <em>data_boundary_t0</em>), consist of coordinate samples taken across the rectangular domain and can even be generated randomly. Users have the flexibility to define the construction of these input datasets. Notably, for unsupervised training, no target datasets are required, as the vanilla PINN relies solely on the governing equations and their respective boundary and initial conditions. Nevertheless, users have the option to include additional complementary datasets (measured or sensed) to create a hybrid loss function in a semi-supervised manner.</p><h3>Converting symbolic expressions into PyTorch tensor objects</h3><p>In SimulAI, mathematical expressions undergo processing and conversion into tensor-like objects using the <a href="https://github.com/IBM/simulai/blob/main/simulai/residuals/_pytorch_residuals.py">symbolic<br /> engine</a>, resulting in the creation of a ‘PDE residual’ object, as demonstrated in the code snippet below:</p><pre> residual = SymbolicOperator(expressions=[f], <br />                              input_vars=input_labels,<br />                              auxiliary_expressions={'periodic_u': g_u,<br />                                                     'periodic_du': g_ux},<br />                              constants={'mu':1e-4,<br />                                         'alpha':5,<br />                                         'beta':-5},<br />                              output_vars=output_labels,<br />                              function=net,<br />                              engine='torch',<br />                              device='gpu')</pre><p>It’s important to highlight that boundary conditions are added as auxiliary expressions, and the constants mu, alpha, and beta are provided within a dictionary. Subsequently, the residual is forwarded to the optimizer instance.</p><h3>The optimizer</h3><p>The <a href="https://github.com/IBM/simulai/blob/main/simulai/optimization/">optimization framework</a> serves as an API for both the PyTorch engine and internal optimization algorithms (currently under construction). It offers a range of configurations, including learning rate schedulers, early-stopping mechanisms, Tensorboard registers, and more.</p><pre>lr = 1e-3 # Initial learning rate<br />  optimizer_config = {'lr': lr}<br />  optimizer = Optimizer('adam', params=optimizer_config,<br />             lr_decay_scheduler_params={'name': 'ExponentialLR',<br />                                         'gamma': 0.9,<br />                                         'decay_frequency': 5_000},<br />             shuffle=False,<br />             summary_writer=True)</pre><p>In addition to the basic optimizer configuration, it is necessary to provide a dictionary with extra parameters:</p><pre>params = {'residual': residual,<br />            'initial_input': data_boundary_t0,<br />            'initial_state': u_init,<br />            'boundary_input': {'periodic_u': [data_boundary_xL,data_boundary_x0],<br />                              'periodic_du': [data_boundary_xL, data_boundary_x0]},<br />            'boundary_penalties': [1, 1],<br />            'initial_penalty': 100}</pre><p>It’s important to note that you can define fixed lambda-penalties for the loss function terms, or you can dynamically adjust them using <a href="https://github.com/IBM/simulai/blob/main/simulai/optimization/_adjusters.py">loss<br />adjusters</a>. For detailed configuration examples, refer to <a href="https://github.com/IBM/simulai/blob/main/examples/PINN/scripts/Bioreactor_Multifidelity_AutoTimeStep_mod_weighting.py">this<br />example</a>. Boundary conditions require special handling, especially for periodic conditions in the context of Allen-Cahn. Finally, to initiate training, use the <em>fit</em> method of the optimizer class.</p><pre>optimizer.fit(op=net,<br />             input_data=data,<br />             n_epochs=100_000,<br />             loss=&quot;pirmse&quot;, params=params,<br />             device='gpu')</pre><p>For a detailed walkthrough of the process, briefly touched on here, please refer to <a href="https://github.com/IBM/simulai/blob/main/examples/PINN/notebooks/allen_cahn_pinn.ipynb">notebook</a>.</p><h3>Evaluations using the model</h3><p>Now that our neural network is trained, we can utilize it for making predictions. To do so, we first define an evaluation spatiotemporal grid with any resolution:</p><pre>X_DIM_F = 500<br />T_DIM_F = 500<br /><br />x_f = np.linspace(*x_interval, X_DIM_F)<br />t_f = np.linspace(*t_interval, T_DIM_F)<br /><br />T_f, X_f = np.meshgrid(t_f, x_f, indexing='ij')<br /><br />data_f = np.hstack([X_f.flatten()[:, None],<br />                    T_f.flatten()[:, None]])</pre><p>Next, we use the <em>eval </em>method, common to all <em>NetworkTemplate</em> objects.</p><pre>approximated_data = net.eval(input_data=torch.from_numpy(data_f.astype(&quot;float32&quot;)))<br />U_f = approximated_data.reshape(T_DIM_F, X_DIM_F)</pre><p>Figure 2 illustrates a sample result from the neural network training.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/543/1*KyKDMJAk1KRVZJ790KndRg.png" /><figcaption>Figure 3. Approximate solution for the Allen-Cahn PDE using a PINN.</figcaption></figure><h3>Saving and restoring the model</h3><p>Once the neural network is trained, it can be saved using the built-in save/restore mechanism:</p><pre>saver = SPFile(compact=False)<br />saver.write(save_dir='.', name='allen_cahn_net', model=net, template=model)</pre><p>This operation ensures the model’s storage in a dedicated directory, comprising a file for the model coefficients (with the extension <em>pth</em>), a Python module housing a copy of the model template (the function <em>model </em>mentioned earlier), and potentially a pickle file for storing configuration arguments. The model is easily restored from disk as follows:</p><pre>loader = SPFile(compact=False)<br />model = loader.read(model_path=&lt;path to the model&gt;, device=&lt;destiny device&gt;)</pre><h3>Conclusion</h3><p>We trust that this concise overview has effectively covered the fundamental aspects of Physics-Informed Neural Networks (PINNs) and the toolkit’s functionalities. While leveraging PINNs to address Partial Differential Equation (PDE) systems stands as a pivotal undertaking to gauge the potency of neural networks in handling Physics datasets, it is crucial to emphasize that the realm of Scientific Machine Learning (SciML) extends well beyond PINNs. It encompasses more traditional data-driven approaches, wherein extensive datasets, either simulated or collected from sensor devices, are utilized to train algorithms tailored for diverse prediction tasks.</p><p>For instance, algorithms like neural operators can undergo training on simulated data, exhibiting prowess in tasks such as time forecasting, dimensionality reduction, and even seamlessly integrating with Physics-informed approaches in a hybrid fashion. SimulAI already accommodates many of these pipelines, and we extend an invitation to the interested community to contribute, evaluate, and offer feedback on their usage. To those eager to contribute directly through code and test cases, rest assured that your contributions are warmly welcomed and highly valued.</p><h3>Acknowledgments</h3><p>A heartfelt appreciation to IBM Research for their support and the robust infrastructure that has played a pivotal role in the success of this project. Our gratitude extends to the entire IBM Research community for fostering an environment of innovation and collaboration.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9fda42d6c6a0" width="1" /><hr /><p><a href="https://medium.com/pytorch/exploring-scientific-machine-learning-pipelines-through-the-simulai-toolkit-9fda42d6c6a0">Exploring scientific machine learning pipelines through the SimulAI toolkit</a> was originally published in <a href="https://medium.com/pytorch">PyTorch</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/26d2e4b9fd92</id>
            <title>Colossal-LLaMA-2: Low Cost and High-quality Domain-specific LLM Solution Using LLaMA and…</title>
            <link>https://medium.com/pytorch/colossal-llama-2-low-cost-and-high-quality-domain-specific-llm-solution-using-llama-and-26d2e4b9fd92?source=rss----512b8efdf2e7---4</link>
            <guid isPermaLink="false">https://medium.com/p/26d2e4b9fd92</guid>
            <pubDate></pubDate>
            <updated>2024-01-29T16:40:21.327Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <h3>Colossal-LLaMA-2: Low Cost and High-quality Domain-specific LLM Solution Using LLaMA and Colossal-AI</h3><p>The most prominent distinction between LLaMA-1 and LLaMA-2 lies in the incorporation of higher-quality corpora, a pivotal factor contributing to significant performance enhancements in LLaMA-2. This, coupled with its commercial availability, extends the potential for creative applications of large models within the open-source community.</p><p>Nevertheless, it’s widely recognized that the cost of pre-training large models from scratch is exorbitant, often humorously referred to as a domain accessible only to those with “50 million dollars” to spare. This deters many companies and developers, so how can we build our own large models at a lower cost?</p><p>Being at the forefront of cost reduction and efficiency enhancement for large models, the<a href="https://github.com/hpcaitech/ColossalAI"> <strong>Colossal-AI</strong></a> team maximizes the core capabilities of LLaMA-2. Through innovative training techniques, Colossal-AI has achieved remarkable results by utilizing only approximately <strong>0.0085 trillion tokens of data, investing 15 hours, and incurring training costs in the range of a few hundred dollars</strong>. This strategy has yielded a high-performance Chinese LLaMA-2 model that consistently outperforms competitors across multiple evaluation benchmarks.</p><p>Building upon the initial framework, the Colossal-AI team began the next iteration of the model. They constructed a more refined and comprehensive data architecture by utilizing 25 billion token data, and ultimately engineered a <strong>refined 13B model with just $5000 USD</strong>.</p><p>In contrast to the original LLaMA-2, Colossal-AI’s model not only enhances Chinese language capabilities but also further refines its proficiency in English. Remarkably, it exhibits performance levels that rival state-of-the-art (SOTA) models of similar scale within the open-source community.</p><p>Underpinning Colossal-AI’s approach are steadfast open-source principles. As a result, this model is made <strong>accessible without any commercial restrictions, with complete transparency extended to the entire training process, code, and model weights</strong>. In conjunction with this, Colossal-AI offers the comprehensive evaluation framework, <strong>ColossalEval</strong>, facilitating cost-effective reproducibility.</p><p>Moreover, the methodologies developed by Colossal-AI can be <strong>readily applied across various domains</strong>, facilitating the economical construction of large models that are pre-trained from scratch.</p><p>Open-source code and weights are available at :</p><p><a href="https://github.com/hpcaitech/ColossalAI"><strong>https://github.com/hpcaitech/ColossalAI</strong></a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*UnSO4zDgZIEocjK5" /></figure><h3>Performance</h3><h3>Colossal-LLaMA-2 7B</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*BYOQmQLqvrzxOoUO" /></figure><p>Note: Based on ColossalEval scores, the scores in parentheses are from the official leaderboard scores of the corresponding models, and C-Eval scores are from the official Leaderboard.</p><p>In commonly observed English evaluation rankings, it can be observed that in the MMLU ranking, Colossal-LLaMA-2–7B-base, with the support of low-cost continual pre-training, has overcome the problem of catastrophic forgetting. Its capabilities have steadily improved (44.47 -&gt; 53.06), showcasing outstanding performance among all 7B-scale models.</p><p>In the Chinese rankings, the primary comparisons were made against CMMLU, AGIEVAL, GAOKAO, and C-Eval. The performance of Colossal-LLaMA-2 significantly outshines other Chinese localization models based on LLaMA-2. Even when compared to other renowned models that employ Chinese language corpora and may cost millions of USD for training from scratch, Colossal-LLaMA-2 still stands out at the same scale. Notably, when compared to the original LLaMA-2, it has made a remarkable leap in Chinese language proficiency (CMMLU: 32.97 -&gt; 49.89).</p><p>In addition, <strong>fine-tuning through methods like SFT and LoRA has limitations in effectively infusing knowledge and capabilities from the base model</strong>. It doesn’t satisfactorily meet the requirements for constructing high-quality domain-specific knowledge or specialized model applications.</p><p>To better evaluate the performance of the models, the Colossal-AI team relies not only on quantitative indicators but also conducts manual evaluations on different model aspects. Below are some examples:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IDaPCqjPMNdHKyS3" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*scUlU5JyE0_roUc1" /></figure><p>Looking at the entire training loss record, it’s evident that while harnessing the cost-effective capabilities of the Colossal-AI system, the model’s convergence is also well-preserved. With a training dataset of only about <strong>8.5 billion tokens and computational costs in the range of hundreds of dollars</strong>, the model achieves such remarkable results. In contrast, many large-scale models available in the market require training with several trillion tokens to ensure effectiveness, incurring significantly higher costs.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*e3SKLmYgeDBlqvGp" /></figure><h3>Colossal-LLaMA-2 13B</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*yE_7zmF7_HNazEBp" /></figure><p>Note: Based on ColossalEval scores, the scores in parentheses are from the official leaderboard scores of the corresponding models, and C-Eval scores are from the official Leaderboard.</p><p>In English MMLU rankings, the Colossal-LLaMA-2–13B-base exhibits a steady improvement in English performance, attributed to low-cost incremental pre-training. Notably, in the GSM8k evaluation, there is a <strong>significant</strong> <strong>improvement</strong> <strong>in English mathematical and reasoning capabilities (31.31 -&gt; 58.83)</strong>, outperforming all other 13B models.</p><p>In Chinese rankings, we primarily compared CMMLU, AGIEVAL, GAOKAO, and C-Eval. The effectiveness of <strong>Colossal-LLaMA-2 surpasses other Chinese</strong> <strong>models based on LLaMA-2 by a wide margin</strong>. Even when compared to well-known Chinese corporation models that are <strong>pre-trained from scratch with potentially</strong> <strong>millions</strong> <strong>of dollars, </strong>Colossal-LLaMA-2’s <strong>performance</strong> <strong>is outstanding</strong>. Particularly noteworthy is the <strong>leap in Chinese proficiency</strong> compared to the original LLaMA-2 (<strong>CMMLU: 38.14 -&gt; 61.8</strong>).</p><p>After analyzing loss records throughout the entire training process, it is clear that through leveraging the cost reduction and enhanced efficiency of the Colossal-AI system <strong>the model’s</strong> <strong>convergence</strong> <strong>is ensured</strong>. Remarkably, achieving such impressive results requires approximately 25 billion tokens, coupled with computational costs of $5000 USD. This is in contrast to prevalent large-scale market models that demand training using several trillion tokens, incurring substantial and expensive computational expenses to achieve comparable effectiveness.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*8rm6hmyRw3CaH4Hu" /></figure><p>So, how did the Colossal-AI team manage to reduce training costs and achieve such impressive results?</p><h3>Vocabulary Expansion and Model Initialization</h3><p>LLaMA-2’s original vocabulary was not specifically optimized for Chinese and had a limited set of Chinese words, resulting in insufficient comprehension of Chinese language data. Consequently, the first step involved expanding the vocabulary of LLaMA-2.</p><p>The Colossal-AI team discovered that:</p><ol><li>Vocabulary expansion not only significantly improved the efficiency of encoding string sequences but also enriched the encoded sequences with more meaningful information. This, in turn, proved highly beneficial for document-level encoding and understanding.</li><li>However, due to the limited volume of continual pre-training data, an extensive expansion of the vocabulary could result in certain words or combinations lacking practical meaning, making it challenging to learn them effectively from the continual pre-training dataset and impacting the final performance.</li><li>An excessively large vocabulary would increase the number of embedding-related parameters, affecting training efficiency.</li></ol><p>Therefore, after conducting numerous experiments while considering both training quality and efficiency, the Colossal-AI team decided to expand the vocabulary from the original 32,000 words in LLaMA-2 to 69,104.</p><p>With the expanded vocabulary in place, the next step involved initializing the embeddings based on the original LLaMA-2 for the new vocabulary. To facilitate a seamless transition of capabilities from the original LLaMA-2 to the Chinese LLaMA-2 while ensuring that the English proficiency remains unaffected in the initial state, the Colossal-AI team employed mean initialization of the new embeddings using the weights from the original LLaMA-2. This approach not only preserved the English language capabilities but also facilitated the smooth transfer of these capabilities to the Chinese language model.</p><h3>Data Construction</h3><p>To further reduce the cost of training, high-quality data plays a key role, especially for continual pre-training which has strict requirements for the quality and distribution of data. To better filter high-quality data, the Colossal-AI team has constructed a complete data cleaning system and toolkit for selecting higher-quality data for continual pre-training.</p><p>The following image shows the complete data governance process of the Colossal-AI team:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3kq_MuT_IPoVBCD8" /></figure><p>In addition to conducting heuristic selection and deduplication of data, scoring and classification filtering were also applied to key data. Suitable data plays a crucial role in stimulating the Chinese capabilities of LLaMA-2, while simultaneously overcoming the catastrophic forgetting problem in English.</p><p>Finally, to improve training efficiency for data on the same topic, the Colossal-AI team sorted the data by length and concatenated it according to the maximum length of 4096.</p><p>In contrast to the 7B version, the 13B model’s training uses a more refined data architecture<strong>,</strong> categorizing data into knowledge-based, functional, and memory replay data<strong>.</strong></p><p>Knowledge-based data is subdivided into over a dozen major categories, including finance, law, education, etc., with each major category further divided into subcategories to enable precise control over different data. Additionally, the scale of data from various verticals was increased to ensure a strong grasp of the model on data from diverse domains.</p><p>To address the community’s demand for functional capabilities in large models, targeted enhancements were made for different natural language processing tasks. This ensures that the model meets a certain level of understanding and proficiency in common NLP tasks during pre-training regarding text summarization, information extraction, and comprehension of complex problem-solving chains.</p><p>Furthermore, memory replay data serves as a crucial component to achieve the model’s mastery of acquired knowledge, effectively enhancing the overall performance and generalization ability of the model.</p><p>To address the growing security concerns, the Colossal-AI team implemented multidimensional enhancements (political sensitivity, religious sensitivity, abusive language, hatred, bias, illegal activities, physical harm, mental health, property privacy, moral and ethical considerations, etc.) to ensure the foundational model possesses robust security and adheres to correct values.</p><h3>Training Strategy</h3><h3>Multi-stage Training</h3><p>In terms of training, given the characteristics of continual pre-training, the Colossal-AI team designed a multi-stage, hierarchical continual pre-training scheme, dividing the training process into three stages:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/0*1Ntho88djO-inDEF" /></figure><ol><li>Large-Scale Pre-training Stage: The goal at this stage is to enable the model to produce relatively smooth text through training with a large amount of corpus. This stage is completed by LLaMA-2. After this stage, the model has already mastered a large amount of English knowledge and can produce smooth results based on Next Token Prediction.</li><li>Chinese Knowledge Injection Stage: This stage relies on high-quality Chinese knowledge. On one hand, it enhances the model’s grasp of Chinese knowledge, and on the other hand, it improves the model’s understanding of the newly added words in the Chinese vocabulary.</li><li>Relevant Knowledge Replay Stage: This stage is dedicated to enhancing the model’s understanding and generalization ability of knowledge and alleviating the catastrophic forgetting problem.</li></ol><p>The multi-stage approach complements each other, ultimately ensuring that the model progresses equally in both Chinese and English abilities.</p><h3>Bucket Training</h3><p>Continual pre-training is extremely sensitive to data distribution, so balance is particularly important. To ensure a balanced distribution of data, the Colossal-AI team designed a data bucketing strategy, dividing the same type of data into 10 different bins. During the training process, each data bucket contains one bin of each type of data, thereby ensuring that each type of data can be utilized evenly by the model.</p><h3>Evaluation System</h3><p>To better assess the performance of the model, the Colossal-AI team has built a complete evaluation system — ColossalEval, which evaluates large language models from multiple dimensions. The framework and code of the process are fully open-source, supporting the reproduction of results and also allowing users to customize datasets and evaluation methods according to the application scenario. The characteristics of the evaluation framework are summarized as follows:</p><ol><li>It includes common datasets for evaluating the knowledge reserve capability of large language models, such as MMLU, CMMLU, etc. For formats like multiple-choice questions and comparing probabilities of ABCD, more comprehensive calculation methods are added, such as absolute matching, single-choice perplexity, etc. This aims to measure the model’s grasp of knowledge more thoroughly.</li><li>Support evaluation for multiple-choice questions and long-text assessments.</li><li>Support evaluation methods for different application scenarios including multi-turn dialogues, role-playing, information extraction, content generation, etc. Users can selectively assess different aspects of the model’s abilities based on their specific needs. Additionally, the system supports the extension of custom prompts and evaluation methods to cater to individual preferences and requirements.</li></ol><h3>Bridging from General Large Models to Domain-specific Large Models</h3><p>From the experience of the Colossal-AI team, constructing a Chinese version of LLaMA-2 can be summarized into the following process:</p><p>So, can this process be reused?</p><p>The answer is affirmative, and it holds great significance in real-world implementation scenarios.</p><p>As the wave of artificial intelligence-driven by ChatGPT surges, major internet giants, AI companies, startups, universities, research institutions, and others are all actively participating in the race for large general-purpose models. However, behind the generality of these large models often lies a lack of domain-specific knowledge. Consequently, the issue of practical applicability becomes particularly serious. While fine-tuning for specific applications can yield some benefits, the absence of domain-specific large models creates performance bottlenecks in application deployment.</p><p>If a domain-specific large model can be rapidly and cost-effectively constructed, followed by fine-tuning for specific business needs, it would undoubtedly advance the deployment of applications, providing a competitive advantage.</p><p>Applying the above process to perform knowledge transfer in <strong>any field allows for the cost-effective construction of lightweight domain-specific foundational large models</strong>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3QwwCCRuFuox72b5" /></figure><p>For constructing foundational large models from scratch, one can also draw inspiration from the aforementioned experiences and Colossal-AI’s cost-reduction and efficiency-enhancing capabilities to efficiently achieve this goal at minimal cost.</p><h3>System Optimization</h3><p>The impressive performance and cost advantages of Colossal-LLaMA-2 are built upon the foundation of the low-cost AI large model development system, Colossal-AI.</p><p>Colossal-AI, based on PyTorch, leverages efficient multi-dimensional parallelism, heterogeneous memory, and other techniques to reduce the development and deployment costs of AI large model training, fine-tuning, and inference. It enhances model task performance, reduces GPU requirements, and more. In just over a year, it has garnered over 30,000 GitHub stars within the open-source community. It ranks first in the world in the field of large model development tools and communities and has collaborated with numerous Fortune 500 companies and other well-known enterprises to develop/optimize models with hundreds of billions or tens of billions of parameters and create domain-specific models.</p><h3>Low Cost and High-quality Model Construction</h3><p>After the construction of multidimensional data and the enhancement of the foundational model’s natural language capabilities, the Colossal-AI team has developed a 7B version model and a more powerful 13B version model. Based on these models, community users can benefit from reduced amounts of high-quality fine-tuning data while fine-tuning, resulting in cheaper costs and the creation of a personalized model.</p><p>Colossal-LLaMA-2 Download: <a href="https://huggingface.co/hpcai-tech/Colossal-LLaMA-2-7b-base">https://huggingface.co/hpcai-tech/Colossal-LLaMA-2-7b-base</a></p><p><a href="https://huggingface.co/hpcai-tech/Colossal-LLaMA-2-13b-base">https://huggingface.co/hpcai-tech/Colossal-LLaMA-2-13b-base</a></p><p>Colossal-AI Open Source Address:<a href="https://github.com/hpcaitech/ColossalAI"> https://github.com/hpcaitech/ColossalAI</a></p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=26d2e4b9fd92" width="1" /><hr /><p><a href="https://medium.com/pytorch/colossal-llama-2-low-cost-and-high-quality-domain-specific-llm-solution-using-llama-and-26d2e4b9fd92">Colossal-LLaMA-2: Low Cost and High-quality Domain-specific LLM Solution Using LLaMA and…</a> was originally published in <a href="https://medium.com/pytorch">PyTorch</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/356a495a20c4</id>
            <title>3D rotations and spatial transformations made easy with RoMa</title>
            <link>https://medium.com/pytorch/3d-rotations-and-spatial-transformations-made-easy-with-roma-356a495a20c4?source=rss----512b8efdf2e7---4</link>
            <guid isPermaLink="false">https://medium.com/p/356a495a20c4</guid>
            <pubDate></pubDate>
            <updated>2024-01-25T00:02:54.604Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><em>Struggling with quaternions, rotation vectors, right-hand rules and all these stuffs? Try RoMa: an easy-to-to-use, stable and efficient library to deal with rotations and spatial transformations in PyTorch.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5wOBI5bLik76q_kucGza9g.png" /><figcaption>Overview of the main features of RoMa</figcaption></figure><p><strong>A bit of context</strong></p><p>Spatial transformations are essential in physics, engineering, and computer vision. Scientists have been studying 3D geometry for centuries and produced great mathematical tools to deal with such transformations, especially rigid motions and rotations. Yet, implementing these tools properly is not trivial.</p><p>Indeed, one often has to use different conventions or representations for different tasks. Quaternions are for example great to compose rotations; rotation matrices are ideal to transform coordinates of large number of points; rotation vectors are compact and useful to represent small rotations, and Euler angles are bad for pretty much everything except user inputs.</p><p>Additionally, turning math into code leads to numerical issues that can be hard to spot, and cause subtle bugs. Hopefully, these issues can be mitigated by picking the right algorithms or by handling special cases. For example, the angle between two rotation matrices can be computed based on different formulas that are mathematically equivalent, but produce drastically different numerical errors in 32bit floating point precision:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*y0zrQlaYddBLqmny9TpDcg.png" /><figcaption>Numerical errors when evaluating the geodesic distance between rotation matrices naively.</figcaption></figure><p><strong>Introducing RoMa</strong></p><p>RoMa aims to overcome these obstacles for you. RoMa is a Python library compatible with PyTorch version 1.6 and above. It provides an easy-to-use, stable and efficient toolbox to deal with rotations, as well as more general spatial transformations.</p><p><strong><em>Conversions between rotation representations</em></strong></p><p>RoMa provides differentiable routines to convert between various rotation representations. For example, one could sample a 3D rotation vector and convert it to a unit quaternion as follows using RoMa:</p><pre>import torch, roma<br />rotvec = torch.randn(3) # 3D rotation vector<br />q = roma.rotvec_to_unitquat(rotvec) # unit quaternion, represented by a 4D tensor</pre><p><strong><em>Batched data</em></strong></p><p>For convenience, functions in RoMa support arbitrary numbers of batch dimensions. One could for example sample a batch of 2x5 random 3D rotation vectors — represented by a 2x5x3 tensor — and convert it to a batch of unit quaternions — represented by a 2x5x4 tensor — using the same syntax as above:</p><pre>import torch, roma<br />rotvec = torch.randn(2,5,3)<br />q = roma.rotvec_to_unitquat(rotvec)</pre><p><strong><em>Regressing rotations</em></strong></p><p>Regressing rotations using a neural network is non trivial because classical neural architectures produce outputs lying in Euclidean space. RoMa implements various differentiable functions to map such output to the rotation space, e.g. by performing special Procrustes orthonormalization of an arbitrary matrix:</p><pre>import torch, roma<br />M = my_fancy_neural_network(some_input) # Method returning an arbitrary 3x3 matrix<br />R = roma.special_procrustes(M) # Orthonormalizing M into a rotation matrix<br />assert roma.is_rotation_matrix(R, epsilon=1e-5)</pre><p><strong><em>Rigid transformations</em></strong></p><p>For a more readable code, RoMa also includes utilities to deal with nonlinear spatial transformations, and notably rigid motions:</p><pre>import torch, roma<br /># Rigid transformation parameterized by a rotation matrix and a translation vector<br />T1 = roma.Rigid(linear=roma.random_rotmat(), translation=torch.randn(3))<br />T2 = roma.Rigid(linear=roma.random_rotmat(), translation=torch.randn(3))<br /><br /># Inverting and composing transformations<br />T = (T1.inverse() @ T2)<br /># Normalization to ensure that T is actually a rigid transformation.<br />T = T.normalize()<br /># Direct access to the translation part<br />T.translation += 0.5<br /># Transformation of points:<br />points = torch.randn(100,3)<br /># Adjusting the shape of T for proper broadcasting.<br />transformed_points = T[None].apply(points)<br /># Transformation of vectors:<br />vectors = torch.randn(10,20,3)<br /># Adjusting the shape of T for proper broadcasting.<br />transformed_vectors = T[None,None].linear_apply(vectors)<br /># Casting the transformation into an homogeneous 4x4 matrix.<br />M = T.to_homogeneous()</pre><p>RoMa can be installed easily using PIP, so give it a try:</p><pre>pip install roma</pre><p>and see the <a href="https://naver.github.io/roma/"><strong>documentation</strong></a> and <a href="https://github.com/naver/roma"><strong>Github</strong></a> repository for more details.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=356a495a20c4" width="1" /><hr /><p><a href="https://medium.com/pytorch/3d-rotations-and-spatial-transformations-made-easy-with-roma-356a495a20c4">3D rotations and spatial transformations made easy with RoMa</a> was originally published in <a href="https://medium.com/pytorch">PyTorch</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/9e0ecabf2815</id>
            <title>torchdistill — a modular, configuration-driven framework for reproducible deep learning and…</title>
            <link>https://medium.com/pytorch/torchdistill-a-modular-configuration-driven-framework-for-reproducible-deep-learning-and-9e0ecabf2815?source=rss----512b8efdf2e7---4</link>
            <guid isPermaLink="false">https://medium.com/p/9e0ecabf2815</guid>
            <pubDate></pubDate>
            <updated>2024-01-04T15:59:07.779Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <h3>torchdistill — a modular, configuration-driven framework for reproducible deep learning and knowledge distillation experiments</h3><h4>Author: <a href="https://yoshitomo-matsubara.net/">Yoshitomo Matsubara</a></h4><p>This article summarizes key features and concepts of <a href="https://github.com/yoshitomo-matsubara/torchdistill"><strong>torchdistill</strong></a><strong> (v1.0.0)</strong>. Refer to <a href="https://yoshitomo-matsubara.github.io/torchdistill/">the official documentation</a> for its APIs and research projects.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*pzDQGfqki1ij5MO7.png" /><figcaption>Logo</figcaption></figure><p><a href="https://github.com/yoshitomo-matsubara/torchdistill"><strong>torchdistill</strong></a> is a modular, configuration-driven machine learning open source software (ML OSS) for reproducible deep learning and knowledge distillation experiments. The ML OSS is available as <a href="https://pypi.org/project/torchdistill/">a PyPI package</a> (pip install torchdistill) and offers various state-of-the-art knowledge distillation methods and enables you to design (new) experiments simply by editing a declarative yaml config file instead of Python code. Even when you need to extract intermediate representations in teacher/student models, you will <strong>NOT</strong> need to reimplement the models, that often change the interface of the forward, but instead specify the module path(s) in the yaml file.</p><blockquote>torchdistill is modular, configuration-driven</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/620/1*x_KkN0HOtSimzSwTAkFIBg.png" /><figcaption>Pipeline with abstracted modules + a declarative PyYAML config file</figcaption></figure><p>In <strong>torchdistill</strong>, many components and PyTorch modules are abstracted e.g., models, datasets, optimizers, losses, and more! You can define them in a declarative PyYAML config file, which contains almost everything to reproduce the experimental result and can be seen as a summary of your experiment. In many cases, you will NOT need to write Python code at all. Take a look at some configurations available in <a href="https://github.com/yoshitomo-matsubara/torchdistill/tree/main/configs/">configs/</a>. You’ll see what modules are abstracted and how they are defined in a declarative PyYAML config file to design an experiment.</p><p>For example, you can instantiate <a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10">torchvision.datasets.CIFAR10</a> for its training and test datasets in two lines, using a PyYAML configuration file (test.yaml).</p><pre>from torchdistill.common import yaml_util<br />config = yaml_util.load_yaml_file('./test.yaml')<br />train_dataset = config['datasets']['cifar10/train']<br />test_dataset = config['datasets']['cifar10/test']</pre><p><strong>test.yaml</strong></p><pre>datasets:<br />  &amp;cifar10_train cifar10/train: !import_call<br />    _name: &amp;dataset_name 'cifar10'<br />    _root: &amp;root_dir !join ['~/datasets/', *dataset_name]<br />    key: 'torchvision.datasets.CIFAR10'<br />    init:<br />      kwargs:<br />        root: *root_dir<br />        train: True<br />        download: True<br />        transform: !import_call<br />          key: 'torchvision.transforms.Compose'<br />          init:<br />            kwargs:<br />              transforms:<br />                - !import_call<br />                  key: 'torchvision.transforms.RandomCrop'<br />                  init:<br />                    kwargs:<br />                      size: 32<br />                      padding: 4<br />                - !import_call<br />                  key: 'torchvision.transforms.RandomHorizontalFlip'<br />                  init:<br />                    kwargs:<br />                      p: 0.5<br />                - !import_call<br />                  key: 'torchvision.transforms.ToTensor'<br />                  init:<br />                - !import_call<br />                  key: 'torchvision.transforms.Normalize'<br />                  init:<br />                    kwargs: &amp;normalize_kwargs<br />                      mean: [0.49139968, 0.48215841, 0.44653091]<br />                      std: [0.24703223, 0.24348513, 0.26158784]<br />  &amp;cifar10_test cifar10/test: !import_call<br />    key: 'torchvision.datasets.CIFAR10'<br />    init:<br />      kwargs:<br />        root: *root_dir<br />        train: False<br />        download: True<br />        transform: !import_call<br />          key: 'torchvision.transforms.Compose'<br />          init:<br />            kwargs:<br />              transforms:<br />                - !import_call<br />                  key: 'torchvision.transforms.ToTensor'<br />                  init:<br />                - !import_call<br />                  key: 'torchvision.transforms.Normalize'<br />                  init:<br />                    kwargs: *normalize_kwargs</pre><p>When you take a close look at the above PyYAML, you will notice that !import_call, a constructor instantiated a class specified by key using kwargs under init (if kwargs does not exist, an empty dict will be used as kwargs). As you can see, the instantiation process is recursive. <br />I.e., config['datasets']['cifar10/train'] will be set by instantiating the following classes in order</p><ol><li><a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop">torchvision.transforms.RandomCrop</a> as part of a list transforms</li><li><a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomHorizontalFlip.html#torchvision.transforms.RandomHorizontalFlip">torchvision.transforms.RandomHorizontalFlip</a>as part of a list transforms</li><li><a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor">torchvision.transforms.ToTensor</a> as part of a list transforms</li><li><a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize">torchvision.transforms.Normalize</a> as part of a list transforms</li><li><a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose">torchvision.transforms.Compose</a> as transform</li><li><a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10">torchvision.datasets.CIFAR10</a> as cifar10/train</li></ol><p>Basically, you can use !import_call for any modules/functions in your locally installed torch and torchvision.<br />E.g., if you want to use CIFAR100 instead of CIFAR10, then you can just replace <a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10">torchvision.datasets.CIFAR10</a> with <a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR100.html#torchvision.datasets.CIFAR100">torchvision.datasets.CIFAR100</a>.</p><p>If you want to use !import_call for your own modules, refer to <a href="https://yoshitomo-matsubara.net/torchdistill/usage.html">the documentation</a>.</p><p>You can instantiate other types of modules (e.g., models) using !import_call , but it is not necessary to use the constructor for all the module types in a PyYAML file.</p><h3>Example 1: Reproducing results of KD methods for ImageNet (ILSVRC 2012)</h3><p>Using <strong>torchdistill</strong>, I reimplemented about 20 different KD methods. Some of the methods were tested in the papers for ResNet-34 and ResNet-18, a popular teacher-student pair for ImageNet (ILSVRC 2012).</p><p>I attempted to reproduce reported accuracy of ResNet-18 for ImageNet (ILSVRC 2012) dataset, using a popular teacher-student pair: ResNet-34 and ResNet-18 (except for Tf-KD, where ResNet-18 was used as a teacher). Hyperparameters are the same as those provided in the papers/code or by the authors.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/551/1*I0LgzOKJf6YfaE2mG2v4yA.png" /><figcaption><a href="https://yoshitomo-matsubara.net/torchdistill/benchmarks.html">https://yoshitomo-matsubara.net/torchdistill/benchmarks.html</a></figcaption></figure><p>The student model trained with all the reimplemented methods achieved better accuracy than the same model trained without the teacher model. However, most of the results (even the reported numbers in the original papers) did not outperform a standard KD method proposed in “<a href="https://arxiv.org/abs/1503.02531">Distilling the Knowledge in a Neural Network” (Hinton et al., 2014)</a>. See <a href="https://github.com/yoshitomo-matsubara/torchdistill?tab=readme-ov-file#citation">the first torchdistill paper</a> for details.</p><p>All the configurations, checkpoints, and script are available in <a href="https://github.com/yoshitomo-matsubara/torchdistill">the official code repository</a>.</p><h3>Example 2: Reproducing GLUE test results of BERT</h3><p>Similarly, I attempted to reproduce GLUE test results of fine-tuned BERT models in <a href="https://aclanthology.org/N19-1423/">“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding” (Devlin et al., 2019)</a>, using <strong>torchdistill</strong> with Hugging Face libraries such as <a href="https://github.com/huggingface/transformers">transformers</a>, <a href="https://github.com/huggingface/datasets">datasets</a>, <a href="https://github.com/huggingface/evaluate">evaluate</a>, and <a href="https://github.com/huggingface/accelerate">accelerate</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/914/1*E40vNiuxovAtlgtjemHNGg.png" /><figcaption><a href="https://aclanthology.org/2023.nlposs-1.18/">https://aclanthology.org/2023.nlposs-1.18/</a></figcaption></figure><p>The results are from <a href="https://gluebenchmark.com/">GLUE Benchmark</a> (not the dev results). The fine-tuned BERT-Base and BERT-Large models achieved test results comparable to the results reported by Devlin et al., 2019.</p><p>Besides the standard fine-tuning (FT) experiments, I conducted knowledge distillation (KD) experiments for fine-tuning BERT-Base, using fine-tuned BERT-Large as teacher models. The KD method (Hinton et al., 2014) helped BERT-Base models improve the performance for most of the tasks, compared to those fine-tuned without the teacher models.</p><p>Those experiments were done on <a href="https://github.com/yoshitomo-matsubara/torchdistill?tab=readme-ov-file#google-colab-examples">Google Colab</a>. All the configurations and script are available in <a href="https://github.com/yoshitomo-matsubara/torchdistill">the official code repository</a>. The model checkpoints and training log files are available at <a href="https://huggingface.co/yoshitomo-matsubara">the Hugging Face Model repositories</a>.</p><h3>Final words</h3><p>This article briefly introduced <strong>torchdistill</strong> and only a few of its features. As <a href="https://pypi.org/project/torchdistill/">a PyPI package</a>, <strong>torchdistill</strong> also offers popular small models, forward hook manager, dataset/model wrappers and loss modules for reimplemented KD methods, and more.</p><p><a href="https://github.com/yoshitomo-matsubara/torchdistill">The GitHub repository</a> also provides example scripts, Google Colab examples, demo e.g., extracting intermediate layers’ input/output (embeddings) without any modifications in model implementations. To learn more about <strong>torchdistill</strong>, see the repository and <a href="https://yoshitomo-matsubara.net/torchdistill/">documentation</a>.</p><p>If you have either a question or feature request, use <a href="https://github.com/yoshitomo-matsubara/torchdistill/discussions">GitHub Discussions</a>. Please search through <a href="https://github.com/yoshitomo-matsubara/torchdistill/issues">GitHub Issues</a> and <a href="https://github.com/yoshitomo-matsubara/torchdistill/discussions">Discussions</a> and make sure your issue/question/request has not been addressed yet.</p><h3>Publications</h3><ul><li>Yoshitomo Matsubara: <a href="https://link.springer.com/chapter/10.1007/978-3-030-76423-4_3">“torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation”</a> @ ICPR 2020 Workshop on Reproducible Research in Pattern Recognition (RRPR)</li><li>Yoshitomo Matsubara: <a href="https://aclanthology.org/2023.nlposs-1.18/">“torchdistill Meets Hugging Face Libraries for Reproducible, Coding-Free Deep Learning Studies: A Case Study on NLP”</a> @ EMNLP 2023 Workshop for Natural Language Processing Open Source Software (NLP-OSS)</li></ul><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9e0ecabf2815" width="1" /><hr /><p><a href="https://medium.com/pytorch/torchdistill-a-modular-configuration-driven-framework-for-reproducible-deep-learning-and-9e0ecabf2815">torchdistill — a modular, configuration-driven framework for reproducible deep learning and…</a> was originally published in <a href="https://medium.com/pytorch">PyTorch</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/861bc0bb92f1</id>
            <title>PyPose: A Library for Robot Learning with Physics-based Optimization</title>
            <link>https://medium.com/pytorch/pypose-a-library-for-robot-learning-with-physics-based-optimization-861bc0bb92f1?source=rss----512b8efdf2e7---4</link>
            <guid isPermaLink="false">https://medium.com/p/861bc0bb92f1</guid>
            <pubDate></pubDate>
            <updated>2023-12-06T16:47:27.916Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <h3>PyPose is now part of the PyTorch Ecosystem!</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2cxH0GmgBa4Fo6ng2LVA1g.jpeg" /><figcaption>PyPose</figcaption></figure><p>We are excited to share our new open-source library <strong>PyPose</strong>. It is a PyTorch-based robotics-oriented library that provides a set of tools and algorithms for connecting deep learning with physics-based optimization.</p><p>PyPose was developed with the aim of making it easier for researchers and developers to build and deploy robotics applications. With PyPose, you can easily create and test various control, planning, SLAM, or other optimization-based algorithms and connect them with deep learning-based perception algorithms.</p><h3>Motivation</h3><p>Deep learning has had remarkable success in robotic perception, but its data-centric nature suffers when it comes to generalizing to ever-changing environments. By contrast, physics-based optimization generalizes better, but it does not perform as well in complicated tasks due to the lack of high-level semantic information and reliance on manual parametric tuning. Taking the best of both worlds, PyPose is created to address deep perceptual models with physics-based optimization.</p><h3>PyPose for researchers and developers</h3><p>PyPose’s architecture is clean and well-organized with an imperative style interface following PyTorch, and is efficient and user-friendly, making it easy to integrate into real-world robotic applications. PyPose supports parallel computing of any order gradients of Lie groups and Lie algebra and 2nd-order optimizers. PyPose achieved more than <em>10x </em>speedup in computation compared to state-of-the-art libraries. PyPose functionalities include but are not limited to SLAM, planning, inertial navigation, and control.</p><p>Some of the features of PyPose include:</p><ul><li>Differentiable Lie group and Lie algebra such as <a href="https://sairlab.org/pypose/"><strong>SO3</strong></a>, <a href="https://pypose.org/docs/main/generated/pypose.SE3/">SE3</a>, <a href="https://pypose.org/docs/main/generated/pypose.so3/">so3</a>, and <a href="https://pypose.org/docs/main/generated/pypose.se3/">se3</a>.</li><li>2nd-order optimizers such as <a href="https://pypose.org/docs/main/generated/pypose.optim.GaussNewton/">GaussNewton</a> and <a href="https://pypose.org/docs/main/generated/pypose.optim.LevenbergMarquardt/">LevenbergMarquardt</a>.</li><li>Many other useful differential modules include various differentiable filters, such as <a href="https://pypose.org/docs/main/generated/pypose.module.System/">dynamics models</a>, <a href="https://pypose.org/docs/main/generated/pypose.module.LQR/">linear quadratic regulator</a>, <a href="https://pypose.org/docs/main/generated/pypose.module.EKF/">EKF</a>, <a href="https://pypose.org/docs/main/generated/pypose.module.UKF/">UKF</a>, <a href="https://pypose.org/docs/main/generated/pypose.module.IMUPreintegrator/">IMU pre-integration</a>, and more. This <a href="https://arxiv.org/abs/2309.13035">article</a> shows simple usage of them.</li></ul><h3>Examples</h3><p><strong>Usage example</strong></p><p>The following code sample shows how to rotate random points and compute the gradient of batched rotation.</p><pre>&gt;&gt;&gt; import torch, pypose as pp</pre><pre>&gt;&gt;&gt; # A random so(3) LieTensor<br />&gt;&gt;&gt; r = pp.randn_so3(2, requires_grad=True)<br />    so3Type LieTensor:<br />    tensor([[ 0.1606,  0.0232, -1.5516],<br />            [-0.0807, -0.7184, -0.1102]], requires_grad=True)</pre><pre>&gt;&gt;&gt; R = r.Exp() # Equivalent to: R = pp.Exp(r)<br />    SO3Type LieTensor:<br />    tensor([[ 0.0724,  0.0104, -0.6995,  0.7109],<br />            [-0.0395, -0.3513, -0.0539,  0.9339]], grad_fn=&lt;AliasBackward0&gt;)</pre><pre>&gt;&gt;&gt; p = R @ torch.randn(3) # Rotate random point<br />    tensor([[ 0.8045, -0.8555,  0.5260],<br />            [ 0.3502,  0.8337,  0.9154]], grad_fn=&lt;ViewBackward0&gt;)</pre><pre>&gt;&gt;&gt; p.sum().backward()     # Compute gradient<br />&gt;&gt;&gt; r.grad                 # Print gradient<br />    tensor([[-0.7920, -0.9510,  1.7110],<br />            [-0.2659,  0.5709, -0.3855]])</pre><p>For more usage, see <a href="https://pypose.org/docs">Documentation</a>. For more applications, see <a href="https://github.com/pypose/pypose/tree/main/examples">Examples</a>.</p><h3>Conclusion</h3><p>We believe PyPose revolutionizes robot learning by seamlessly connecting classic robotics with modern learning methods. Development of PyPose is continuous. We are adding more functionalities to our library to address various tasks from several fields of robot learning.</p><p>If you are interested in learning and using PyPose, then get started with our <a href="https://pypose.org/tutorials/">tutorials</a>.</p><p>We welcome developers from around the world. If you are interested in contributing to PyPose, then get started with our <a href="https://github.com/pypose/pypose/blob/main/CONTRIBUTING.md">GitHub</a> contribution.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=861bc0bb92f1" width="1" /><hr /><p><a href="https://medium.com/pytorch/pypose-a-library-for-robot-learning-with-physics-based-optimization-861bc0bb92f1">PyPose: A Library for Robot Learning with Physics-based Optimization</a> was originally published in <a href="https://medium.com/pytorch">PyTorch</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/7a93ae01ff2d</id>
            <title>How Activation Checkpointing enables scaling up training deep learning models</title>
            <link>https://medium.com/pytorch/how-activation-checkpointing-enables-scaling-up-training-deep-learning-models-7a93ae01ff2d?source=rss----512b8efdf2e7---4</link>
            <guid isPermaLink="false">https://medium.com/p/7a93ae01ff2d</guid>
            <pubDate></pubDate>
            <updated>2023-11-08T20:45:33.191Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <blockquote><em>By Yiftach Beer, Omri Bar</em></blockquote><h3><strong>Overview</strong></h3><p>Activation checkpointing is a technique used for reducing the memory footprint at the cost of more compute. It utilizes the simple observation that we can avoid saving intermediate tensors necessary for backward computation if we just recompute them on demand instead.</p><p>Currently there are two implementations of activation checkpointing available in PyTorch, reentrant and non-reentrant. The non-reentrant version was implemented later to address some of the limitations of reentrant checkpoint which are detailed in <a href="https://pytorch.org/docs/2.0/checkpoint.html">PyTorch’s official docs</a>. You can pass the use_reentrant flag to specify which version of checkpoint to use. Currently, the use_reentrant flag is optional and the reentrant version is the default. In 2.1 however, not explicitly passing the flag will be deprecated. In a future version of PyTorch, non-reentrant will become the default.</p><p>In this post, we first give some background about how PyTorch’s automatic differentiation works in general. Then we explore the new non-reentrant implementation of activation checkpointing and compare it with the earlier reentrant implementation. The implementations presented will be simplified for clarity.</p><h3><strong>Autograd in PyTorch</strong></h3><p>Before we dive in, let us briefly review some concepts needed for later.</p><p>The basic building block of PyTorch for storing and manipulating data is the tensor. By default, a tensor is not too different from a numpy array with GPU support. When a tensor has its .requires_grad attribute set to True, the <a href="https://pytorch.org/docs/stable/autograd.html">autograd</a> engine kicks in.</p><p>Every transformation applied to the tensor then creates — along with the resulting tensor — a special object that knows how to compute the transformation’s backward pass for backpropagation. This object can be accessed through the result tensor’s .grad_fn attribute.</p><p>The same object is also connected to other similar objects, all serving as nodes in the Directed Acyclic Graph (<a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">DAG</a>) called the computational graph. When a new node is created, autograd adds it to the graph by making its .next_functions attribute point to the existing nodes from which it was created.</p><p>Let’s focus on a concrete example. In the following code snippet:</p><pre>a = torch.tensor([2.], requires_grad=True)<br />b = torch.tensor([3.], requires_grad=True)<br />c = a + b<br />d = c.sin()</pre><p>c and d have nodes corresponding to the backward pass of the add and sine functions, respectively.</p><p>a and b, which are tensors that were created directly and not as a part of an operation, are called leaf tensors.</p><p>Such nodes have, instead of the regular node, an AccumulateGrad node which has a .variable attribute pointing to their tensor.</p><p><em>A helpful way to think about these is as two tiers — one for tensors and one for backward functions making the computational graph</em>. One tier (bottom in the figure) is made of tensors which are not connected to each other, but may be connected through a .grad_fn attribute to a backward function; and the other tier (top in the figure) is for the backward functions, which are unaware of the tensors — except for the AccumulateGrad special function — and are connected using a .next_functions attribute.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*FvO6wMs-7skyVoWW" /></figure><p>Each operation performed on a tensor with requires_grad creates — along with the resulting tensor — a new node in the computational graph. This behavior can be disabled inside a torch.no_grad() context manager and re-enabled in an inner torch.enable_grad() context manager.</p><p>However, not all functions of the tensor class create nodes in the computational graph — for example, torch.detach() copies the tensor without its .grad_fn, as a new tensor disconnected from any computational graph.</p><p>Higher level modules in the torch.nn package, such as Linear and MultiheadAttention, do not themselves take part in the computational graph. Instead, when they are called, they simply add the lower-level nodes they are composed of.</p><p>For example, consider a Linear block, which is made of multiple operations including matrix multiply with a <strong>weight </strong>tensor and addition with its <strong>bias </strong>tensor:</p><pre>x = torch.tensor([2.])<br />fc = nn.Linear(1, 1)<br />y = fc(x)</pre><p>Grouping the temporary intermediate tensors for simplification, this is how it looks under the hood:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*1YUmx_UsSelreMyf" /></figure><p>Once the computational graph has been constructed, a call to tensor.backward() — in turn calling torch.autograd.backward() — will recursively compute the gradients up to the leaf nodes where .grad is stored. When this process halts, the role of the computational graph ends and it is discarded (unless retain_graph=True is specified).</p><h3><strong>The new hook-based non-reentrant variant</strong></h3><p>The non-reentrant variant of activation checkpointing makes use of autograd’s <a href="https://pytorch.org/tutorials/intermediate/autograd_saved_tensors_hooks_tutorial.html">saved variable hooks mechanism</a>.</p><p>Here’s a simple example for how hooks are used:</p><pre>storage = []<br /><br />def pack(x):<br />    storage.append(x)<br />    return len(storage) - 1<br /><br />def unpack(x):<br />    return storage[x]<br /><br />x = torch.randn(1024, requires_grad=True)<br />with torch.autograd.graph.saved_tensors_hooks(pack, unpack):<br />    y = torch.square(x)<br />y.sum().backward()</pre><p>Behind the scenes, the square operator saves its input for the backward calculation (as the derivative of f(<strong>x</strong>)=x² is 2*<strong>x</strong>). Here, instead of saving the “large” tensor, we only store its (lightweight) index in the graph, and use that index to reconstruct it later. Though in this toy example the actual tensor is also stored (thus not saving any space) this will be a basis for the actual non-reentrant version of activation checkpointing.</p><p>With this understanding, let us explore the non-reentrant implementation of activation checkpointing based on saved tensor hooks. It has been simplified for clarity, but the overall structure remains:</p><pre>class Frame:  # a struct for shared variables<br />    def __init__(self):<br />        self.recomputed = []<br />        self.count = 0<br /><br /><br />class RecomputationHook(torch.autograd.graph.saved_tensors_hooks):<br />    def __init__(self, frame):<br />        def pack(x):<br />            frame.recomputed.append(x.detach())<br />            return x.detach()<br /><br />        def unpack(X):  # is only relevant for more complex scenarios<br />            return x<br /><br />        super().__init__(pack, unpack)<br /><br /><br />class CheckpointHook(torch.autograd.graph.saved_tensors_hooks):<br />    def __init__(self, frame, run_function, args):<br />        def pack(unused_x): <br />            i = frame.count<br />            frame.count += 1<br />            return i<br /><br />        def unpack(i):<br />            if not frame.recomputed:  # only once, while unpacking the first tensor during backward<br />                with RecomputationHook(frame), torch.autograd.enable_grad():<br />                    run_function(*args)<br />            res = frame.recomputed[i]<br />            frame.recomputed[i] = None<br />            return res<br /><br />        super().__init__(pack, unpack)<br /><br />def checkpoint_without_reentrant(run_function, *args):<br />    with CheckpointHook(Frame(), run_function, args):<br />        res = run_function(*args)<br />    return res</pre><p>When the non-reentrant activation checkpoint is called, the function’s forward pass is run in a CheckpointHook context manager. Under this context manager, any tensor packed and saved for the backward pass is discarded and replaced with a placeholder (here we arbitrarily use its index i).</p><p>During the backward pass, the first backward function that tries to access and unpack its saved tensors, triggers the forward() function to be recomputed under a RecomputationHook, which intercepts any tensors saved to store them in the recomputed list (detached from the computational graph to avoid reference cycles). It is important to note that the whole mechanism relies on the recomputed tensors being accessed in the same order in both forward and backward. To make sure that is the case, the real implementation also contains the code to save/restore the global state (e.g. preserving <a href="https://pytorch.org/docs/stable/random.html">RNG states</a>, which is important to ensure that modules such as Dropout produce the same output in both calls to run_function).</p><p>Much of the code not shown here deals with handling more complex cases — in the actual code, each of the variables has its tensor saved <em>once per graph</em>, and there’s also an early stopping mechanism to minimize unnecessary computations. The overall structure, however, is the same.</p><h3><strong>What’s New</strong></h3><p>Here are some of the the scenarios that the new version of non-reentrant activation checkpointing in the 2.1 release will support:</p><ol><li>Nested checkpointing — calling <em>another checkpointed function</em> from within a checkpointed function. This feature would allow the user to make an even more extreme trade off between memory and compute, potentially reducing theoretical minimum even further to O(log(n)) (from O(sqrt(n)) in the non-nested case):</li></ol><pre>def inner1(x):<br />  ...<br /><br />def inner2(x)<br />  ...<br /><br />def outer(x)<br />  y = checkpoint(inner1, x)<br />  z = checkpoint(inner2, y)<br />  return z<br /><br />a = torch.ones(1, requires_grad=True)<br />out = checkpoint(outer, a)<br />out.backward()</pre><p>2. Support for calling .grad()/.backward() within checkpointed functions — this is useful for higher-order gradient computation.</p><p>3. Improved checks for non-determinism and improved debuggability. Recall that an important assumption non-reentrant checkpoint makes is that the original and recomputed forward calls must save tensors for the backward in the same exact order. Beginning in 2.1, basic tensor metadata will be stored and checked to help validate that this is the case. Furthermore, if any checks for non-determinism fail, users can run checkpoint with debug=True which can provide traces of the ops executed during the original and recomputed runs as well as stack traces at the point in time those ops were called in order to help the user pinpoint where the non-determinism occurred.</p><p>4. Improved memory savings when retain_graph is specified</p><p>For more information, please consult <a href="https://pytorch.org/docs/2.1/checkpoint.html#torch.utils.checkpoint.checkpoint">the docs</a>. There’s also <a href="https://github.com/pytorch/pytorch/blob/670c5cf96249db28cde757da5a6aa97569760102/torch/utils/checkpoint.py#L365">a comprehensive comment</a> inside the code that details the various scenarios handled, and consulting it might be necessary to get a complete understanding. <a href="https://docs.google.com/document/d/1UDLhTNv6_kvuDTRlsjfj9WdqtNaQNr8ahrvdBIB6914/edit">This design doc</a> also contains some information about why choices were made and what new scenarios are supported.</p><h3><strong>The reentrant variant</strong></h3><p>An earlier implementation, called the reentrant variant, does not utilize saved variable hooks but instead uses a custom autograd Function, modifying the computational graph.</p><p>As the <a href="https://github.com/pytorch/pytorch/blob/670c5cf96249db28cde757da5a6aa97569760102/torch/utils/checkpoint.py#L75">official implementation</a> contains many details, we focus on a simplified one instead:</p><pre>class Checkpoint(torch.autograd.Function):<br /><br /> @staticmethod<br /> def forward(ctx, run_function, input):<br />  ctx.run_function = run_function<br />  ctx.save_for_backward(input)<br /><br />  with torch.no_grad():<br /> output = run_function(input)<br /><br />return output<br /><br /> @staticmethod<br /> def backward(ctx, output_grad):<br />run_function = ctx.run_function<br />input = ctx.saved_tensors<br /><br />detached_input = input.detach()<br />detached_input.requires_grad_(input.requires_grad)<br /><br />with torch.enable_grad():<br /> output = run_function(detached_input)<br /><br />torch.autograd.backward(output, output_grad)<br /><br />return None, input.grad</pre><p>In the forward pass, we calculate the output of the given run_function module on the given input. Since the call is inside a <em>with </em>torch.no_grad() context manager, no intermediate nodes and no backward nodes are created — the output of the operation is directly connected through a CheckpointBackward to the backward function of the inputs.</p><p>In the backward pass, we first load the objects passed to forward. Then, we detach the input and re-run the forward computation, this time in a torch.enable_grad() context manager to allow building a computational graph up to the output, which we backpropagate through to update the parameters that live inside this block. Note how we have to return two values, one for run_function and one for input, but the former is None as a module is not a differentiable tensor.</p><p>The dynamics are illustrated below:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*dAVBJTR3z0kl0Ni1" /></figure><p>What is noteworthy is how the <em>gradient calculations are not part of the main computational graph anymore — each time, a “mini computational graph” is constructed in which the actual gradient computations happen, whereas the original graph just coordinates and forwards gradients</em><strong>.</strong></p><p>The inner graph is not completely independent — we do need to pass grad_output to it, and return the input.grad it computed to the rest of the outer graph. But all the parameters of the current block, which hide behind run_function, get their .grad attribute populated inside that inner backward() call.</p><p>To make things simple, we left out some details such as saving/restoring global state for the second forward pass, and allowing a varying number of arguments and outputs, not all of which are tensors.</p><p>This implementation is called the reentrant variant as it uses a nested backward pass, called “reentrant backward” in PyTorch terminology. While using a nested backward pass might seem simple, in practice this implementation has limitations, e.g. it does not work well with DDP and FSDP in some cases.</p><h3><strong>Usage</strong></h3><p>Luckily, the complexities of both design are all wrapped in a simple-to-use API — the new implementation to use is specified using a use_reentrant flag, where using False (i.e. the new implementation) will become the default in a future version:</p><pre>from torch.utils.checkpoint import checkpoint<br /><br />checkpoint(run_function, args, use_reentrant=False)</pre><p>You may refer to <a href="https://pytorch.org/docs/2.1/checkpoint.html#torch.utils.checkpoint.checkpoint">the docs</a> for additional arguments and options.</p><p>We hope that after understanding these details, you will be able to use activation checkpointing more effectively, customize it to your needs and contribute to the PyTorch repository.</p><h3><strong>Acknowledgements</strong></h3><p>We would like to thank Geeta Chauhan from Meta AI/ML and the PyTorch team for their assistance in preparing this post.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7a93ae01ff2d" width="1" /><hr /><p><a href="https://medium.com/pytorch/how-activation-checkpointing-enables-scaling-up-training-deep-learning-models-7a93ae01ff2d">How Activation Checkpointing enables scaling up training deep learning models</a> was originally published in <a href="https://medium.com/pytorch">PyTorch</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/ae0def293084</id>
            <title>torch.compile, explained</title>
            <link>https://medium.com/pytorch/torch-compile-explained-ae0def293084?source=rss----512b8efdf2e7---4</link>
            <guid isPermaLink="false">https://medium.com/p/ae0def293084</guid>
            <pubDate></pubDate>
            <updated>2023-10-26T15:01:41.733Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Have you ever felt overwhelmed by the complexities of torch.compile? Diving into its workings can feel like black magic, with bytecode and Python internal details that many users fail to understand, hindering them from understanding and debugging torch.compile.</p><p>I am excited to introduce depyf, a new tool to pull out all the artifacts of torch.compile, and to decompile all the bytecode into source code so that every user understands it.</p><p>Note: please install the package by pip install depyf before going on to the example below.</p><h3>Example usage</h3><pre>import torch<br />+ @torch.compile(backend=&quot;eager&quot;)<br />- @torch.compile<br />def toy_example(a, b):<br />    x = a / (torch.abs(a) + 1)<br />    if b.sum() &lt; 0:<br />        b = b * -1<br />    return x * b<br /><br />for _ in range(100):<br />    toy_example(torch.randn(10), torch.randn(10))<br /><br />+ from depyf.explain import dump_src<br />+ src = dump_src(toy_example)<br />+ with open(&quot;explained_code.py&quot;, &quot;w&quot;) as f:<br />+     f.write(src)</pre><p>It’s this simple: switch the backend to “eager”, and run the dump_src function to pull out all the artifacts from torch.compile.</p><p>Note: (+)means to add new lines to your code, (-)means to remove old lines from your code.</p><p>In the dumped explained_code.py file, you can see something like below:</p><pre>def guard_2(L):<br />    return (___guarded_code.valid) \<br />        and (___check_global_state()) \<br />        and (hasattr(L['b'], '_dynamo_dynamic_indices') == False) \<br />        and (hasattr(L['x'], '_dynamo_dynamic_indices') == False) \<br />        and (utils_device.CURRENT_DEVICE == None) \<br />        and (___skip_backend_check() or ___current_backend() == ___lookup_backend(5096739488)) \<br />        and (___check_tensors(L['b'], L['x'], tensor_check_names=tensor_check_names))<br /><br />def __compiled_fn_4(L_b_ : torch.Tensor, L_x_ : torch.Tensor):<br />      l_b_ = L_b_<br />      l_x_ = L_x_<br />      mul = l_x_ * l_b_;  l_x_ = l_b_ = None<br />      return (mul,)<br /><br /><br />def compiled_code_2(b, x):<br />      return __compiled_fn_4(b, x)[0]<br /><br /><br />def __resume_at_38_2(b, x):<br />    # Note: if there is a compiled version below, this function might well not be executed directly. Please check the compiled version if possible.<br />    return x * b<br /><br />def compiled___resume_at_38_2(b, x):<br />    L = {&quot;b&quot;: b, &quot;x&quot;: x}<br />    if guard_2(L):<br />        return compiled_code_2(b, x)<br />    # Note: this function might well not be executed directly. It might well be compiled again, i.e. adding one more guards and compiled code.<br />    return __resume_at_38_2(b, x)<br /><br />#============ end of __resume_at_38_2 ============#<br /><br />def guard_1(L):<br />    return (___guarded_code.valid) \<br />        and (___check_global_state()) \<br />        and (hasattr(L['b'], '_dynamo_dynamic_indices') == False) \<br />        and (hasattr(L['x'], '_dynamo_dynamic_indices') == False) \<br />        and (utils_device.CURRENT_DEVICE == None) \<br />        and (___skip_backend_check() or ___current_backend() == ___lookup_backend(5096739488)) \<br />        and (___check_tensors(L['b'], L['x'], tensor_check_names=tensor_check_names))<br /><br />def __compiled_fn_3(L_b_ : torch.Tensor, L_x_ : torch.Tensor):<br />      l_b_ = L_b_<br />      l_x_ = L_x_<br />      b = l_b_ * -1;  l_b_ = None<br />      mul_1 = l_x_ * b;  l_x_ = b = None<br />      return (mul_1,)<br /><br /><br />def compiled_code_1(b, x):<br />      return __compiled_fn_3(b, x)[0]<br /><br /><br />def __resume_at_30_1(b, x):<br />    # Note: if there is a compiled version below, this function might well not be executed directly. Please check the compiled version if possible.<br />    b = b * -1<br />    return x * b<br /><br />def compiled___resume_at_30_1(b, x):<br />    L = {&quot;b&quot;: b, &quot;x&quot;: x}<br />    if guard_1(L):<br />        return compiled_code_1(b, x)<br />    # Note: this function might well not be executed directly. It might well be compiled again, i.e. adding one more guards and compiled code.<br />    return __resume_at_30_1(b, x)<br /><br />#============ end of __resume_at_30_1 ============#<br /><br />def guard_0(L):<br />    return (___guarded_code.valid) \<br />        and (___check_global_state()) \<br />        and (hasattr(L['a'], '_dynamo_dynamic_indices') == False) \<br />        and (hasattr(L['b'], '_dynamo_dynamic_indices') == False) \<br />        and (utils_device.CURRENT_DEVICE == None) \<br />        and (___skip_backend_check() or ___current_backend() == ___lookup_backend(5096739488)) \<br />        and (___check_tensors(L['a'], L['b'], tensor_check_names=tensor_check_names))<br /><br />def __compiled_fn_0(L_a_ : torch.Tensor, L_b_ : torch.Tensor):<br />      l_a_ = L_a_<br />      l_b_ = L_b_<br />      abs_1 = torch.abs(l_a_)<br />      add = abs_1 + 1;  abs_1 = None<br />      x = l_a_ / add;  l_a_ = add = None<br />      sum_1 = l_b_.sum();  l_b_ = None<br />      lt = sum_1 &lt; 0;  sum_1 = None<br />      return (x, lt)<br /><br /><br />def compiled_code_0(a, b):<br />      __temp_29 = __compiled_fn_0(a, b)<br />      x = __temp_29[0]<br />      if __temp_29[1]:<br />          return __resume_at_30_1(b, x)<br />      return __resume_at_38_2(b, x)<br /><br /><br />def toy_example(a, b):<br />    # Note: if there is a compiled version below, this function might well not be executed directly. Please check the compiled version if possible.<br />    x = a / (torch.abs(a) + 1)<br />    if b.sum() &lt; 0:<br />        b = b * -1<br />    return x * b<br /><br />def compiled_toy_example(a, b):<br />    L = {&quot;a&quot;: a, &quot;b&quot;: b}<br />    if guard_0(L):<br />        return compiled_code_0(a, b)<br />    # Note: this function might well not be executed directly. It might well be compiled again, i.e. adding one more guards and compiled code.<br />    return toy_example(a, b)<br /><br />#============ end of toy_example ============#</pre><p>You can explore the code with your favorite IDE. Start from the toy_example function, and pay attention to the compiled_toy_example function below, walk through all the details of guards/compiled code/compiled subgraph/resume functions. It’s all in readable source code format!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*eCjnMkRlDXTIPDZkK1dX6Q.jpeg" /></figure><p>Hopefully, by using this package, everyone can understand torch.compile now! The mental model is shown in the above flowchart.</p><p>For more advanced usage, please refer to the github repository <a href="https://github.com/thuml/depyf">depyf</a>.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ae0def293084" width="1" /><hr /><p><a href="https://medium.com/pytorch/torch-compile-explained-ae0def293084">torch.compile, explained</a> was originally published in <a href="https://medium.com/pytorch">PyTorch</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/563db4a69322</id>
            <title>How PyTorch enables medical breakthroughs with federated learning at Owkin</title>
            <link>https://medium.com/pytorch/how-pytorch-enables-medical-breakthroughs-with-federated-learning-at-owkin-563db4a69322?source=rss----512b8efdf2e7---4</link>
            <guid isPermaLink="false">https://medium.com/p/563db4a69322</guid>
            <pubDate></pubDate>
            <updated>2023-08-01T16:54:24.013Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><em>An overview of AI BioTech company Owkin and how their FL framework Substra is used with PyTorch to enable drug discovery.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9Fwn3XP_YJy-0wEe" /></figure><p>Owkin uses AI to find the right treatment for every patient. Their aim is to integrate the best of human and artificial intelligence to deliver better drugs and diagnostics at scale.</p><p>Owkin leverages PyTorch in combination with other technologies to build multimodal models that help researchers better understand complex biology through AI and discover new medical treatments.</p><p>One of the many innovations at Owkin is the use of <a href="https://owkin.com/what-is-federated-learning">federated learning</a> to train more robust and representative models that facilitate scientific breakthroughs in a privacy- enhancing way. We’ll take a look below at some of their medical research powered by the open source federated learning software <a href="https://owkin.com/substra">Substra</a>, which was recently added to the PyTorch ecosystem.</p><h3>How PyTorch helps Owkin in the fight against cancer</h3><p>Owkin uses PyTorch to build models and pipelines for various medical research outcomes, such as accelerating clinical development, identifying new biomarkers, or building tools to help doctors diagnose patients more effectively. Owkin leverages PyTorch for most of their research projects; citing the flexibility and hackability of the framework as the main reasons. The design philosophy of PyTorch is even embodied in some of their own machine learning pipelines.</p><p><strong>Case Study: PACpAInt</strong></p><p><a href="https://www.nature.com/articles/s41467-023-39026-y">PACpAInt</a> is a multi-step deep learning model recently co-developed by Owkin and Assistance Publique-Hopitaux de Paris (AP-HP) that decodes the complexity of pancreatic cancer, potentially revolutionizing the diagnosis and treatment of a disease with the<a href="https://www.pancreaticcancer.org.uk/what-we-do/media-centre/pancreatic-cancer-statistics/"> lowest survival rate</a> of all common cancers. It predicts tumor subtypes on surgical and biopsies specimens and independently predicts survival of patients. Without PACpAInt, molecular subtyping requires costly, lengthy, and complex RNA sequencing — which few patients can access. This level of analysis typically requires that the same data be manually analyzed by highly trained pathologists — which hospitals worldwide currently <a href="https://thepathologist.com/outside-the-lab/constant-demand-patchy-supply">face a shortage of</a>. AI tools like PACpAInt greatly increase the speed and accuracy of patient diagnosis, enabling doctors to tailor treatments more efficiently to the individual person.</p><a href="https://medium.com/media/9147ac23dd1340fdc96588f406bfcfd3/href">https://medium.com/media/9147ac23dd1340fdc96588f406bfcfd3/href</a><p>Similar models built by Owkin such as <a href="https://www.nature.com/articles/s41467-020-17678-4">HE2RNA</a> help predict treatment responses and survival outcomes of patients based on H&amp;E slides (medical image data). The code for this model has also been open sourced and is available <a href="https://github.com/owkin/HE2RNA_code">here</a>.</p><p>Owkin then uses federated learning, a <a href="https://medium.com/@aliimran_36956/how-collaboration-is-revolutionizing-medicine-34999060794e">privacy enhancing technology</a> (PET), to scale the training of these models on larger volumes of data and more diverse datasets. The <a href="https://www.nature.com/articles/s41591-022-02155-w">HealthChain</a> project was featured in Nature Medicine as a milestone achievement for how federated learning can empower AI in the medical domain, connecting real hospitals training models to answer medical questions in triple negative breast cancer. Including diversity in data sources from the beginning, as opposed to only using additional datasets for validation, generates models that are more generalisable and less biased. Hence they can be more readily applied in real world settings and lead to better outcomes for patients.</p><h3>What is federated learning</h3><p>Federated learning (FL) is a decentralized machine learning procedure to train models using multiple data providers. Instead of gathering data on a single server, the data remains locked on local servers as only the algorithms and models travel between the data holders.</p><p>The goal of this approach is to build models that benefit from a larger pool of more diverse data as compared to a single source. Not only does this method result in increased performance and improve the statistical robustness of the model trained, it also allows data scientists and researchers to use data in a way that respects individual data ownership and privacy. You can check out the <a href="https://huggingface.co/spaces/owkin/substra">Substra space on Hugging Face</a> to run a quick FL experiment if you’re interested in exploring an example.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*pWc0-Hgb2qUwfJuK" /><figcaption>An animation of Federated Averaging (FedAvg</figcaption></figure><p>The above graph shows how <a href="https://arxiv.org/abs/1602.05629">federated averaging</a> works, which is one of many FL strategies. In this strategy, the model trains on multiple datasets simultaneously on training nodes and then averages the different models’ weights at an aggregation node.</p><h3>What is Substra</h3><p>Substra is an open source FL software developed by Owkin which is now hosted by the Linux Foundation for AI and Data. Substra provides a proven framework to securely power the training and validation of models on distributed datasets. It includes a flexible Python interface for easily integrating FL into existing machine learning stacks, but also comes with a web application to monitor and analyze the results of FL experiments.</p><p>Although Substra is machine learning framework agnostic and can be used with any framework on any data modality, it also comes with a special interface for PyTorch users. FL researchers often opt to use PyTorch due to its additional flexibility as compared with other tools, which is very valuable in FL due to the high amount of iteration involved in building models. You can find a <a href="https://docs.substra.org/en/stable/substrafl_doc/examples/get_started/run_mnist_torch.html#sphx-glr-substrafl-doc-examples-get-started-run-mnist-torch-py">Substra example leveraging PyTorch here</a>.</p><h3>Real world applications of PyTorch with Substra</h3><p>Owkin is one of the leading companies in applied federated learning and have been working on projects since its introduction in medical research, including the flagship <a href="https://www.melloddy.eu/">MELLODDY</a> project which was a pivotal moment in the field. This was the largest ever pharma-industry AI collaboration for federated drug discovery, where Substra securely connected 10 pharma partners supported by 7 tech partners, enabling a collaboration between 100+ experts. The project contained the world’s largest collection of small molecules ( &gt;10 million annotated) with known biochemical or cellular activity. This enabled more accurate predictive models in drug discovery, with results recently published in the <a href="https://ojs.aaai.org/index.php/AAAI/index">proceedings of the AAAI conference on AI.</a></p><p>Due to the uniqueness of the project — where highly competitive companies were trying to collaborate on private business data — the best approach was to use federated learning. The partners chose to keep the head of their respective company models private while sharing a common body to build a base model. This was why PyTorch was selected by all the partners due to the flexibility of its building blocks.</p><p>To learn more about the project and the outcomes, you can check out the <a href="https://github.com/melloddy">MELLODDY open source Github</a> organization, where the repositories such as the <a href="https://github.com/melloddy/MELLODDY-TUNER">MELLODD Tuner</a> provide useful tools for understanding the work required in data preprocessing for federated learning.</p><p>PyTorch and Substra are also being used in the <a href="https://commonfund.nih.gov/bridge2ai">Bridge2AI</a> project, which is a unique National Institute of Health-led collaboration in which different universities and researchers are seeking to collaborate on voice datasets to discover how voice data can be used as a biomarker. We’ll do a deep-dive on this project later this year to explore exactly how the PyTorch models used in the Bridge2AI project enable medical breakthroughs.</p><p>As FL research grows in popularity, Substra is also being deployed in some recently launched large scale healthcare projects such as <a href="https://www.optima-oncology.eu/">OPTIMA</a> and <a href="https://digital-strategy.ec.europa.eu/en/policies/cancer-imaging">EUCAIM</a>.</p><h3>How to get started today</h3><p>If you’d like to learn more about FL, the best way to get started would be to simply go through the <a href="https://docs.substra.org/en/stable/substrafl_doc/examples/get_started/run_mnist_torch.html#sphx-glr-substrafl-doc-examples-get-started-run-mnist-torch-py">PyTorch example</a> on the Substra documentation.</p><p>Federated learning is still an evolving field and many different avenues for research are still open. Whether it be new strategies for how to federate centers or research in FL attacks, there’s a lot left to explore in this domain. Owkin also recently open sourced a collection of ready to use multimodal datasets called <a href="https://github.com/owkin/FLamby">FLamby</a> which helps evaluate different FL methods, which can be easily used in experiments.</p><p>Meta’s own FAIR team also invests in Federated Learning and provides <a href="https://github.com/facebookresearch/FLSim">FLSim</a> as exploration in this space. It is exciting to see more projects and companies investing in Federated Learning and we look forward to collaborating more in open source moving forward.</p><p>Come join us on the <a href="https://join.slack.com/t/substra-workspace/shared_invite/zt-1fqnk0nw6-xoPwuLJ8dAPXThfyldX8yA">Substra community on Slack</a> if you have a federated learning project in mind or if you’d simply like to learn more.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=563db4a69322" width="1" /><hr /><p><a href="https://medium.com/pytorch/how-pytorch-enables-medical-breakthroughs-with-federated-learning-at-owkin-563db4a69322">How PyTorch enables medical breakthroughs with federated learning at Owkin</a> was originally published in <a href="https://medium.com/pytorch">PyTorch</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/849f42bbc32a</id>
            <title>Unveiling the Power of Semi-Supervised Learning: The Unified Semi-Supervised Learning Benchmark</title>
            <link>https://medium.com/pytorch/unveiling-the-power-of-semi-supervised-learning-the-unified-semi-supervised-learning-benchmark-849f42bbc32a?source=rss----512b8efdf2e7---4</link>
            <guid isPermaLink="false">https://medium.com/p/849f42bbc32a</guid>
            <pubDate></pubDate>
            <updated>2023-07-06T14:53:06.926Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Machine Learning models thrive on high-quality, fully-annotated data. The traditional supervised learning approach typically requires data on the scale of millions, or even billions, to train large foundational models. However, obtaining such a vast amount of labeled data is often tedious and labor-intensive. As an alternative, semi-supervised learning (SSL) aims to enhance model generalization with only a fraction of labeled data, complemented by a considerable amount of unlabeled data. This blog introduces USB — the Unified Semi-Supervised Learning Framework and Benchmark, covering multi-modalities and various SSL scenarios.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*esowVVPdzTgM5gTb.png" /></figure><h3>Meet USB: A New More Academia-Friendly Benchmark Library with Diverse SSL Tasks</h3><p>Researchers from Microsoft Research Asia, in conjunction with researchers from Westlake University, the Tokyo Institute of Technology, Carnegie Mellon University, and the Max Planck Institute, proposed USB: the first Unified Semi-Supervised Learning Benchmark for Computer Vision (CV), Natural Language Processing (NLP), and Audio classification tasks. In contrast to previous SSL benchmarks, such as TorchSSL, which solely concentrated on a limited number of vision tasks, USB offers a broad spectrum of SSL tasks, spanning multi-modalities and accommodating different practical situations.</p><p>In particular, USB encompasses standard semi-supervised learning tasks for vision, text, and audio classification, where both the labeled and the unlabeled data distributions are balanced. It also extends to more demanding scenarios like long-tailed/imbalanced SSL and open-set SSL (work in progress), wherein either or both the labeled and unlabeled data distributions may be skewed. USB’s code structure encourages easy expansion to more benchmark settings. Furthermore, USB is the first to utilize pretrained models to substantially reduce the training cost of SSL algorithms (from 7000 GPU hours to 900 GPU hours), thereby making SSL research more accessible to academic researchers, particularly those in smaller research groups. The paper detailing USB has been accepted by NeurIPS 2022.</p><p>Paper: <a href="https://arxiv.org/pdf/2208.07204.pdf">https://arxiv.org/pdf/2208.07204.pdf</a></p><p>Github Repo: <a href="https://github.com/microsoft/Semi-supervised-learning">https://github.com/microsoft/Semi-supervised-learning</a></p><h3>Motivation Behind USB</h3><p>The past few decades have seen the flourishing of SSL, with significant strides made in Consistency Regularization and Self-Training with confidence thresholding, which have proven to be promising results. On unlabeled data, the model is encouraged to make consistent predictions for inputs under different perturbations, and a confidence thresholding mechanism is usually employed to select unlabeled data for training.</p><p>For instance, FixMatch [1] employs Augmentation Anchoring and Fixed Thresholding techniques to enhance the model’s generalization to different strongly augmented data, and reduce noisy pseudo labels. During training, FixMatch filters out unlabeled data with prediction confidence lower than a user-provided or predefined threshold. FlexMatch [3] and FreeMatch [4] introduce class-wise adaptive thresholding for a more flexible and efficient utilization of unlabeled data. SoftMatch [4] considers the issue of using unlabeled data from a re-weighting perspective. These methods are typically implemented in the Pytorch-based SSL codebase TorchSSL [5], proposed with FlexMatch [3].</p><p>Despite the fast development of SSL, we noticed that most SSL papers primarily focus on computer vision (CV) classification tasks, leaving researchers in other fields like natural language processing (NLP) and Audio processing wondering whether these SSL algorithms would work in their domains as well. Also, most work tends to focus on ideal data scenarios with balanced labeled and unlabeled data distributions, while, in reality, we often encounter imbalanced and out-of-domain data distribution. Another issue is that most SSL papers are being published by tech giants and not by academia. Academic labs, often hindered by computational resources constraints, are unable to contribute significantly to the development of semi-supervised fields. Generally, current SSL benchmarks suffer from these major issues:</p><p>(1) Insufficient Diversity: Existing SSL benchmarks are predominantly limited to computer vision (CV) classification tasks (i.e., CIFAR-10/100, SVHN, STL-10, and ImageNet classification), which neglects consistent and diverse evaluation on classification tasks in NLP and Audio, where there’s also a common issue of insufficient labeled data.</p><p>(2) Impractical Evaluation: Current SSL benchmarks mostly conduct evaluations on ideal data distribution with perfectly balanced labeled and unlabeled data. However, in practical situations, the data distribution, especially the unlabeled data, can be imbalanced and out-of-domain.</p><p>(3) Time-Consuming and Unfriendly to Academia: Existing SSL benchmarks like TorchSSL are often resource-intensive and environmentally unfriendly, requiring training deep neural network models, often from scratch. For instance, evaluating FixMatch[1] with TorchSSL takes about 300 GPU days, which puts SSL-related research beyond the reach of many research labs (especially in academia or smaller research groups), impeding the progress of SSL.</p><h3>Advancements Introduced by USB</h3><p>So how exactly does USB solve the prevailing challenges faced by current SSL benchmarks? It does so by incorporating the following advancements:</p><p>(1) Augmented Task Diversity: USB expands the scope of tasks by introducing 5 datasets each for CV, NLP, and audio domains. This comprehensive benchmark provides a diverse and challenging platform enabling consistent evaluation of SSL algorithms across domains and tasks. The table below gives a detailed comparison of tasks and training time between USB and TorchSSL.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*K_lmWrHBd28VLX4T.png" /></figure><p>(2) Facilitating Practical Evaluations: USB not only includes settings with balanced data distributions, but also more challenging settings with imbalanced and open-set data distributions. These configurations can also be expanded to different modalities that USB supports.</p><p>(3) Enhanced Training Efficiency: USB has integrated the pretrained Vision Transformer into SSL, thus eliminating the need for training ResNets from scratch. It has been observed that using a pretrained model significantly reduces the number of training iterations (for instance, it reduces the number of training iterations for CV tasks from 1 million steps to just 200,000 steps) without compromising performance.</p><p>(4) Improved User-Friendliness: The research team has open-sourced a modular codebase featuring 14 SSL algorithms along with related configuration files for easy replication. To ensure users can get started quickly, USB comes with detailed documentation and tutorials. Furthermore, USB offers a pip package — semilearn — allowing users to directly use the SSL algorithm. The team aspires to include new algorithms (such as imbalanced SSL algorithms, etc.) and more challenging datasets in USB’s future iterations. The following figure showcases the algorithms and modules currently supported by USB.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*173Nes-IorgmLqCR.png" /></figure><h3>Diving into USB: Usage Examples</h3><p>USB provides easy-to-use and modular codebase for training and evaluating SSL algorithms on supported benchmark, adaopting SSL algorithms on custom datasets, and designing/implementing new SSL algorithms. Detailed <a href="https://github.com/microsoft/Semi-supervised-learning/tree/main/notebooks">tutorials</a> are provided. In this section, we will quickly go through an example of adopting any supoorted SSL algorithm on custom data.</p><h4>Step 0: Import semilearn</h4><pre>import numpy as np<br />from torchvision import transforms<br />from semilearn import get_data_loader, get_net_builder, get_algorithm, get_config, Trainer<br />from semilearn import split_ssl_data, BasicDataset</pre><h4>Step 1: Define the config file</h4><p>In USB, we provide a set of config file for each algorithm and each setting (number of labels, imabalnced ratio, etc). Before start training on custom data, users need to specify in the config file/dict, the algorithm they want to use and the hyper-parameters for the selected algorithm.</p><pre># define configs and create config<br />config = {<br />    'algorithm': 'fixmatch',  # specify which algorithm you want to use.<br />    'net': 'vit_tiny_patch2_32', # specify which model you want to use.<br />    'use_pretrain': True, # whether or not to use pre-trained models<br />    'pretrain_path': 'https://github.com/microsoft/Semi-supervised-learning/releases/download/v.0.0.0/vit_tiny_patch2_32_mlp_im_1k_32.pth', # the pretrained model path we have provided</pre><pre>    # optimization configs<br />    'epoch': 100,  # set to 100<br />    'num_train_iter': 102400,  # set to 102400<br />    'num_eval_iter': 1024,   # set to 1024<br />    'num_log_iter': 256,    # set to 256<br />    'optim': 'AdamW',   # AdamW optimizer<br />    'lr': 5e-4,  # Learning rate<br />    'layer_decay': 0.5,  # Layer-wise decay learning rate  <br />    'batch_size': 16,  # Batch size <br />    'eval_batch_size': 16,</pre><pre>    # dataset configs <br />    'dataset': 'mnist', # default dataset config, can be ignored if using custom data<br />    'num_labels': 40,   # number of labels in the dataset, can be ignored if already specified/spliited in custom data<br />    'num_classes': 10, # number of classes<br />    'img_size': 32,  # image size <br />    'crop_ratio': 0.875,<br />    'data_dir': './data',</pre><pre>    # algorithm specific configs<br />    'hard_label': True,<br />    'uratio': 2,<br />    'ulb_loss_ratio': 1.0,</pre><pre>    # device configs<br />    'gpu': 0,<br />    'world_size': 1,<br />    &quot;num_workers&quot;: 2,<br />    'distributed': False,<br />}<br />config = get_config(config)</pre><p>After specifing the config, we can load the algorithm (with specified parameters):</p><pre># create model and specify algorithm<br />algorithm = get_algorithm(config,  get_net_builder(config.net, from_name=False), tb_log=None, logger=None)</pre><h4>Step 2: Load Custom Data</h4><p>Next step is loading the custom data. If in your custom data, you already have labeled data and unlabeled data spliited, you can load them using the Dataset provided in USB direclty.</p><pre>lb_data = np.random.randint(0, 255, size=3072 * 1000).reshape((-1, 32, 32, 3))<br />lb_data = np.uint8(lb_data)<br />lb_target = np.random.randint(0, 10, size=1000)</pre><pre>ulb_data = np.random.randint(0, 255, size=3072 * 5000).reshape((-1, 32, 32, 3))<br />ulb_data = np.uint8(ulb_data)<br />ulb_target = np.random.randint(0, 10, size=5000)</pre><pre>train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),<br />                                      transforms.RandomCrop(32, padding=int(32 * 0.125), padding_mode='reflect'),<br />                                      transforms.ToTensor(),<br />                                      transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])</pre><pre>train_strong_transform = transforms.Compose([transforms.RandomHorizontalFlip(),<br />                                             transforms.RandomCrop(32, padding=int(32 * 0.125), padding_mode='reflect'),<br />                                             transforms.ToTensor(),<br />                                             transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])</pre><pre>lb_dataset = BasicDataset(config.algorithm, lb_data, lb_target, config.num_classes, train_transform, is_ulb=False)<br />ulb_dataset = BasicDataset(config.algorithm, lb_data, lb_target, config.num_classes, train_transform, is_ulb=True, strong_transform=train_strong_transform)</pre><p>If you don not have spliited data but only want to experiment on some complete academic datasets, you can use the api provided for splitting the complete data:</p><pre># replace with your own code<br />data = np.random.randint(0, 255, size=3072 * 1000).reshape((-1, 32, 32, 3))<br />data = np.uint8(data)<br />target = np.random.randint(0, 10, size=1000)<br />lb_data, lb_target, ulb_data, ulb_target = split_ssl_data(config, data, target, 10,<br />                                                          config.num_labels, include_lb_to_ulb=config.include_lb_to_ulb)</pre><p>Then create the evaluation dataset:</p><pre>eval_data = np.random.randint(0, 255, size=3072 * 100).reshape((-1, 32, 32, 3))<br />eval_data = np.uint8(eval_data)<br />eval_target = np.random.randint(0, 10, size=100)</pre><pre>eval_transform = transforms.Compose([transforms.Resize(32),<br />                                      transforms.ToTensor(),<br />                                      transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])</pre><pre>eval_dataset = BasicDataset(config.algorithm, lb_data, lb_target, config.num_classes, eval_transform, is_ulb=False)</pre><p>Wrap the datasets to dataloaders:</p><pre>train_lb_loader = get_data_loader(config, lb_dataset, config.batch_size)<br />train_ulb_loader = get_data_loader(config, ulb_dataset, int(config.batch_size * config.uratio))<br />eval_loader = get_data_loader(config, eval_dataset, config.eval_batch_size)</pre><h4>Step 3: Train and Evaluate</h4><p>Training and evaluation can be done in 3 line of code with semilearn:</p><pre>trainer = Trainer(config, algorithm)<br />trainer.fit(train_lb_loader, train_ulb_loader, eval_loader)<br />trainer.evaluate(eval_loader)</pre><h4>More Examples</h4><p>More examples on how to use USB can be found in our repo!</p><h3>Looking Forward: The Future of USB</h3><p>As we look towards the future of USB, we have a clear vision in mind. We aim to expand the functionality and usability of USB. We plan to extend USB to more practical settings, tackling more complex and challenging real-world data distributions. A focus will be on dealing with imbalanced datasets and out-of-domain data. We are also looking at the wider application of pre-training within USB’s SSL algorithms. By leveraging pre-trained models, we anticipate significant improvements in performance and efficiency. Lastly, we aim to create an open and vibrant research community around USB. We plan to continually integrate state-of-the-art SSL algorithms into our codebase and encourage contributions from researchers globally. Join us on this exciting journey as we aim to revolutionize the landscape of semi-supervised learning and set new benchmarks in machine learning research.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=849f42bbc32a" width="1" /><hr /><p><a href="https://medium.com/pytorch/unveiling-the-power-of-semi-supervised-learning-the-unified-semi-supervised-learning-benchmark-849f42bbc32a">Unveiling the Power of Semi-Supervised Learning: The Unified Semi-Supervised Learning Benchmark</a> was originally published in <a href="https://medium.com/pytorch">PyTorch</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/37c4c0ef6ae1</id>
            <title>Introducing TorchOpt: A High-Performance Differentiable Optimization Library for PyTorch</title>
            <link>https://medium.com/pytorch/introducing-torchopt-a-high-performance-differentiable-optimization-library-for-pytorch-37c4c0ef6ae1?source=rss----512b8efdf2e7---4</link>
            <guid isPermaLink="false">https://medium.com/p/37c4c0ef6ae1</guid>
            <pubDate></pubDate>
            <updated>2023-07-05T15:34:01.252Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <blockquote>Explore TorchOpt, a PyTorch-based library that revolutionizes differentiable optimization with its unified programming abstraction, high-performance distributed execution runtime, and support for various differentiation modes.”</blockquote><p><em>This post is authored by Bo Liu, a Ph.D. student at National University of Singapore in </em><a href="https://www.comp.nus.edu.sg/cs/"><em>Department of Computer Science</em></a><em>. He was one of the members of the MetaOPT Team. The team also includes Jie Ren, Xidong Feng, Xuehai Pan, Luo Mai, Yaodong Yang.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Rjlk3USV0NFFS41LcqH2jA.png" /></figure><h3>Introducing TorchOpt</h3><p>The realm of machine learning (ML) has been transformed by differentiable programming, which facilitates automatic computation of derivatives within a high-level language. Its widespread application, from the backpropagation of neural networks to Bayesian inference and probabilistic programming, has significantly powered the progress of ML and its applications. It has enabled efficient and composable automatic differentiation (AD) tools, paving the way for advancements in differentiable optimization [1, 2], simulators [3, 4], engineering [5], and science [6]. The burgeoning number of differentiable optimization algorithms has underscored the essential role of differentiable programming.</p><p>Enter <a href="https://github.com/metaopt/torchopt">TorchOpt</a> — an efficient library for differentiable optimization that builds upon <a href="https://pytorch.org/">PyTorch</a>. TorchOpt is available on GitHub at <a href="https://github.com/metaopt/torchopt">https://github.com/metaopt/torchopt</a>.</p><p>TorchOpt offers:</p><ul><li><strong>Versatility</strong>: TorchOpt encompasses three differentiation modes — explicit differentiation, implicit differentiation, and zero-order differentiation, catering to various differentiable optimization needs.</li><li><strong>Flexibility</strong>: TorchOpt delivers a functional and objective-oriented API to cater to different user preferences. You can implement differentiable optimization in a style akin to JAX or PyTorch.</li><li><strong>Efficiency</strong>: TorchOpt offers CPU/GPU-accelerated differentiable optimizers, an RPC-based distributed training framework, and fast tree operations, dramatically enhancing training efficiency for bi-level optimization problems.</li></ul><h3>Why TorchOpt?</h3><p>TorchOpt melds two pivotal facets — a unified and expressive differentiable optimization programming abstraction and a high-speed distributed execution runtime.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ENkt-d2NcYizhYHb" /><figcaption>Unified and expressive differentiable optimization programming abstraction</figcaption></figure><p>TorchOpt presents an abstraction that promotes the efficient definition and analysis of differentiable optimization programs, accommodating explicit, implicit, and zero-order gradients.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7Br5-b81HvjHdiPbKcrmwA.png" /><figcaption>TorchOpt’s differentiation modes. By formulating the problem as a differentiable problem, TorchOpt offers Autograd support for the backward pass (dotted lines).</figcaption></figure><p>TorchOpt offers a diverse set of low-level, high-level, functional, and Object-Oriented (OO) APIs to enable users to incorporate differentiable optimization within the computational graphs produced by PyTorch. Specifically, TorchOpt supports three differentiation modes for handling differentiable optimization problems:</p><p>(i) Explicit gradient for unrolled optimization,</p><p>(ii) Implicit gradient for solution-based iterative optimization,</p><p>(iii) Zero-order gradient estimation for non-smooth/non-differentiable functions.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Ak5KdZl4u24-LUE-" /><figcaption>High-performance and distributed execution runtime</figcaption></figure><p>TorchOpt offers high-performance and distributed execution runtime containing several accelerated solutions to support fast differentiation with different modes on GPU &amp; CPU and distributed training features for multi-node multi-GPU. The figures below show the comparison of TorchOpt with other baselines with CPU/GPU-accelerated op and distributed training.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*U4IOUYYw0k2zjQSe" /><figcaption>Performance of TorchOpt, (a) and (b) are the forward/backward time (Adam optimizer) in different parameter sizes comparing TorchOpt and PyTorch, (c) is the speedup ratio on multi-GPUs using RPC compared with the sequential implementation.</figcaption></figure><p>For PyTorch researchers and developers, TorchOpt’s features enable efficient declaration and analysis of various differentiable optimization programs, complete parallelization of computation-intensive differentiation operations, and automatic distribution of computation to distributed devices.</p><h3>Usage Examples</h3><p>Let’s delve into two specific usage examples of TorchOpt. We’ll guide you through each step, providing visuals or code examples for better comprehension.</p><h4>A warm-up example for differentiable optimizers</h4><p>Let us start with a warm-up example.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9JoJUiqpuK5iUYrBHwUYjg.png" /></figure><p>Given the analytical solution above, let’s validate it using MetaOptimizer in TorchOpt. MetaOptimizer is our differentiable optimizer's main class. It combines with the functional optimizers torchopt.sgd and torchopt.adam to define our high-level APIs torchopt.MetaSGD and torchopt.MetaAdam.</p><p>Let us start. First, define the network.</p><pre>from IPython.display import display<br /><br />import torch<br />import torch.nn as nn<br />import torch.nn.functional as F<br /><br />import torchopt<br /><br /><br />class Net(nn.Module):<br />    def __init__(self):<br />        super().__init__()<br />        self.a = nn.Parameter(torch.tensor(1.0), requires_grad=True)<br /><br />    def forward(self, x):<br />        return self.a * (x**2)</pre><p>Then we declare the network (parameterized by a) and the meta-parameter x. Do not forget to set flag requires_grad=True for x.</p><pre>net = Net()<br />x = nn.Parameter(torch.tensor(2.0), requires_grad=True)</pre><p>Next we declare the meta-optimizer. Here we show two equivalent ways of defining the meta-optimizer.</p><pre># Low-level API<br />optim = torchopt.MetaOptimizer(net, torchopt.sgd(lr=1.0))<br /><br /># High-level API<br />optim = torchopt.MetaSGD(net, lr=1.0)</pre><p>The meta-optimizer takes the network as input and use method step to update the network (parameterized by a). Finally, we show how a bi-level process works.</p><pre>inner_loss = net(x)<br />optim.step(inner_loss)<br /><br />outer_loss = net(x)<br />outer_loss.backward()<br /># x.grad = - 4 * lr * x^3 + 2 * a_0 * x<br />#        = - 4 * 1 * 2^3 + 2 * 1 * 2<br />#        = -32 + 4<br />#        = -28<br />print(f'x.grad = {x.grad!r}')</pre><p>The output is:</p><pre>x.grad = tensor(-28.)</pre><h4>Implementing Model-Agnostic Meta-Learning (MAML) Using TorchOpt</h4><p>Let us start with the core idea of the Model-Agnostic Meta-Learning (MAML) algorithm. MAML is an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples.</p><p>In the MAML approach, a model is trained on a variety of tasks, and then fine-tuned with one or a few gradient steps computed on a small amount of data from a new task. The key insight of MAML is to train the initial model such that these few steps of fine-tuning lead to good generalization performance on the new task.</p><p>The update rule in MAML is defined as:</p><p>Given a learning rate alpha for the fine-tuning step, theta should minimize</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*haTJfzqj1_aPRVQK.png" /></figure><p>Optimizing this objective is the goal of the meta-training procedure. Given a task i from the distribution of tasks, p(T), the model parameters, theta, are updated using one or more gradient descent steps on the loss L_i of task i, resulting in the task-specific parameters theta_i'. The update rule is written as theta_i' = theta - alpha * grad(L_i(theta)), where alpha is the learning rate and grad denotes the gradient.</p><p>After this update for each task in the batch, the model parameters theta are updated using gradient descent on the sum of the losses L_i of all tasks i in the batch, with the loss L_i computed using the task-specific parameters theta_i'. This update rule is written as theta = theta - beta * grad(sum_i(L_i(theta_i')), where beta is the learning rate and grad denotes the gradient.</p><p>Here, alpha and beta are hyperparameters that determine the step size of the gradient descent updates. The learning rate alpha is typically chosen to be small so that the model can adapt quickly to each task, while the learning rate beta is typically chosen to be large so that the model can learn effectively from the distribution of tasks.</p><p>Now, let’s explain the provided code example of implementing the MAML algorithm in reinforcement learning with TorchOpt.</p><p>We start by defining some parameters related to the tasks, trajectories, states, actions, and iterations.</p><pre>import argparse<br />from typing import NamedTuple<br /><br />import gym<br />import numpy as np<br />import torch<br />import torch.optim as optim<br /><br />import torchopt<br />from helpers.policy import CategoricalMLPPolicy<br /><br /><br />TASK_NUM = 40<br />TRAJ_NUM = 20<br />TRAJ_LEN = 10<br /><br />STATE_DIM = 10<br />ACTION_DIM = 5<br /><br />GAMMA = 0.99<br />LAMBDA = 0.95<br /><br />outer_iters = 500<br />inner_iters = 1</pre><p>Next, we define a class named Traj to represent a trajectory, which includes the observed states, actions taken, the states observed after taking the actions, the rewards obtained, and the gamma values for discounting future rewards.</p><pre>class Traj(NamedTuple):<br />    obs: np.ndarray<br />    acs: np.ndarray<br />    next_obs: np.ndarray<br />    rews: np.ndarray<br />    gammas: np.ndarray</pre><p>We then define a function sample_traj to generate a trajectory given the environment, task, policy, and parameters. This function simulates the interaction between the policy and the environment for TRAJ_LEN steps.</p><pre>def sample_traj(env, task, policy):<br />    env.reset_task(task)<br />    obs_buf = np.zeros(shape=(TRAJ_LEN, TRAJ_NUM, STATE_DIM), dtype=np.float32)<br />    next_obs_buf = np.zeros(shape=(TRAJ_LEN, TRAJ_NUM, STATE_DIM), dtype=np.float32)<br />    acs_buf = np.zeros(shape=(TRAJ_LEN, TRAJ_NUM), dtype=np.int8)<br />    rews_buf = np.zeros(shape=(TRAJ_LEN, TRAJ_NUM), dtype=np.float32)<br />    gammas_buf = np.zeros(shape=(TRAJ_LEN, TRAJ_NUM), dtype=np.float32)<br />    with torch.no_grad():<br />        for batch in range(TRAJ_NUM):<br />            ob = env.reset()<br />            for step in range(TRAJ_LEN):<br />                ob_tensor = torch.from_numpy(ob)<br />                pi, _ = policy(ob_tensor)<br />                ac_tensor = pi.sample()<br />                ac = ac_tensor.cpu().numpy()<br />                next_ob, rew, done, info = env.step(ac)<br /><br />                obs_buf[step][batch] = ob<br />                next_obs_buf[step][batch] = next_ob<br />                acs_buf[step][batch] = ac<br />                rews_buf[step][batch] = rew<br />                gammas_buf[step][batch] = (1 - done) * GAMMA<br />                ob = next_ob<br />    return Traj(<br />        obs=obs_buf,<br />        acs=acs_buf,<br />        next_obs=next_obs_buf,<br />        rews=rews_buf,<br />        gammas=gammas_buf,<br />    )</pre><p>The a2c_loss function is used to compute the loss for the Actor-Critic (A2C) algorithm. The A2C algorithm is a type of policy gradient method that uses a value function (the critic) to reduce the variance of the policy gradient (the actor).</p><pre>def a2c_loss(traj, policy, value_coef):<br />    lambdas = np.ones_like(traj.gammas) * LAMBDA<br />    _, next_values = policy(torch.from_numpy(traj.next_obs))<br />    next_values = torch.squeeze(next_values, -1).detach().numpy()<br />    # Work backwards to compute `G_{T-1}`, ..., `G_0`.<br />    returns = []<br />    g = next_values[-1, :]<br />    for i in reversed(range(next_values.shape[0])):<br />        g = traj.rews[i, :] + traj.gammas[i, :] * (<br />            (1 - lambdas[i, :]) * next_values[i, :] + lambdas[i, :] * g<br />        )<br />        returns.insert(0, g)<br />    lambda_returns = torch.from_numpy(np.array(returns))<br />    pi, values = policy(torch.from_numpy(traj.obs))<br />    log_probs = pi.log_prob(torch.from_numpy(traj.acs))<br />    advs = lambda_returns - torch.squeeze(values, -1)<br />    action_loss = -(advs.detach() * log_probs).mean()<br />    value_loss = advs.pow(2).mean()<br /><br />    loss = action_loss + value_coef * value_loss<br />    return loss</pre><p>The evaluate function is used to evaluate the performance of the policy on different tasks. It uses the inner optimizer to fine-tune the policy on each task and then computes the rewards before and after the fine-tuning.</p><pre>def evaluate(env, seed, task_num, policy):<br />    pre_reward_ls = []<br />    post_reward_ls = []<br />    inner_opt = torchopt.MetaSGD(policy, lr=0.1)<br />    env = gym.make(<br />        'TabularMDP-v0',<br />        num_states=STATE_DIM,<br />        num_actions=ACTION_DIM,<br />        max_episode_steps=TRAJ_LEN,<br />        seed=args.seed,<br />    )<br />    tasks = env.sample_tasks(num_tasks=task_num)<br />    policy_state_dict = torchopt.extract_state_dict(policy)<br />    optim_state_dict = torchopt.extract_state_dict(inner_opt)<br />    for idx in range(task_num):<br />        for _ in range(inner_iters):<br />            pre_trajs = sample_traj(env, tasks[idx], policy)<br />            inner_loss = a2c_loss(pre_trajs, policy, value_coef=0.5)<br />            inner_opt.step(inner_loss)<br />        post_trajs = sample_traj(env, tasks[idx], policy)<br /><br />        # Logging<br />        pre_reward_ls.append(np.sum(pre_trajs.rews, axis=0).mean())<br />        post_reward_ls.append(np.sum(post_trajs.rews, axis=0).mean())<br /><br />        torchopt.recover_state_dict(policy, policy_state_dict)<br />        torchopt.recover_state_dict(inner_opt, optim_state_dict)<br />    return pre_reward_ls, post_reward_ls</pre><p>In the main function, we initialize the environment, policy, and optimizers. The policy is a simple MLP that outputs a categorical distribution over the actions. The inner optimizer is used to update the policy parameters during the fine-tuning phase, and the outer optimizer is used to update the policy parameters during the meta-training phase. The performance is evaluated by the rewards before and after the fine-tuning. The training process is logged and printed for each outer iteration.</p><pre>def main(args):<br />    # init training<br />    torch.manual_seed(args.seed)<br />    torch.cuda.manual_seed_all(args.seed)<br />    # Env<br />    env = gym.make(<br />        'TabularMDP-v0',<br />        num_states=STATE_DIM,<br />        num_actions=ACTION_DIM,<br />        max_episode_steps=TRAJ_LEN,<br />        seed=args.seed,<br />    )<br />    # Policy<br />    policy = CategoricalMLPPolicy(input_size=STATE_DIM, output_size=ACTION_DIM)<br />    inner_opt = torchopt.MetaSGD(policy, lr=0.1)<br />    outer_opt = optim.Adam(policy.parameters(), lr=1e-3)<br />    train_pre_reward = []<br />    train_post_reward = []<br />    test_pre_reward = []<br />    test_post_reward = []<br /><br />    for i in range(outer_iters):<br />        tasks = env.sample_tasks(num_tasks=TASK_NUM)<br />        train_pre_reward_ls = []<br />        train_post_reward_ls = []<br /><br />        outer_opt.zero_grad()<br /><br />        policy_state_dict = torchopt.extract_state_dict(policy)<br />        optim_state_dict = torchopt.extract_state_dict(inner_opt)<br />        for idx in range(TASK_NUM):<br />            for _ in range(inner_iters):<br />                pre_trajs = sample_traj(env, tasks[idx], policy)<br />                inner_loss = a2c_loss(pre_trajs, policy, value_coef=0.5)<br />                inner_opt.step(inner_loss)<br />            post_trajs = sample_traj(env, tasks[idx], policy)<br />            outer_loss = a2c_loss(post_trajs, policy, value_coef=0.5)<br />            outer_loss.backward()<br />            torchopt.recover_state_dict(policy, policy_state_dict)<br />            torchopt.recover_state_dict(inner_opt, optim_state_dict)<br />            # Logging<br />            train_pre_reward_ls.append(np.sum(pre_trajs.rews, axis=0).mean())<br />            train_post_reward_ls.append(np.sum(post_trajs.rews, axis=0).mean())<br />        outer_opt.step()<br /><br />        test_pre_reward_ls, test_post_reward_ls = evaluate(env, args.seed, TASK_NUM, policy)<br /><br />        train_pre_reward.append(sum(train_pre_reward_ls) / TASK_NUM)<br />        train_post_reward.append(sum(train_post_reward_ls) / TASK_NUM)<br />        test_pre_reward.append(sum(test_pre_reward_ls) / TASK_NUM)<br />        test_post_reward.append(sum(test_post_reward_ls) / TASK_NUM)<br /><br />        print('Train_iters', i)<br />        print('train_pre_reward', sum(train_pre_reward_ls) / TASK_NUM)<br />        print('train_post_reward', sum(train_post_reward_ls) / TASK_NUM)<br />        print('test_pre_reward', sum(test_pre_reward_ls) / TASK_NUM)<br />        print('test_post_reward', sum(test_post_reward_ls) / TASK_NUM)</pre><p>In summary, this code example shows how to implement the MAML algorithm for reinforcement learning tasks using TorchOpt. The MAML algorithm is implemented in a flexible way that is compatible with any model trained with gradient descent, making it a powerful tool for meta-learning tasks.</p><h3>Forward-Looking Statement</h3><p>TorchOpt is a novel and efficient differentiable optimization library for PyTorch. Our experimental results highlight TorchOpt’s potential as a user-friendly, high-performance, and scalable library for supporting challenging gradient computation with PyTorch. We plan to support more complex differentiation modes and cover more non-trivial gradient computation problems in the future. TorchOpt has already proved useful for meta-gradient research, and we are confident that it can serve as a critical auto-differentiation tool for an even broader range of differentiable optimization problems.</p><p>We’re enthusiastic about TorchOpt’s potential and are dedicated to its ongoing development and refinement. We welcome community feedback and contributions to help us make TorchOpt even better. Stay tuned for more updates and features in the coming months!</p><h3>Acknowledgements</h3><ol><li><em>The </em><a href="https://github.com/google/jaxopt/tree/main"><em>JAXopt</em></a><em> [7] library, with its well-designed APIs for implicit gradient differentiation, has greatly inspired us. Its approach to hardware-accelerated, batchable, and differentiable optimization solutions has offered us significant insights into managing optimization problems effectively.</em></li><li><a href="https://github.com/deepmind/optax/tree/master"><em>Optax</em></a><em> [8], with its focus on functional programming and gradient processing, has been a fundamental basis for our work. The manner in which it combines low-level ingredients into custom optimizers has inspired us in designing our own functional APIs, greatly enhancing the efficiency of our project.</em></li><li><a href="https://github.com/leopard-ai/betty"><em>Betty</em></a><em> [9], an automatic differentiation library for generalized meta-learning and multilevel optimization, has also been a valuable reference for us. While not directly integrated into our project, its features have offered us useful insights and contributed to the conceptualization and design of features in our own TorchOpt library.</em></li></ol><p><strong>References</strong></p><p>[1] Liu, B., Feng, X., Ren, J., Mai, L., Zhu, R., Zhang, H., … &amp; Yang, Y. (2022). A theoretical understanding of gradient bias in meta-reinforcement learning. <em>Advances in Neural Information Processing Systems</em>, <em>35</em>, 31059–31072.</p><p>[2] Finn, C., Abbeel, P., &amp; Levine, S. (2017, July). Model-agnostic meta-learning for fast adaptation of deep networks. In <em>International conference on machine learning</em> (pp. 1126–1135). PMLR.</p><p>[3] Hu, Y., Anderson, L., Li, T. M., Sun, Q., Carr, N., Ragan-Kelley, J., &amp; Durand, F. (2019). Difftaichi: Differentiable programming for physical simulation. <em>arXiv preprint arXiv:1910.00935</em>.</p><p>[4] Freeman, C. D., Frey, E., Raichuk, A., Girgin, S., Mordatch, I., &amp; Bachem, O. (2021). Brax — A Differentiable Physics Engine for Large Scale Rigid Body Simulation. <em>arXiv preprint arXiv:2106.13281</em>.</p><p>[5] Schoenholz, S., &amp; Cubuk, E. D. (2020). Jax md: a framework for differentiable physics. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 11428–11441.</p><p>[6] Raissi, M., Perdikaris, P., &amp; Karniadakis, G. E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. <em>Journal of Computational physics</em>, <em>378</em>, 686–707.</p><p>[7] Blondel, M., Berthet, Q., Cuturi, M., Frostig, R., Hoyer, S., Llinares-López, F., … &amp; Vert, J. P. (2022). Efficient and modular implicit differentiation. <em>Advances in neural information processing systems</em>, <em>35</em>, 5230–5242.</p><p>[8] Babuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, J., Buchlovsky, P., Budden, D., Cai, T., Clark, A., Danihelka, I., Dedieu, A., Fantacci, C., Godwin, J., Jones, C., Hemsley, R., Hennigan, T., Hessel, M., Hou, S., Kapturowski, S., … Viola, F. (2020). The DeepMind JAX Ecosystem. <a href="http://github.com/deepmind"><em>http://github.com/deepmind</em></a><em>.</em></p><p>[9] Choe, S. K., Neiswanger, W., Xie, P., &amp; Xing, E. (2022). Betty: An automatic differentiation library for multilevel optimization. arXiv preprint arXiv:2207.02849.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=37c4c0ef6ae1" width="1" /><hr /><p><a href="https://medium.com/pytorch/introducing-torchopt-a-high-performance-differentiable-optimization-library-for-pytorch-37c4c0ef6ae1">Introducing TorchOpt: A High-Performance Differentiable Optimization Library for PyTorch</a> was originally published in <a href="https://medium.com/pytorch">PyTorch</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>