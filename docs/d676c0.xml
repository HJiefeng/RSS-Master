<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>Netflix TechBlog - Medium</title>
        <link>https://netflixtechblog.com?source=rss----2615bd06b42e---4</link>
        
        <item>
            <id>https://medium.com/p/e4e86caf9249</id>
            <title>Behind the Scenes: Building a Robust Ads Event Processing Pipeline</title>
            <link>https://netflixtechblog.com/behind-the-scenes-building-a-robust-ads-event-processing-pipeline-e4e86caf9249?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/e4e86caf9249</guid>
            <pubDate></pubDate>
            <updated>2025-05-09T20:00:22.604Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://www.linkedin.com/in/kineshsatiya/">Kinesh Satiya</a></p><h4>Introduction</h4><p>In a digital advertising platform, a robust feedback system is essential for the lifecycle and success of an ad campaign. This system comprises of diverse sub-systems designed to monitor, measure, and optimize ad campaigns. At Netflix, we embarked on a journey to build a robust event processing platform that not only meets the current demands but also scales for future needs. This blog post delves into the architectural evolution and technical decisions that underpin our Ads event processing pipeline.</p><p>Ad serving acts like the “brain” — making decisions, optimizing delivery and ensuring right Ad is shown to the right member at the right time. Meanwhile, ad events, after an Ad is rendered, function like “heartbeats”, continuously providing real-time feedback (oxygen/nutrients) that fuels better decision-making, optimizations, reporting, measurement, and billing. Expanding on this analogy:</p><ul><li>Just as the brain relies on continuous blood flow, ad serving depends on a steady stream of ad events to adjust next ad serving decision, frequency capping, pacing, and personalization.</li><li>If the nervous system stops sending signals (ad events stop flowing), the brain (ad serving) lacks critical insights and starts making poor decisions or even fails.</li><li>The healthier and more accurate the event stream (just like strong heart function), the better the ad serving system can adapt, optimize, and drive business outcomes.</li></ul><p>Let’s dive into the journey of building this pipeline.</p><h4>The Pilot</h4><p>In November 2022, we launched a brand <a href="https://about.netflix.com/en/news/announcing-basic-with-ads-us">new basic ads plan</a>, in partnership with Microsoft. The software systems extended the existing Netflix playback systems to play ads. Initially, the system was designed to be simple, secure, and efficient, with an underlying ethos of device-originated and server-proxied operations. The system consisted of three main components: the Microsoft Ad Server, Netflix Ads Manager, and Ad Event Handler. Each ad served required tracking to ensure the feedback loop functioned effectively, providing the external ad server with insights on impressions, frequency capping (advertiser policy that limits the number of times a user sees a specific ad), and monetization processes.</p><p>Key features of this system include:</p><ol><li><strong>Client Request: </strong>Client devices request for ads during an ad break from Netflix playback systems, which is then decorated with information by ads manager to request ads from the ad server.</li><li><strong>Server-Side Ad Insertion:</strong> The Ad Server sends ad responses using the VAST (Video Ad Serving Template) format.</li><li><strong>Netflix Ads Manager:</strong> This service parses VAST documents, extracts tracking event information, and creates a simplified response structure for Netflix playback systems and client devices. <br /> — The tracking information is packed into a structured protobuf data model.<br /> — This structure is encrypted to create an opaque token.<br /> — The final response, informs the client devices, when to send an event and the corresponding token.</li><li><strong>Client Device:</strong> During ad playback, client devices send events accompanied by a token. The Netflix telemetry system then enqueues all these events in Kafka for asynchronous processing.</li><li><strong>Ads Event Handler:</strong> This component is a Kafka consumer, that reads/decrypts the event payload and forwards the tracking information encoded back to the ad server and other vendors.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*S_6xv6LxoRyL8KtUXqWueQ.png" /><figcaption>Fig 1: Basic Ad Event Handling System</figcaption></figure><p>There is an <a href="https://netflixtechblog.com/ensuring-the-successful-launch-of-ads-on-netflix-f99490fdf1ba">excellent prior blog</a> post that explains how this systems was tested end-to-end at scale. This system design allowed us to quickly add new integrations for verification with vendors like DV, IAS and Nielsen for measurement.</p><h4>The Expansion</h4><p>As we continued to expand our third-party (3P) advertising vendors for measurement, tracking and verification, we identified a critical trend: growth in the volume of data encapsulated within opaque tokens. These tokens, which are cached on client devices, present a risk of elevated memory usage, potentially impacting device performance. We also anticipated increase in third-party tracking URLs, metadata needs, and more event types as our business added new capabilities.</p><p>To strategically address these challenges, we introduced a new persistence layer using <a href="https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30">Key-Value abstraction</a>, between ad serving and event handling system: Ads Metadata Registry. This transient storage service stores metadata for each Ad served, and upon callback, event handler would read the tracking information to relay information to the vendors. The contract between the client device and Ads systems continues to use the opaque token per event, but now, instead of tracking information, it contains reference identifiers — Ad ID, the corresponding metadata record ID in the registry and the event name. This approach future proofed our systems to handle any growth in data that needs to pass from ad serving to event handling systems.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0wP5cyAj84Vju7ryabJE1A.png" /><figcaption>Fig 2: Storage service between Ad Serving &amp; Reporting</figcaption></figure><h4>The Evolution</h4><p>In January of 2024, we decided to invest in in-house advertising technology platform. This implied that the event processing pipeline had to evolve significantly — attain parity with existing offerings and continue to support new product launches with rapid iterations using in-house Netflix Ad Server. This required re-evaluation of the entire architecture across all of Ads engineering teams.</p><p>First, we made an inventory of the use-cases that would need to be supported through ad events.</p><ol><li>We’d need to start supporting frequency capping in-house for all ads through Netflix Ad server.</li><li>Incorporate pricing information for impressions to set the stage for billing events, which are used to charge advertisers.</li><li>A robust reporting system to share campaign reports with advertisers, combined with metrics data collection, helps assess the delivery and effectiveness of the campaign.</li><li>Scale event handler to perform tracking information look-ups across different vendors.</li></ol><p>Next, we examined upcoming launches, such as Pause/Display ads, to gain deeper insights into our strategic initiatives. We recognized that Display Ads would utilize a distinct logging framework, suggesting that different upstream pipelines might deliver ad telemetry. However, the downstream use-cases were expected to remain largely consistent. Additionally, by reviewing the goals of our telemetry teams, we saw large initiatives aimed at upgrading the platform, indicating potential future migrations.</p><p>Keeping the above insights &amp; challenges in mind,</p><ul><li>We planned a centralized ad event collection system. This centralized service would consolidate common operations like decryption of tokens, enrichment, hashing identifiers into a single step execution and provide a single unified data contract to consumers that is highly extensible (like being agnostic to ad server &amp; ad media).</li><li>We proposed moving all consumers of ad telemetry downstream of the centralized service. This creates a clean separation between upstream systems and consumers in Ads Engineering.</li><li>In the initial development phase of our advertising system, a crucial component was the creation of ad sessions based on individual ad events. This system was constructed using ad playback telemetry, which allowed us to gather essential metrics from these ad sessions. A significant decision in this plan was to position the ad sessionization process downstream of the raw ad events.</li><li>The proposal also recommended moving all our Ads data processing pipelines for reporting/analytics/metrics for Ads using the data published by the centralized system.</li></ul><p>Putting together all the components in our vision -</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*aPL3RHeFEzlw_psaLddWKw.png" /><figcaption>Fig 3: Ad Event processing pipeline</figcaption></figure><p>Key components on event processing pipeline -</p><p><strong>Ads Event Publisher:</strong> This centralized system is responsible for collecting ads telemetry and providing unified ad events to the ads engineering teams. It supports various functions such as measurement, finance/billing, reporting, frequency capping, and maintaining an essential feedback loop back to the ad server.</p><p><strong>Realtime Consumers</strong></p><ol><li><strong>Frequency Capping: </strong>This system tracks impressions for each campaign, profile, and any other frequency capping parameters set up for the campaign. It is utilized by the Ad Server during each ad decision to ensure ads are served with frequency limits.</li><li><strong>Ads Metrics: </strong>This component is a Flink job that transforms raw data to a set of dimensions and metrics, subsequently writing to Apache Druid OLAP database. The streaming data is further backed by an offline process that corrects any inaccuracy during streaming ingestion and providing accurate metrics. It provides real-time metrics to assess the delivery health of campaigns and applies budget capping functionality.</li><li><strong>Ads Sessionizer: </strong>An Apache Flink job that consolidates all events related to a single ad into an Ad Session. This session provides real-time information about ad playback, offering essential business insights and reporting. It is a crucial job that supports all downstream analytical and reporting processes.</li><li><strong>Ads Event Handler: </strong>This service continuously sends information to ad vendors by reading tracking information from ad events, ensuring accurate and timely data exchange.</li></ol><p><strong>Billing/Revenue: </strong>These are offline workflows designed to curate impressions, supporting billing and revenue recognition processes.</p><p><strong>Ads Reporting &amp; Metrics: </strong>This service powers reporting module for our account managers and provides a centralized metrics API that help assess the delivery of a campaign.</p><p>This was a massive multi-quarter effort across different engineering teams. With extensive planning (kudos to our TPM team!) and coordination, we were able to iterate fast, build several services and execute the vision above, to power our in-house ads technology platform.</p><h4>Conclusion</h4><p>These systems have significantly accelerated our ability to launch new capabilities for the business.</p><ul><li>Through our partnership with Microsoft, Display Ad events were integrated into the new pipeline for reusability and ensuring when launching through Netflix ads systems, all use-cases were covered.</li><li>Programmatic buying capabilities now support the exchange of numerous trackers and dynamic bid prices on impression events.</li><li>Sharing opt-out signals helps ensure privacy and compliance with GDPR regulations for Ads business in Europe, supporting accurate reporting and measurement.</li><li>New event types like Ad clicks and scanning of QR codes events also flow through the pipeline, ensuring all metrics and reporting are tracked consistently.</li></ul><p><strong>Key Takeways</strong></p><ul><li><strong>Strategic, incremental evolution:</strong> The development of our ads event processing systems has been a carefully orchestrated journey. Each iteration was meticulously planned by addressing existing challenges, anticipating future needs, and showcasing teamwork, planning, and coordination across various teams. These pillars have been fundamental to the success of this journey.</li><li><strong>Data contract:</strong> A clear data contract has been pivotal in ensuring consistency in interpretation and interoperability across our systems. By standardizing the data models, and establishing a clear data exchange between ad serving, and centralized event collection, our teams have been able to iterate at exceptional speed and continue to deliver many launches on time.</li><li><strong>Separation of concerns: </strong>Consumers are relieved from the need to understand each source of ad telemetry or manage updates and migrations. Instead, a centralized system handles these tasks, allowing consumers to focus on their core business logic.</li></ul><p>We have an exciting list of projects on the horizon. These include managing ad events from ads on Netflix live streams, de-duplication processes, and enriching data signals to deliver enhanced reporting and insights. Additionally, we are advancing our Native Ads strategy, integrating Conversion API for improved conversion tracking, among many others.</p><p>This is definitely not a season finale; it’s just the beginning of our journey to create a best-in-class ads technology platform. We warmly invite you to share your thoughts and comments with us. If you’re interested in learning more or becoming a part of this innovative journey, <a href="https://jobs.netflix.com/">Ads Engineering is hiring</a>!</p><h4><em>Acknowledgements</em></h4><p><em>A special thanks to our amazing colleagues and teams who helped build our foundational post-impression system: </em><a href="https://www.linkedin.com/in/simonspencer1/"><em>Simon Spencer</em></a><em>, </em><a href="https://www.linkedin.com/in/priyankaavj/"><em>Priyankaa Vijayakumar,</em></a><em> </em><a href="https://www.linkedin.com/in/indrajit-roy-choudhury-5b011754/"><em>Indrajit Roy Choudhury</em></a><em>; Ads TPM team — </em><a href="https://www.linkedin.com/in/sonyabellamy/"><em>Sonya Bellamy</em></a><em>; the Ad Serving Team —</em><a href="https://www.linkedin.com/in/andrewjsweeney/"><em> Andrew Sweeney</em></a><em>, </em><a href="https://www.linkedin.com/in/tim-z-b9112034/"><em>Tim Zheng</em></a>, <a href="https://www.linkedin.com/in/haidongt/"><em>Haidong Tang</em></a><em> and </em><a href="https://www.linkedin.com/in/edhbarker/"><em>Ed Barker</em></a><em>; the Ads Data Engineering Team — </em><a href="https://www.linkedin.com/in/sonalisharma/"><em>Sonali Sharma</em></a><em>, </em><a href="https://www.linkedin.com/in/harshavardhan-arepalli-3a9b0992/"><em>Harsha Arepalli</em></a><em>, and </em><a href="https://www.linkedin.com/in/winifredtran/"><em>Wini Tran</em></a><em>; Product Data Systems — </em><a href="https://www.linkedin.com/in/d3cay/"><em>David Klosowski;</em></a><em> and the entire Ads Reporting and Measurement team!</em></p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e4e86caf9249" width="1" /><hr /><p><a href="https://netflixtechblog.com/behind-the-scenes-building-a-robust-ads-event-processing-pipeline-e4e86caf9249">Behind the Scenes: Building a Robust Ads Event Processing Pipeline</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/58c13d2a6f6e</id>
            <title>Measuring Dialogue Intelligibility for Netflix Content</title>
            <link>https://netflixtechblog.com/measuring-dialogue-intelligibility-for-netflix-content-58c13d2a6f6e?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/58c13d2a6f6e</guid>
            <pubDate></pubDate>
            <updated>2025-05-08T17:52:31.889Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><em>Enhancing Member Experience Through Strategic Collaboration</em></p><p><a href="https://www.linkedin.com/in/ozziesutherland/">Ozzie Sutherland</a>, <a href="https://www.linkedin.com/in/iroroorife/">Iroro Orife</a>, <a href="https://www.linkedin.com/in/chih-wei-wu-73081689/">Chih-Wei Wu</a>, <a href="https://www.linkedin.com/in/bhanusrikanth/">Bhanu Srikanth</a></p><p>At Netflix, delivering the best possible experience for our members is at the heart of everything we do, and we know we can’t do it alone. That’s why we work closely with a diverse ecosystem of technology partners, combining their deep expertise with our creative and operational insights. Together, we explore new ideas, develop practical tools, and push technical boundaries in service of storytelling. This collaboration not only empowers the talented creatives working on our shows with better tools to bring their vision to life, but also helps us innovate in service of our members. By building these partnerships on trust, transparency, and shared purpose, we’re able to move faster and more meaningfully, always with the goal of making our stories more immersive, accessible, and enjoyable for audiences everywhere. One area where this collaboration is making a meaningful impact is in improving dialogue intelligibility, from set to screen. We call this the Dialogue Integrity Pipeline.</p><h4>Dialogue Integrity Pipeline</h4><p>We’ve all been there, settling in for a night of entertainment, only to find ourselves straining to catch what was just said on screen. You’re wrapped up in the story, totally invested, when suddenly a key line of dialogue vanishes into thin air. “Wait, what did they say? I can’t understand the dialogue! What just happened?”</p><p>You may pick up the remote and rewind, turn up the volume, or try to stay with it and hope this doesn’t happen again. Creating sophisticated, modern series and films requires an incredible artistic &amp; technical effort. At Netflix, we strive to ensure those great stories are easy for the audience to enjoy. Dialogue intelligibility can break down at multiple points in what we call the <strong>Dialogue Integrity Pipeline</strong>, the journey from on-set capture to final playback at home. Many facets of the process can contribute to dialogue that’s difficult to understand:</p><ul><li>Naturalistic acting styles, diverse speech patterns, and accents</li><li>Noisy locations, microphone placement problems on set</li><li>Cinematic (high dynamic range) mixing styles, excessive dialogue processing, substandard equipment</li><li>Audio compromises through the distribution pipeline</li><li>TVs with inadequate speakers, noisy home environments</li></ul><p>Addressing these issues is critical to maintaining the standard of excellence our content deserves.</p><h4>Measurement at Scale</h4><p>Netflix utilizes industry-standard loudness meters to measure content and its adherence to our core loudness specifications. This tool also provides feedback on audio dynamic range (loud to soft) which impacts dialogue intelligibility. The Audio Algorithms team at Netflix wanted to take these measurements further and develop a holistic understanding of dialogue intelligibility throughout the runtime of a given title.</p><p>The team developed a Speech Intelligibility measurement system based on the Short-time Objective Intelligibility (STOI) metric [<a href="https://www.researchgate.net/profile/Cees-Taal/publication/224219052_An_Algorithm_for_Intelligibility_Prediction_of_Time-Frequency_Weighted_Noisy_Speech/links/0deec51da9fbbc5eea000000/An-Algorithm-for-Intelligibility-Prediction-of-Time-Frequency-Weighted-Noisy-Speech.pdf">Taal et al.</a> (IEEE <em>Transactions on Audio, Speech, and Language Processing</em>)]. Firstly, a speech activity detector analyses the dialogue stem to render speech utterances, which are then compared to non-speech sounds in the mix, typically Music and Effects. Then the system calculates the Signal-to-Noise ratio, in each speech frequency band, the results of which are summarized succinctly, per-utterance on the range [0, 1.0], to quantify the degree to which competing Music and Effects can distract the listener.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*WSViFfuvT8pcZshi" /><figcaption>This chart shows how eSTOI (extended Short-Time Objective Intelligibility) method measures dialogue (fg [foreground] stem in the graphic) against non-speech (bg [background] stem in the graphic) to judge intelligibility based on competing non-speech sound.</figcaption></figure><h4>Optimizing Dialogue Prior to Delivery</h4><p>Understanding dialogue intelligibility across Netflix titles is invaluable, but our mission goes beyond analysis — we strive to empower creators with the tools to craft mixes that resonate seamlessly with audiences at home.</p><p>Seeing the lack of dedicated Dialogue Intelligibility Meter plugins for Digital Audio Workstations, we teamed up with industry leaders, Fraunhofer Institute for Digital Media Technology IDMT (Fraunhofer IDMT) and Nugen Audio to pioneer a solution that enhances creative control and ensures crystal-clear dialogue from mix to final delivery.</p><p>We collaborated with Fraunhofer IDMT to adapt their machine-learning-based speech intelligibility solution for cross-platform plugin standards and brought in Nugen Audio to develop DAW-compatible plugins.</p><h4>Fraunhofer IDMT</h4><p>The Fraunhofer Department of Hearing, Speech, and Audio Technology HSA has done significant research and development on media processing tools that measure speech intelligibility. In 2020, the machine learning-based method was integrated into Steinberg’s Nuendo Digital Audio Workstation. We approached the Fraunhofer engineering team with a collaboration proposal to make their technology accessible to other audio workstations through the cross-platform VST (Virtual Studio Technology) and AAX (Avid Audio Extension) plugin standards. The scientists were keen on the project and provided their dialogue intelligibility library.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*wuapXe2lajcx3tTj" /><figcaption>The Fraunhofer IDMT Dialogue Intelligibility Meter integrated into the Steinberg Nuendo Digital Audio Workstation.</figcaption></figure><h4>Nugen Audio</h4><p>Nugen Audio created the VisLM plugin to provide sound teams with an efficient and accurate way to measure mixes for conformance to traditional broadcast &amp; streaming specifications — Full Mix Loudness, Dialogue Loudness, and True Peak. Since then, VisLM has become a widely used tool throughout the global post-production industry. Nugen Audio partnered with Fraunhofer, integrating the Fraunhofer IDMT Dialogue Intelligibility libraries into a new industry-first tool — Nugen DialogCheck. This tool gives <strong>re-recording mixers</strong> real-time insights, helping them adjust dialogue clarity at the most crucial points in the mixing process, ensuring every word is clear and understood.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gGt-DpKR806J2jqT" /></figure><h4>Clearer Dialogue Through Collaboration</h4><p>Crafting crystal-clear dialogue isn’t just a technical challenge — it’s an art that requires continuous innovation and strong industry collaboration. To empower creators, Netflix and its partners are embedding advanced intelligibility measurement tools directly into DAWs, giving sound teams the ability to:</p><ul><li>Detect and resolve dialogue clarity issues early in the mix.</li><li>Fine-tune speech intelligibility without compromising artistic intent.</li><li>Deliver immersive, accessible storytelling to every viewer, in any listening environment.</li></ul><p>At Netflix, we’re committed to pushing the boundaries of audio excellence, from innovating scaled intelligibility measurements to collaborating with Fraunhofer and Nugen Audio on cutting-edge tools like the DialogCheck Plugin, we’re setting a new standard for dialogue clarity — ensuring every word is heard exactly as creators intended. But innovation doesn’t happen in isolation. By working together with our partners, we can continue to push the limits of what’s possible, fueling creativity and driving the future of storytelling.</p><p>Finally, we’d like to extend a heartfelt thanks to Scott Kramer for his contributions to this initiative.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=58c13d2a6f6e" width="1" /><hr /><p><a href="https://netflixtechblog.com/measuring-dialogue-intelligibility-for-netflix-content-58c13d2a6f6e">Measuring Dialogue Intelligibility for Netflix Content</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/afe6d644a3bc</id>
            <title>How Netflix Accurately Attributes eBPF Flow Logs</title>
            <link>https://netflixtechblog.com/how-netflix-accurately-attributes-ebpf-flow-logs-afe6d644a3bc?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/afe6d644a3bc</guid>
            <pubDate></pubDate>
            <updated>2025-04-09T18:15:11.749Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>By <a href="https://www.linkedin.com/in/chengxie90/">Cheng Xie</a>, <a href="https://www.linkedin.com/in/bryan-shultz-85983829/">Bryan Shultz</a>, and <a href="https://www.linkedin.com/in/christine-xu-1b77191b/">Christine Xu</a></p><p>In a previous <a href="https://netflixtechblog.com/how-netflix-uses-ebpf-flow-logs-at-scale-for-network-insight-e3ea997dca96">blog post</a>, we described how Netflix uses eBPF to capture TCP flow logs at scale for enhanced cloud network insights. In this post, we delve deeper into how Netflix solved a core problem: accurately attributing flow IP addresses to workload identities.</p><h3>A Brief Recap</h3><p><strong>FlowExporter</strong> is a sidecar that runs alongside all Netflix workloads in the AWS Cloud. It uses eBPF and <a href="https://www.brendangregg.com/blog/2018-03-22/tcp-tracepoints.html">TCP tracepoints</a> to monitor TCP socket state changes. When a TCP socket closes, FlowExporter generates a flow log record that includes the IP addresses, ports, timestamps, and additional socket statistics. On average, 5 million records are produced per second.</p><p>In cloud environments, IP addresses are reassigned to different workloads as workload instances are created and terminated, so IP addresses alone cannot provide insights on which workloads are communicating. To make the flow logs useful, each IP address must be attributed to its corresponding workload identity. <strong>FlowCollector</strong>, a backend service, collects flow logs from FlowExporter instances across the fleet, attributes the IP addresses, and sends these attributed flows to Netflix’s <a href="https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873">Data Mesh</a> for subsequent stream and batch processing.</p><p>The eBPF flow logs provide a comprehensive view of service topology and network health across Netflix’s extensive microservices fleet, regardless of the programming language, RPC mechanism, or application-layer protocol used by individual workloads. This is especially useful for reaching the corners where our <a href="https://netflixtechblog.com/zero-configuration-service-mesh-with-on-demand-cluster-discovery-ac6483b52a51">Service Mesh</a> does not yet have coverage.</p><h3>The Problem with Misattribution</h3><p>Accurately attributing flow IP addresses to workload identities has been a significant challenge since our eBPF flow logs were introduced.</p><p>As noted in our previous blog post, our initial attribution approach relied on <a href="https://youtu.be/8C9xNVYbCVk?si=Mqic7typcyB-v3JR&amp;t=1687">Sonar</a>, an internal IP address tracking service that emits an event whenever an IP address in Netflix’s AWS VPCs is assigned or unassigned to a workload. FlowCollector consumes a stream of IP address change events from Sonar and uses this information to attribute flow IP addresses in real-time.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*QIn-JibEFM2CLans" /></figure><p>The fundamental drawback of this method is that it can lead to misattribution. Delays and failures are inevitable in distributed systems, which may delay IP address change events from reaching FlowCollector. For instance, an IP address may initially be assigned to workload X but later reassigned to workload Y. However, if the change event for this reassignment is delayed, FlowCollector will continue to assume that the IP address belongs to workload X, resulting in misattributed flows. Additionally, event timestamps may be inaccurate depending on how they are captured.</p><p>Misattribution rendered the flow data unreliable for decision-making. Users often depend on flow logs to validate workload dependencies, but misattribution creates confusion. Without expert knowledge of expected dependencies, users would struggle to identify or confirm misattribution. Moreover, misattribution occurred frequently for critical services with a large footprint due to frequent IP address changes. Overall, misattribution makes fleet-wide dependency analysis impractical.</p><p>As a workaround, we made FlowCollector hold received flows for 15 minutes before attribution, allowing time for delayed IP address change events. While this approach reduced misattribution, it did not eliminate it. Moreover, the waiting period made the data less fresh, reducing its utility for real-time analysis.</p><p>Fully eliminating misattribution is crucial because it only takes a single misattributed flow to produce an incorrect workload dependency. Solving this problem required a complete rethinking of our approach. Over the past year, Netflix developed a new attribution method that has finally eliminated misattribution, as detailed in the rest of this post.</p><h3>Attributing Local IP Addresses</h3><p>Each socket has two IP addresses: a local IP address and a remote IP address. Previously, we used the same method to attribute both. However, attributing the local IP address should be a simpler task since the local IP address belongs to the instance where FlowExporter captures the socket. Therefore, FlowExporter should determine the local workload identity from its environment and attribute the local IP address before sending the flow to FlowCollector.</p><p>This is straightforward for workloads running directly on EC2 instances, as Netflix’s <a href="https://www.youtube.com/watch?v=-mmOT9I6JlY">Metatron</a> provisions workload identity certificates to each EC2 instance at boot time. FlowExporter can simply read these certificates from the local disk to determine the local workload identity.</p><p>Attributing local IP addresses for container workloads running on Netflix’s container platform, <a href="https://netflixtechblog.com/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436">Titus</a>, is more challenging. FlowExporter runs at the container host level, where each host manages multiple container workloads with different identities. When FlowExporter’s eBPF programs receive a socket event from TCP tracepoints in the kernel, the socket may have been created by one of the container workloads or by the host itself. Therefore, FlowExporter must determine which workload to attribute the socket’s local IP address to. To solve this problem, we leveraged <a href="https://www.youtube.com/watch?v=fmUM9bMoCNE">IPMan</a>, Netflix’s container IP address assignment service. IPManAgent, a daemon running on every container host, is responsible for assigning and unassigning IP addresses. As container workloads are launched, IPManAgent writes an IP-address-to-workload-ID mapping to an eBPF map, which FlowExporter’s eBPF programs can then use to look up the workload ID associated with a socket local IP address.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fyfZ6m2NrMq1NgRQ" /></figure><p>Another challenge was to accommodate Netflix’s <a href="https://lpc.events/event/11/contributions/932/attachments/908/1764/LPC%202021_%20Talking%20IPv6%20to%20IPv4%20Without%20NAT_2.pdf">IPv6 to IPv4 translation mechanism</a> on Titus. To facilitate IPv6 migration, Netflix developed a mechanism that enables IPv6-only containers to communicate with IPv4 destinations without incurring NAT64 overhead. This mechanism intercepts connect syscalls and replaces the underlying socket with one that uses a shared IPv4 address assigned to the container host. This confuses FlowExporter because the kernel reports the same local IPv4 address for sockets created by different container workloads. To disambiguate, local port information is additionally required. We modified Titus to write a mapping of (local IPv4 address, local port) to the workload ID into an eBPF map whenever a connect syscall is intercepted. FlowExporter’s eBPF programs then use this map to correctly attribute sockets created by the translation mechanism.</p><p>With these problems solved, we can now accurately attribute the local IP address of every flow.</p><h3>Attributing Remote IP Addresses</h3><p>Once the local IP address attribution problem is solved, accurately attributing remote IP addresses becomes feasible. Now, each flow reported by FlowExporter includes the local IP address, the local workload identity, and connection start/end timestamps. As FlowCollector receives these flows, it can learn the time ranges during which each workload owns a given IP address. For instance, if FlowCollector sees a flow with local IP address 10.0.0.1 associated with workload X that starts at t1 and ends at t2, it can deduce that 10.0.0.1 belonged to workload X from t1 to t2. Since Netflix uses <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/set-time.html">Amazon Time Sync</a> across its fleet, the timestamps (captured by FlowExporter) are reliable.</p><p>The FlowCollector service cluster consists of many nodes. Every node must be capable of attributing arbitrary remote IP addresses and, therefore, requires knowledge of all workload IP addresses and their recent ownership records. To represent this knowledge, each node maintains an in-memory hashmap that maps an IP address to a list of time ranges, as illustrated by the following Go structs:</p><pre>type IPAddressTracker struct {<br />    ipToTimeRanges map[netip.Addr]timeRanges<br />}<br /><br />type timeRanges []timeRange<br /><br />type timeRange struct {<br />    workloadID   string<br />    start        time.Time<br />    end          time.Time<br />}</pre><p>To populate the hashmap, FlowCollector extracts the local IP address, local workload identity, start time, and end time from each received flow and creates/extends the corresponding time ranges in the map. The time ranges for each IP address are sorted in ascending order, and they are non-overlapping since an IP address cannot belong to two different workloads simultaneously.</p><p>Since each flow is only sent to one FlowCollector node, each node must share the time ranges it learned from received flows with other nodes. We implemented a broadcasting mechanism using Kafka, where each node publishes learned time ranges to all other nodes. Although more efficient broadcasting implementations exist, the Kafka-based approach is simple and has worked well for us.</p><p>Now, FlowCollector can attribute remote IP addresses by looking them up in the populated map, which returns a list of time ranges. It then uses the flow’s start timestamp to determine the corresponding time range and associated workload identity. If the start time does not fall within any time range, FlowCollector will retry after a delay, eventually giving up if the retry fails. Such failures may occur when flows are lost or broadcast messages are delayed. For our use cases, it is acceptable to leave a small percentage of flows unattributed, but any misattribution is unacceptable.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*o8tJzaxRlWDBIYBS" /></figure><p>This new method achieves accurate attribution thanks to the continuous heartbeats, each associated with a reliable time range of IP address ownership. It handles transient issues gracefully — a few delayed or lost heartbeats do not lead to misattribution. In contrast, the previous method relied solely on discrete IP address assignment and unassignment events. Lacking heartbeats, it had to presume an IP address remained assigned until notified otherwise (which can be hours or days later), making it vulnerable to misattribution when the notifications were delayed.</p><p>One detail is that when FlowCollector receives a flow, it cannot attribute its remote IP address right away because it requires the latest observed time ranges for the remote IP address. Since FlowExporter reports flows in batches every minute, FlowCollector must wait until it receives the flow batch from the remote workload FlowExporter for the last minute, which may not have arrived yet. To address this, FlowCollector temporarily stores received flows on disk for one minute before attributing their remote IP addresses. This introduces a 1-minute delay, but it is much shorter than the 15-minute delay with the previous approach.</p><p>In addition to producing accurate attribution, the new method is also cost-effective thanks to its simplicity and in-memory lookups. Because the in-memory state can be quickly rebuilt when a FlowCollector node starts up, no persistent storage is required. With 30 c7i.2xlarge instances, we can process 5 million flows per second for the entire Netflix fleet.</p><h3>Attributing Cross-Regional IP Addresses</h3><p>For simplicity, we have so far glossed over one topic: regionalization. Netflix’s cloud microservices operate across multiple AWS regions. To optimize flow reporting and minimize cross-regional traffic, a FlowCollector cluster runs in each major region, and FlowExporter agents send flows to their corresponding regional FlowCollector. When FlowCollector receives a flow, its local IP address is guaranteed to be within the region.</p><p>To minimize cross-region traffic, the broadcasting mechanism is limited to FlowCollector nodes within the same region. Consequently, the IP address time ranges map contains only IP addresses from that region. However, cross-regional flows have a remote IP address in a different region. To attribute these flows, the receiving FlowCollector node forwards them to nodes in the corresponding region. FlowCollector determines the region for a remote IP address by looking up a trie built from all Netflix VPC CIDRs. This approach is more efficient than broadcasting IP address time range updates across all regions, as only 1% of Netflix flows are cross-regional.</p><h3>Attributing Non-Workload IP Addresses</h3><p>So far, FlowCollector can accurately attribute IP addresses belonging to Netflix’s cloud workloads. However, not all flow IP addresses fall into this category. For instance, a significant portion of flows goes through AWS ELBs. For these flows, their remote IP addresses are associated with the ELBs, where we cannot run FlowExporter. Consequently, FlowCollector cannot determine their identities by simply observing the received flows. To attribute these remote IP addresses, we continue to use IP address change events from Sonar, which crawls AWS resources to detect changes in IP address assignments. Although this data stream may contain inaccurate timestamps and be delayed, misattribution is not a main concern since ELB IP address reassignment occurs very infrequently.</p><h3>Verifying Correctness</h3><p>Verifying that the new method has eliminated misattribution is challenging due to the lack of a definitive source of truth for workload dependencies to validate flow logs against; the flow logs themselves are intended to serve as this source of truth, after all. To build confidence, we analyzed the flow logs of a large service with well-understood dependencies. A large footprint is necessary, as misattribution is more prevalent in services with numerous instances, and there must be a reliable method to determine the dependencies for this service without relying on flow logs.</p><p>Netflix’s cloud gateway, <a href="https://github.com/Netflix/zuul">Zuul</a>, served this purpose perfectly due to its extensive footprint (handling all cloud ingress traffic), its large number of downstream dependencies, and our ability to derive its dependencies from its routing configurations as the source of truth for comparison with flow logs. We found no misattribution for flows through Zuul over a two-week window. This provided strong confidence that the new attribution method has eliminated misattribution. In the previous approach, approximately 40% of Zuul’s dependencies reported by the flow logs were misattributed.</p><h3>Conclusion</h3><p>With misattribution solved, eBPF flow logs now deliver dependable, fleet-wide insights into Netflix’s service topology and network health. This advancement unlocks numerous exciting opportunities in areas such as service dependency auditing, security analysis, and incident triage, while helping Netflix engineers develop a better understanding of our ever-evolving distributed systems.</p><h3>Acknowledgments</h3><p>We would like to thank <a href="https://www.linkedin.com/in/mdubcovsky/">Martin Dubcovsky</a>, <a href="https://www.linkedin.com/in/joannekoong/">Joanne Koong</a>, <a href="https://www.linkedin.com/in/troshko/">Taras Roshko</a>, <a href="https://www.linkedin.com/in/nabilschear/">Nabil Schear</a>, <a href="https://www.linkedin.com/in/jacobmeyers35/">Jacob Meyers</a>, <a href="https://www.linkedin.com/in/parshap/">Parsha Pourkhomami</a>, <a href="https://www.linkedin.com/in/hechaoli/">Hechao Li</a>, <a href="https://www.linkedin.com/in/donavanfritz/">Donavan Fritz</a>, <a href="https://www.linkedin.com/in/rob-gulewich-0335b52/">Rob Gulewich</a>, <a href="https://www.linkedin.com/in/amanda-li-410286166/">Amanda Li</a>, <a href="https://www.linkedin.com/in/jdsalem/">John Salem</a>, <a href="https://www.linkedin.com/in/haananth/">Hariharan Ananthakrishnan</a>, <a href="https://www.linkedin.com/in/joshmachine/">Keerti Lakshminarayan</a>, and other stunning colleagues for their feedback, inspiration, and contributions to the success of this effort.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=afe6d644a3bc" width="1" /><hr /><p><a href="https://netflixtechblog.com/how-netflix-accurately-attributes-ebpf-flow-logs-afe6d644a3bc">How Netflix Accurately Attributes eBPF Flow Logs</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/fc3c108c0a22</id>
            <title>Globalizing Productions with Netflix’s Media Production Suite</title>
            <link>https://netflixtechblog.com/globalizing-productions-with-netflixs-media-production-suite-fc3c108c0a22?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/fc3c108c0a22</guid>
            <pubDate></pubDate>
            <updated>2025-03-31T16:02:04.810Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://www.linkedin.com/in/jesse-korosi-44790985/"><strong>Jesse Korosi</strong></a>,<strong> </strong><a href="https://www.linkedin.com/in/thijsvdkamp/"><strong>Thijs van de Kamp</strong></a><strong>, </strong><a href="https://www.linkedin.com/in/mayralvega/"><strong>Mayra Vega</strong></a>,<strong> </strong><a href="https://www.linkedin.com/in/laurafuturo/"><strong>Laura Futuro</strong></a>,<strong> </strong><a href="https://www.linkedin.com/in/margoline/"><strong>Anton Margoline</strong></a></p><p>The journey from script to screen is full of challenges in the ever-evolving world of film and television. The industry has always innovated, and over the last decade, it started moving towards cloud-based workflows. However, unlocking cloud innovation and all its benefits on a global scale has proven to be difficult. The opportunity is clear: streamline complex media management logistics, eliminate tedious, non-creative task-based work and enable productions to focus on what matters most–creative storytelling. With these challenges in mind, Netflix has developed a suite of tools by filmmakers for filmmakers: the Media Production Suite (MPS).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CUGxiNprXnLcOmhI" /></figure><h3><strong>What are we solving for?</strong></h3><p>Significant time and resources are devoted to managing media logistics throughout the production lifecycle. An average Netflix title produces around ~200 Terabytes of Original Camera Files (OCF), with outliers up to 700 Terabytes, not including any work-in-progress files, VFX assets, 3D assets, etc. The data produced on set is traditionally copied to physical tape stock like LTO. This workflow has been considered the industry norm for a long time and may be cost-effective, but comes with trade-offs. Aside from needing to physically ship and track all movement of the tape stock, storing media on a physical tape makes it harder to search, play and share media assets; slowing down accessibility to production media when needed, especially when titles need to collaborate with talent and vendors all over the world.</p><p>Even when workflows are fully digital, the distribution of media between multiple departments and vendors can still be challenging. A lack of automation and standardization often results in a labour-intensive process across post-production and VFX with a lot of dependencies that introduce potential human errors and security risks. Many productions utilize a large variety of vendors, making this collaboration a large technical puzzle. As file sizes grow and workflows become more complex, these issues are magnified, leading to inefficiencies that slow down post-production and reduce the available time spent on creative work.</p><p>Moving media into the cloud introduces new challenges for production and post ramping up to meet the operational and technological hurdles this poses. For some post-production facilities, it’s not uncommon to see a wall of portable hard drives at their facility, with media being hand-carried between vendors because alternatives are not available. The need for a centralized, cloud-based solution that transcends these barriers is more pressing than ever. This results in a willingness to embrace new and innovative ideas, even if exploratory, and introduce drastic workflow changes to productions in pursuit of creative evolution.</p><p>At Netflix, we believe that great stories can come from anywhere, but we have seen that technical limitations in traditional workflows reduce access to media and restrict filmmakers’ access to talent. Besides the need for robust cloud storage for their media, artists need access to powerful workstations and real-time playback. Depending on the market, or production budget, cutting-edge technology might not be available or affordable.</p><p>What if we started charting a course to break free from many of these technical limitations and found ways to enhance creativity? Industry trade shows like the International Broadcast Convention (IBC) and the National Association of Broadcasters Show (NAB) highlight a strong global trend: instead of bringing media to the artist/applications (traditional workflow) we see the shift towards bringing people and applications to the media (cloud workflows and remote workstations). The concept of cloud-based workflows is not new, as many technology leaders in our industry have been experimenting in this space for more than a decade. However, executing this vision at a Netflix scale with hundreds of titles a year has not been done before…</p><h3><strong>The challenge of building a global technology to solve this</strong></h3><p>Building solutions at a global scale poses significant challenges. The art of making movies and series lacks equal access to technology, best practices, and global standardization. Different countries worldwide are at different phases of innovation based on local needs and nuances. While some regions boast over a century of cinematic history and have a strong industry, others are just beginning to carve their niche. This vast gap presents a unique challenge: developing global technology that caters to both established and emerging markets, each with distinct languages and workflows.</p><p>The large diversity of needs by talent and vendors globally creates a standardization challenge and can be seen when productions use a global talent pool. Many mature post-production and VFX facilities have built scripts and automation that flow between various artists and personnel within their facility; allowing a more streamlined workflow, even though the customization is time-consuming. E.g., Transcoding, or transcriptions that automatically run when files are dropped in a hot folder, with the expectation that certain sidecar metadata files will accompany them with a specific organizational structure. Embracing and integrating new workflows introduces the fear of disrupting a well-established process, increasing additional pressure on the profit margins of vendors. Small workflow changes that may seem arbitrary may actually have a large impact on vendors. Therefore, innovation should provide meaningful benefits to a title in order to get adopted at scale. Reliability, a proven track record, strong support, and an incredibly low tolerance for bugs, or issues are top of mind in well-established markets.</p><p>In developing this suite, we recognized the necessity of addressing the vast array of titles that flow through Netflix without the luxury of expanding into a massive operational entity. Consequently, automation became imperative. The intricacies of color and framing management, along with deliverables, must be seamlessly controlled and effortlessly managed by the user, without the need for manual intervention. Therefore, we cannot lean into humans configuring JSON files behind the scenes to map camera formats into deliverables. By embracing open standards, we not only streamline these processes but also facilitate smoother collaboration across diverse markets and countries, ensuring that our global productions can operate with unparalleled efficiency and cohesion. To ensure this, we’ve decided to lean heavily into standards like <a href="https://www.oscars.org/science-technology/sci-tech-projects/aces">ACES</a>, <a href="https://acescentral.com/knowledge-base-2/when-is-amf-used/">AMF</a>, <a href="https://theasc.com/society/ascmitc/asc-media-hash-list">ASC MHL</a>, <a href="https://theasc.com/society/ascmitc/asc-framing-decision-list">ASC FDL</a>, and <a href="https://github.com/OpenTimelineIO">OTIO</a>. ACES and AMF for color pipeline management. ASC MHL for any file management/verifications. ASC FDL will serve as our framing interoperability and OTIO for any timeline interchange. Leaning into standards like this means that many things can be automated at scale and more importantly, high-complexity workflows can be offered to markets or shows that don’t normally have access to them. As an example, if a show is shot on various camera formats all framed and recorded at different resolutions, with different lenses and different safeties on each frame. The task of normalizing all of these for a VFX vendor into one common container with a normalized center extracted frame is often only offered to very high-end titles, considering it takes a human behind the curtain to create all of these mappings. But by leaning into a standard like the FDL, it means this can now easily be automated, and the control for these mappings, put directly in the hands of users.</p><h3><strong>Our Answer — Content Hub’s Media Production Suite (MPS)</strong></h3><a href="https://medium.com/media/20f44e6583c1fc3db670399bdd288444/href">https://medium.com/media/20f44e6583c1fc3db670399bdd288444/href</a><p>Building a global scalable solution that could be utilized in a diversity of markets has been an exciting challenge. We set out to provide customizable and feature-rich tooling for advanced users while remaining intuitive and streamlined enough for less experienced filmmakers. With collaboration from Netflix teams, vendors, and talent across the globe, we’ve taken a bold step forward in enabling a suite of tools inside Netflix Content Hub that democratizes technology: the Media Production Suite. While leveraging our scale economies and access to resources, we can now unlock global talent pools for our productions, drastically reduce non-creative task-based work, streamline workflows, and level the playing field between our markets, ultimately maximizing the time available for what matters most; creative work!</p><h3>So what is it?</h3><p>1. <strong>Netflix Hybrid Infrastructure</strong>: Netflix has invested in a hybrid infrastructure, a mix of cloud-based and physically distributed capabilities operating in multiple locations across the world and close to our productions to optimize user performance. This infrastructure is available for Netflix shows and is foundational under Content Hub’s Media Production Suite tooling. Local storage and compute services are connected through the Netflix Open Connect network (Netflix Content Delivery Network) to the infrastructure of Amazon Web Services (AWS). The system facilitates large volumes of camera and sound media and is built for speed. In order to ensure that productions have sufficient upload speeds to get their media into the cloud, Netflix has started to roll out Content Hub Ingest Centers globally to provide high-speed internet connectivity where required. With all media centralized, MPS eliminates the need for physical media transport and reduces the risk of human error. This approach not only streamlines operations but also enhances security and accessibility.</p><p>2. <strong>Automation and Tooling</strong>: In addition to the Netflix Hybrid infrastructure layer, MPS consists of a suite of tools that tap into the media in the Netflix ecosystem.</p><p><strong>Footage Ingest</strong> — An application that allows users to upload media/files into Content Hub.</p><p><strong>Media Library</strong> — A central library that allows users to search, preview, share and download media.</p><p><strong>Dailies</strong> — A workflow, backed by an operational team, offering automated Quality Control of your footage, sound sync, application of color, rendering, and delivering dailies directly to editorial.</p><p><strong>Remote Workstations</strong> — Offering access to remote editorial workstations and storage for post-production needs.</p><p><strong>VFX Pulls</strong> — An automated method for converting and delivering visual effects plates, associated color, and framing files to VFX vendors.</p><p><strong>Conform Pulls</strong> — An automated method for consolidating, trimming, and delivering all OCF to picture-finishing vendors.</p><p><strong>Media Downloader</strong> — An automated download tool that initiates a download once media has been made available in the Netflix cloud.</p><p>While each of the individual tools within MPS is at different states of maturity, over 350 titles have made use of at least one of the tools noted above. Input has been taken from all over the world while developing, with users ranging from UCAN (United States/Canada), EMEA (Europe, Middle East, and Africa), SEA (South East Asia), LATAM (Latin America), and APAC (Asia Pacific).</p><h3><strong>Senna: Early Adoption and Insightful Feedback Driving MPS Evolution</strong></h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*NLeFw4FDGx2jsZg7" /><figcaption><em>Media from the Brazilian-produced series ‘Senna’ being reviewed in MPS</em></figcaption></figure><p>The Brazilian-produced series <em>Senna</em>, which follows the life of legendary Formula 1 driver Ayrton Senna, utilized MPS to reshape their content creation workflow, overcome geographical barriers, and unlock innovation to support world-class storytelling for a global audience. <em>Senna</em> is a groundbreaking series, not just for its storytelling but for its production journey across Argentina, Uruguay, Brazil, and the United Kingdom. With editorial teams spread across Porto Alegre and Spain, and VFX studios collaborating across locations in Brazil, Canada, the United States, and India, all orchestrated by our subsidiary Scanline VFX. The series exemplifies the global nature of modern filmmaking and was the perfect fit for Netflix’s new Content Hub Media Production Suite (MPS).</p><p>At the heart of <em>Senna’s</em> workflow orchestration is MPS. While each of the tools within MPS is based on an opt-in model, in order to use many of the downstream services, the first step is ensuring that the original camera files (OCF) and original sound files (OSF) are uploaded. “<em>We knew we were going to shoot in different places,</em>” said Post Supervisor Gabriel Queiroz,<em>“to have all this material cloud-based, it’s definitely one of the most important things for us. It would be hard to bring all this media physically from Argentina or wherever to Brazil. It will take us a lot of time.”</em> With <em>Senna</em> shooting across locations, allowing production the capability of uploading their OCF and OSF resulted in no longer requiring shuttling hard drives on airplanes, creating LTO tapes, &amp; managing physical shipments for their negative. And yes, you read that correctly; when utilizing MPS, we don’t require LTO tapes to be written unless there are title-specific needs.</p><p>With <em>Senna</em> beginning production back in June of 2023, our investment in MPS was still very early stages, and the tooling was considered beta. However, with the help, feedback, and partnership from this production, it was quickly realized that the investment was worth doubling down on. Since the early version used on <em>Senna</em>, Netflix has been spinning up ingest centers around the world, where drives can be dropped off, and within a matter of hours, all original camera files are uploaded into the Netflix ecosystem. While creating the ability to upload is not a novel concept, behind the scenes, it’s far from simple. Once a drive has been plugged in and our Netflix Footage Ingest application is opened, a validation is run, ensuring all expected media from set is on the drive. After media has been uploaded and checksums are run validating media integrity, all media is inspected, metadata is extracted, and assets are created for viewing/sharing/downloading with playable proxies. All media is then automatically backed up to a second tier of cloud-based storage for the final archive.</p><p>Traditionally, if you wanted to check in with your post vendor on how things are going for each of these media management steps noted above, or whether or not you can clear on set camera cards if you haven’t gotten a completion notification, you would have to pick up the phone and call your vendor. For <em>Senna</em>, anyone who wanted visibility on progress, simply logged in to Content Hub and could see any activity in the Footage Ingest dashboard, as well as look up any information needed on past uploads.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*7gaSH8-YpnOTKnpu" /><figcaption><em>Remote monitoring media being uploaded and archived using the MPS Footage Ingest workflow</em></figcaption></figure><p>While many services in MPS are available once media has been uploaded, <em>Senna’s</em> use of MPS focused on VFX. With <em>Senna</em> shooting a high volume of footage and the show having a high volume of VFX shots, according to Post Supervisor Gabriel Queiroz <em>“Using MPS was basically a no-brainer, </em>[having]<em> used the tool before, I knew what it could bring to the project. And to be honest, with the amount of footage that we have, it was just so much material and with the amount of vendors we have, knowing that we would have to deliver all this footage to all these kinds of vendors, including outside of Brazil and to different parts of the world.”</em></p><p>With a traditional workflow, utilizing available resources in Latin America, VFX Pulls would have been done manually. This process is prone to human error and more importantly, for a show like <em>Senna</em>, too slow and would have resulted in different I/O methods for every vendor.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*_CzN6mkamROqqxjo" /><figcaption><em>Illustrating a traditional VFX Editor having to manage various I/O methods</em></figcaption></figure><p>By utilizing MPS, the Assistant Editor was able to log into Content Hub, upload an EDL, and have their VFX Pulls automatically transcoded, color files consolidated and all media placed into a Google Drive style folder built directly in Content Hub (called Workspaces). The VFX Editor was able to make any additional tweaks they wanted to the directory before farming out each of the shots to whichever vendor they were meant for. When it came time for the VFX vendors to then send shots back to editorial or DI, this was also done through MPS. Having one standard method for I/O for all VFX file sharing meant that Editorial and DI did not have to manage a different file transfer/workflow for every single vendor that was onboarded.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*V01AyZdu0si2N0z5" /><figcaption><em>Illustrating a more streamlined workflow for VFX vendors when using MPS</em></figcaption></figure><p>After picture was locked and it was time for <em>Senna</em> to do their Online, the DI facility Quanta was able to utilize the Conform Pull service within MPS. The Conform Pull service allowed their team to upload an EDL, which ran a QC on all of the media from within their edit to ensure a smooth conform and then consolidated, trimmed, and packaged up all of the media they needed for the online. Since this early beta and thanks to learnings from many shows like Senna, advancements have been made in the system’s ability to match back to source media for both Conform and VFX Pulls. Rather than requiring an exact match between EDL and source OCF, there are several variations of fuzzy matching that can take place, as well as a current investigation in using one of our perceptual matching algorithms, allowing for a perceptual conform using computer vision, instead of solely relying on metadata.</p><a href="https://medium.com/media/894002e879e7fa87d8223112d1afd762/href">https://medium.com/media/894002e879e7fa87d8223112d1afd762/href</a><h3>Conclusion</h3><p>The Media Production Suite (MPS) represents a transformative leap in how we approach media production at Netflix. By embracing open standards, we have crafted a scalable solution that not only makes economic sense but also democratizes access to advanced production tools across the globe. This approach allows us to eliminate tedious tasks, enabling our teams to focus on what truly matters: creative storytelling. By fostering global collaboration and leveraging the power of cloud-based workflows, we’re not just enhancing efficiency but also elevating the quality of our productions. As we continue to innovate and refine our processes, we remain committed to breaking down barriers and unlocking the full potential of creative talent worldwide. The future of filmmaking is here, and with MPS, we are leading the charge toward a more connected and creatively empowered industry.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fc3c108c0a22" width="1" /><hr /><p><a href="https://netflixtechblog.com/globalizing-productions-with-netflixs-media-production-suite-fc3c108c0a22">Globalizing Productions with Netflix’s Media Production Suite</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/1a0bd8e02d39</id>
            <title>Foundation Model for Personalized Recommendation</title>
            <link>https://netflixtechblog.com/foundation-model-for-personalized-recommendation-1a0bd8e02d39?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/1a0bd8e02d39</guid>
            <pubDate></pubDate>
            <updated>2025-03-21T19:41:34.464Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>By <a href="https://www.linkedin.com/in/markhsiao/">Ko-Jen Hsiao</a>, <a href="https://www.linkedin.com/in/yesufeng/">Yesu Feng</a> and <a href="https://www.linkedin.com/in/sudarshanlamkhede/">Sudarshan Lamkhede</a></p><h3>Motivation</h3><p>Netflix’s personalized recommender system is a complex system, boasting a variety of specialized machine learned models each catering to distinct needs including “Continue Watching” and “Today’s Top Picks for You.” (Refer to our recent <a href="https://videorecsys.com/slides/mark_talk3.pdf">overview</a> for more details). However, as we expanded our set of personalization algorithms to meet increasing business needs, maintenance of the recommender system became quite costly. Furthermore, it was difficult to transfer innovations from one model to another, given that most are independently trained despite using common data sources. This scenario underscored the need for a new recommender system architecture where member preference learning is centralized, enhancing accessibility and utility across different models.</p><p>Particularly, these models predominantly extract features from members’ recent interaction histories on the platform. Yet, many are confined to a brief temporal window due to constraints in serving latency or training costs. This limitation has inspired us to develop a foundation model for recommendation. This model aims to assimilate information both from members’ comprehensive interaction histories and our content at a very large scale. It facilitates the distribution of these learnings to other models, either through shared model weights for fine tuning or directly through embeddings.</p><p>The impetus for constructing a foundational recommendation model is based on the paradigm shift in natural language processing (NLP) to large language models (LLMs). In NLP, the trend is moving away from numerous small, specialized models towards a single, large language model that can perform a variety of tasks either directly or with minimal fine-tuning. Key insights from this shift include:</p><ol><li><strong>A Data-Centric Approach</strong>: Shifting focus from model-centric strategies, which heavily rely on feature engineering, to a data-centric one. This approach prioritizes the accumulation of large-scale, high-quality data and, where feasible, aims for end-to-end learning.</li><li><strong>Leveraging Semi-Supervised Learning</strong>: The next-token prediction objective in LLMs has proven remarkably effective. It enables large-scale semi-supervised learning using unlabeled data while also equipping the model with a surprisingly deep understanding of world knowledge.</li></ol><p>These insights have shaped the design of our foundation model, enabling a transition from maintaining numerous small, specialized models to building a scalable, efficient system. By scaling up semi-supervised training data and model parameters, we aim to develop a model that not only meets current needs but also adapts dynamically to evolving demands, ensuring sustainable innovation and resource efficiency.</p><h3>Data</h3><p>At Netflix, user engagement spans a wide spectrum, from casual browsing to committed movie watching. With over 300 million users at the end of 2024, this translates into hundreds of billions of interactions — an immense dataset comparable in scale to the token volume of large language models (LLMs). However, as in LLMs, the quality of data often outweighs its sheer volume. To harness this data effectively, we employ a process of interaction tokenization, ensuring meaningful events are identified and redundancies are minimized.</p><p><strong>Tokenizing User Interactions</strong>: Not all raw user actions contribute equally to understanding preferences. Tokenization helps define what constitutes a meaningful “token” in a sequence. Drawing an analogy to Byte Pair Encoding (BPE) in NLP, we can think of tokenization as merging adjacent actions to form new, higher-level tokens. However, unlike language tokenization, creating these new tokens requires careful consideration of what information to retain. For instance, the total watch duration might need to be summed or engagement types aggregated to preserve critical details.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*1dhdoLxKnf_fcZOq" /><figcaption><strong>Figure 1.</strong>Tokenization of user interaction history by merging actions on the same title, preserving important information.</figcaption></figure><p>This tradeoff between granular data and sequence compression is akin to the balance in LLMs between vocabulary size and context window. In our case, the goal is to balance the length of interaction history against the level of detail retained in individual tokens. Overly lossy tokenization risks losing valuable signals, while too granular a sequence can exceed practical limits on processing time and memory.</p><p>Even with such strategies, interaction histories from active users can span thousands of events, exceeding the capacity of transformer models with standard self attention layers. In recommendation systems, context windows during inference are often limited to hundreds of events — not due to model capability but because these services typically require millisecond-level latency. This constraint is more stringent than what is typical in LLM applications, where longer inference times (seconds) are more tolerable.</p><p>To address this during training, we implement two key solutions:</p><ol><li><strong>Sparse Attention Mechanisms</strong>: By leveraging sparse attention techniques such as low-rank compression, the model can extend its context window to several hundred events while maintaining computational efficiency. This enables it to process more extensive interaction histories and derive richer insights into long-term preferences.</li><li><a href="https://arxiv.org/abs/2409.14517"><strong>Sliding Window Sampling</strong></a>: During training, we sample overlapping windows of interactions from the full sequence. This ensures the model is exposed to different segments of the user’s history over multiple epochs, allowing it to learn from the entire sequence without requiring an impractically large context window.</li></ol><p>At inference time, when multi-step decoding is needed, we can deploy KV caching to efficiently reuse past computations and maintain low latency.</p><p>These approaches collectively allow us to balance the need for detailed, long-term interaction modeling with the practical constraints of model training and inference, enhancing both the precision and scalability of our recommendation system.</p><p><strong>Information in Each ‘Token’</strong>: While the first part of our tokenization process focuses on structuring sequences of interactions, the next critical step is defining the rich information contained within each token. Unlike LLMs, which typically rely on a single embedding space to represent input tokens, our interaction events are packed with heterogeneous details. These include attributes of the action itself (such as locale, time, duration, and device type) as well as information about the content (such as item ID and metadata like genre and release country). Most of these features, especially categorical ones, are directly embedded within the model, embracing an end-to-end learning approach. However, certain features require special attention. For example, timestamps need additional processing to capture both absolute and relative notions of time, with absolute time being particularly important for understanding time-sensitive behaviors.</p><p>To enhance prediction accuracy in sequential recommendation systems, we organize token features into two categories:</p><ol><li><strong>Request-Time Features</strong>: These are features available at the moment of prediction, such as log-in time, device, or location.</li><li><strong>Post-Action Features</strong>: These are details available after an interaction has occurred, such as the specific show interacted with or the duration of the interaction.</li></ol><p>To predict the next interaction, we combine request-time features from the current step with post-action features from the <a href="https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/18140">previous step</a>. This blending of contextual and historical information ensures each token in the sequence carries a comprehensive representation, capturing both the immediate context and user behavior patterns over time.</p><h3>Considerations for Model Objective and Architecture</h3><p>As previously mentioned, our default approach employs the autoregressive next-token prediction objective, similar to GPT. This strategy effectively leverages the vast scale of unlabeled user interaction data. The adoption of this objective in recommendation systems has shown multiple successes [1–3]. However, given the distinct differences between language tasks and recommendation tasks, we have made several critical modifications to the objective.</p><p>Firstly, during the pretraining phase of typical LLMs, such as GPT, every target token is generally treated with equal weight. In contrast, in our model, not all user interactions are of equal importance. For instance, a 5-minute trailer play should not carry the same weight as a 2-hour full movie watch. A greater challenge arises when trying to align long-term user satisfaction with specific interactions and recommendations. To address this, we can adopt a multi-token prediction objective during training, where the model predicts the next <em>n</em> tokens at each step instead of a single token[4]. This approach encourages the model to capture longer-term dependencies and avoid myopic predictions focused solely on immediate next events.</p><p>Secondly, we can use multiple fields in our input data as auxiliary prediction objectives in addition to predicting the next item ID, which remains the primary target. For example, we can derive genres from the items in the original sequence and use this genre sequence as an auxiliary target. This approach serves several purposes: it acts as a regularizer to reduce overfitting on noisy item ID predictions, provides additional insights into user intentions or long-term genre preferences, and, when structured hierarchically, can improve the accuracy of predicting the target item ID. By first predicting auxiliary targets, such as genre or original language, the model effectively narrows down the candidate list, simplifying subsequent item ID prediction.</p><h3>Unique Challenges for Recommendation FM</h3><p>In addition to the infrastructure challenges posed by training bigger models with substantial amounts of user interaction data that are common when trying to build foundation models, there are several unique hurdles specific to recommendations to make them viable. One of unique challenges is entity cold-starting.</p><p>At Netflix, our mission is to entertain the world. New titles are added to the catalog frequently. Therefore the recommendation foundation models require a cold start capability, which means the models need to estimate members’ preferences for newly launched titles before anyone has engaged with them. To enable this, our foundation model training framework is built with the following two capabilities: Incremental training and being able to do inference with unseen entities.</p><ol><li><strong>Incremental training </strong>: Foundation models are trained on extensive datasets, including every member’s history of plays and actions, making frequent retraining impractical. However, our catalog and member preferences continually evolve. Unlike large language models, which can be incrementally trained with stable token vocabularies, our recommendation models require new embeddings for new titles, necessitating expanded embedding layers and output components. To address this, we warm-start new models by reusing parameters from previous models and initializing new parameters for new titles. For example, new title embeddings can be initialized by adding slight random noise to existing average embeddings or by using a weighted combination of similar titles’ embeddings based on metadata. This approach allows new titles to start with relevant embeddings, facilitating faster fine-tuning. In practice, the initialization method becomes less critical when more member interaction data is used for fine-tuning.</li><li><strong>Dealing with unseen entities </strong>: Even with incremental training, it’s not always guaranteed to learn efficiently on new entities (ex: newly launched titles). It’s also possible that there will be some new entities that are not included/seen in the training data even if we fine-tune foundation models on a frequent basis. Therefore, it’s also important to let foundation models use metadata information of entities and inputs, not just member interaction data. Thus, our foundation model combines both learnable item id embeddings and learnable embeddings from metadata. The following diagram demonstrates this idea.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*7qnfUGWgXtVUjhP9" /><figcaption><strong>Figure 2. </strong>Titles are associated with various metadata, such as genres, storylines, and tones. Each type of metadata could be represented by averaging its respective embeddings, which are then concatenated to form the overall metadata-based embedding for the title.</figcaption></figure><p>To create the final title embedding, we combine this metadata-based embedding with a fully-learnable ID-based embedding using a mixing layer. Instead of simply summing these embeddings, we use an attention mechanism based on the “age” of the entity. This approach allows new titles with limited interaction data to rely more on metadata, while established titles can depend more on ID-based embeddings. Since titles with similar metadata can have different user engagement, their embeddings should reflect these differences. Introducing some randomness during training encourages the model to learn from metadata rather than relying solely on ID embeddings. This method ensures that newly-launched or pre-launch titles have reasonable embeddings even with no user interaction data.</p><h3>Downstream Applications and Challenges</h3><p>Our recommendation foundation model is designed to understand long-term member preferences and can be utilized in various ways by downstream applications:</p><ol><li><strong>Direct Use as a Predictive Model </strong>The model is primarily trained to predict the next entity a user will interact with. It includes multiple predictor heads for different tasks, such as forecasting member preferences for various genres. These can be directly applied to meet diverse business needs..</li><li><strong>Utilizing embeddings </strong>The model generates valuable embeddings for members and entities like videos, games, and genres. These embeddings are calculated in batch jobs and stored for use in both offline and online applications. They can serve as features in other models or be used for candidate generation, such as retrieving appealing titles for a user. High-quality title embeddings also support title-to-title recommendations. However, one important consideration is that the embedding space has arbitrary, uninterpretable dimensions and is incompatible across different model training runs. This poses challenges for downstream consumers, who must adapt to each retraining and redeployment, risking bugs due to invalidated assumptions about the embedding structure. To address this, we apply an orthogonal low-rank transformation to stabilize the user/item embedding space, ensuring consistent meaning of embedding dimensions, even as the base foundation model is retrained and redeployed.</li><li><strong>Fine-Tuning with Specific Data </strong>The model’s adaptability allows for fine-tuning with application-specific data. Users can integrate the full model or subgraphs into their own models, fine-tuning them with less data and computational power. This approach achieves performance comparable to previous models, despite the initial foundation model requiring significant resources.</li></ol><h3>Scaling Foundation Models for Netflix Recommendations</h3><p>In scaling up our foundation model for Netflix recommendations, we draw inspiration from the success of large language models (LLMs). Just as LLMs have demonstrated the power of scaling in improving performance, we find that scaling is crucial for enhancing generative recommendation tasks. Successful scaling demands robust evaluation, efficient training algorithms, and substantial computing resources. Evaluation must effectively differentiate model performance and identify areas for improvement. Scaling involves data, model, and context scaling, incorporating user engagement, external reviews, multimedia assets, and high-quality embeddings. Our experiments confirm that the scaling law also applies to our foundation model, with consistent improvements observed as we increase data and model size.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dEypYqp643q6GcVzn3IIww.png" /><figcaption><strong>Figure 3. </strong>The relationship between model parameter size and relative performance improvement. The plot demonstrates the scaling law in recommendation modeling, showing a trend of increased performance with larger model sizes. The x-axis is logarithmically scaled to highlight growth across different magnitudes.</figcaption></figure><h3>Conclusion</h3><p>In conclusion, our Foundation Model for Personalized Recommendation represents a significant step towards creating a unified, data-centric system that leverages large-scale data to increase the quality of recommendations for our members. This approach borrows insights from Large Language Models (LLMs), particularly the principles of semi-supervised learning and end-to-end training, aiming to harness the vast scale of unlabeled user interaction data. Addressing unique challenges, like cold start and presentation bias, the model also acknowledges the distinct differences between language tasks and recommendation. The Foundation Model allows various downstream applications, from direct use as a predictive model to generate user and entity embeddings for other applications, and can be fine-tuned for specific canvases. We see promising results from downstream integrations. This move from multiple specialized models to a more comprehensive system marks an exciting development in the field of personalized recommendation systems.</p><h3>Acknowledgements</h3><p>Contributors to this work (name in alphabetical order): <a href="https://www.linkedin.com/in/aileisun/">Ai-Lei Sun</a> <a href="https://www.linkedin.com/in/aishafenton/">Aish Fenton</a> <a href="https://www.linkedin.com/in/annecocos/">Anne Cocos</a> <a href="https://www.linkedin.com/in/foranuj/">Anuj Shah</a> <a href="https://www.linkedin.com/in/arashaghevli/">Arash Aghevli</a> <a href="https://www.linkedin.com/in/baolin-li-659426115/">Baolin Li</a> <a href="https://www.linkedin.com/in/bowei-yan-0080a326/">Bowei Yan</a> <a href="https://www.linkedin.com/in/danielzheng256/">Dan Zheng</a> <a href="https://www.linkedin.com/in/dwliang/">Dawen Liang</a> <a href="https://www.linkedin.com/in/ding-tong-2812785a/">Ding Tong</a> <a href="https://www.linkedin.com/in/divya-gadde-3ba01551/">Divya Gadde</a> <a href="https://www.linkedin.com/in/emma-yanyang-kong-6904b457/">Emma Kong</a> <a href="https://www.linkedin.com/in/gary-y-62175170/">Gary Yeh</a> <a href="https://www.linkedin.com/in/inbar-naor-6b973a50/">Inbar Naor</a> <a href="https://www.linkedin.com/in/jinwangw/">Jin Wang</a> <a href="https://www.linkedin.com/in/jbasilico/">Justin Basilico</a> <a href="https://www.linkedin.com/in/kabir-nagrecha/overlay/about-this-profile/">Kabir Nagrecha</a> <a href="https://www.linkedin.com/in/kzielnicki/">Kevin Zielnicki</a> <a href="https://www.linkedin.com/in/linasbaltrunas/">Linas Baltrunas</a> <a href="https://www.linkedin.com/in/lingyi-liu-4b866016/">Lingyi Liu</a> <a href="https://www.linkedin.com/in/lequn-luke-wang-9226b2129/">Luke Wang</a> <a href="https://www.linkedin.com/in/matan-appelbaum-39472b96/">Matan Appelbaum</a> <a href="https://www.linkedin.com/in/tuzhucheng/">Michael Tu</a> <a href="https://www.linkedin.com/in/moumitab/">Moumita Bhattacharya</a> <a href="https://www.linkedin.com/in/pabloadelgado/">Pablo Delgado</a> <a href="https://www.linkedin.com/in/qiuling-xu-a445b815a/">Qiuling Xu</a> <a href="https://www.linkedin.com/in/rakeshkomuravelli/">Rakesh Komuravelli</a> <a href="https://www.linkedin.com/in/raveeshbhalla/">Raveesh Bhalla</a> <a href="https://www.linkedin.com/in/rob-story-b21a4912/">Rob Story</a> <a href="https://www.linkedin.com/in/rogermenezes/">Roger Menezes</a> <a href="https://www.linkedin.com/in/sejoon-oh/">Sejoon Oh</a> <a href="https://www.linkedin.com/in/shahrzad-naseri-1b988760/">Shahrzad Naseri</a> <a href="https://www.linkedin.com/in/swanandjoshi7/">Swanand Joshi</a> <a href="https://www.linkedin.com/in/trungnguyen324/">Trung Nguyen</a> <a href="https://www.linkedin.com/in/vito-ostuni-0b576027/">Vito Ostuni </a><a href="https://www.linkedin.com/in/thomasweiwang/">Wei Wang</a> <a href="https://www.linkedin.com/in/zhezhangncsu/">Zhe Zhang</a></p><h3>Reference</h3><ol><li>C. K. Kang and J. McAuley, “Self-Attentive Sequential Recommendation,” <em>2018 IEEE International Conference on Data Mining (ICDM)</em>, Singapore, 2018, pp. 197–206, doi: 10.1109/ICDM.2018.00035.</li><li>F. Sun et al., “BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer,” <em>Proceedings of the 28th ACM International Conference on Information and Knowledge Management (CIKM ‘19)</em>, Beijing, China, 2019, pp. 1441–1450, doi: 10.1145/3357384.3357895.</li><li>J. Zhai et al., “Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations,” <em>arXiv preprint arXiv:2402.17152</em>, 2024.</li><li>F. Gloeckle, B. Youbi Idrissi, B. Rozière, D. Lopez-Paz, and G. Synnaeve, “Better &amp; Faster Large Language Models via Multi-token Prediction,” arXiv preprint arXiv:2404.19737, Apr. 2024.</li></ol><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1a0bd8e02d39" width="1" /><hr /><p><a href="https://netflixtechblog.com/foundation-model-for-personalized-recommendation-1a0bd8e02d39">Foundation Model for Personalized Recommendation</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/c9ab1f4bd72b</id>
            <title>HDR10+ Now Streaming on Netflix</title>
            <link>https://netflixtechblog.com/hdr10-now-streaming-on-netflix-c9ab1f4bd72b?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/c9ab1f4bd72b</guid>
            <pubDate></pubDate>
            <updated>2025-03-25T22:10:13.122Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><a href="https://www.linkedin.com/in/rquero">Roger Quero</a>, <a href="https://www.linkedin.com/in/liwei-guo">Liwei Guo</a>, <a href="https://www.linkedin.com/in/jeffrwatts/">Jeff Watts</a>, <a href="https://www.linkedin.com/in/joseph-mccormick-7b386026">Joseph McCormick</a>, <a href="https://www.linkedin.com/in/agataopalach/">Agata Opalach</a>, <a href="https://www.linkedin.com/in/anush-moorthy-b8451142/">Anush Moorthy</a></p><p>We are excited to announce that we are now streaming HDR10+ content on our service for AV1-enabled devices, enhancing the viewing experience for certified HDR10+ devices, which previously only received HDR10 content. The dynamic metadata included in our HDR10+ content improves the quality and accuracy of the picture when viewed on these devices.</p><h3>Delighting Members with Even Better Picture Quality</h3><p>Nearly a decade ago, we made a bold move to be a pioneering adopter of High Dynamic Range (HDR) technology. HDR enables images to have more details, vivid colors, and improved realism. We began producing our shows and movies in HDR, encoding them in HDR, and streaming them in HDR for our members. We were confident that it would greatly enhance our members’ viewing experience, and unlock new creative visions — and we were right! In the last five years, HDR streaming has increased by more than 300%, while the number of HDR-configured devices watching Netflix has more than doubled. Since launching HDR with season one of <em>Marco Polo</em>, Netflix now has over 11,000 hours of HDR titles for members to immerse themselves in.</p><p>We continue to enhance member joy while maintaining creative vision by adding support for HDR10+. This will further augment Netflix’s growing HDR ecosystem, preserve creative intent on even more devices, and provide a more immersive viewing experience.</p><p>We enabled HDR10+ on Netflix using the <a href="https://aomedia.org/specifications/av1/">AV1 video codec</a> that was standardized by the Alliance for Open Media (AOM) in 2018. AV1 is one of the most efficient codecs available today. We <a href="https://netflixtechblog.com/bringing-av1-streaming-to-netflix-members-tvs-b7fc88e42320">previously enabled</a> AV1 encoding for SDR content, and saw tremendous value for our members, including higher and more consistent visual quality, lower play delay and increased streaming at the highest resolution. AV1-SDR is already the second most streamed codec at Netflix, behind H.264/AVC, which has been around for over 20 years! With the addition of HDR10+ streams to AV1, we expect the day is not far when AV1 will be the most streamed codec at Netflix.</p><p>To enhance our offering, we have been adding HDR10+ streams to both new releases and existing popular HDR titles. AV1-HDR10+ now accounts for 50% of all eligible viewing hours. We will continue expanding our HDR10+ offerings with the goal of providing an HDR10+ experience for all HDR titles by the end of this year¹.</p><h3><strong>Industry Adopted Formats</strong></h3><p>Today, the industry recognizes three prevalent HDR formats: Dolby Vision, HDR10, and HDR10+. For all three HDR Formats, metadata is embedded in the content, serving as instructions to guide the playback device — whether it’s a TV, mobile device, or computer — on how to display the image.</p><p>HDR10 is the most widely adopted HDR format, supported by all HDR devices. HDR10 uses static metadata that is defined once for the entire content detailing aspects such as the maximum content light level (MaxCLL), maximum frame average light level (MaxFALL), as well as characteristics of the mastering display used for color grading. This metadata only allows for a one-size-fits-all tone mapping of the content for display devices. It cannot account for dynamic contrast across scenes, which most content contains.</p><p>HDR10+ and Dolby Vision improve on this with dynamic metadata that provides content image statistics on a per-frame basis, enabling optimized tone mapping adjustments for each scene. This achieves greater perceptual fidelity to the original, preserving creative intent.</p><h3><strong>HDR10 vs. HDR10+</strong></h3><p>The figure below shows screen grabs of two AV1-encoded frames of the same content displayed using HDR10 (top) and HDR10+ (bottom).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AjnQRaY7VFZoonX5SI36IA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gsW42hweG6RMbWwQjy1etg.png" /></figure><p><em>Photographs of devices displaying the same frame with HDR10 metadata (top) and HDR10+ metadata (bottom). Notice the preservation of the flashlight detail in the HDR10+ capture, and the over-exposure of the region under the flashlight in the HDR10 one².</em></p><p>As seen in the flashlight on the table, the highlight details are clipped in the HDR10 content, but are recovered in HDR10+. Further, the region under the flashlight is overexposed in the HDR10 content, while HDR10+ renders that region with greater fidelity to the source. The reason HDR10+, with its dynamic metadata, shines in this example is that the scenes preceding and following the scene with this frame have markedly different luminance statistics. The static HDR10 metadata is unable to account for the change in the content. While this is a simple example, the dynamic metadata in HDR10+ demonstrates such value across any set of scenes. This consistency allows our members to stay immersed in the content, and better preserves creative intent.</p><h3><strong>Receiving HDR10+</strong></h3><p>At the time of launch, these requirements must be satisfied to receive HDR10+:</p><p>1.Member must have a Netflix Premium plan subscription</p><p>2. Title must be available in HDR10+ format</p><p>3. Member device must support AV1 &amp; HDR10+. Here are some examples of compatible devices:</p><ul><li>SmartTVs, mobile phones, and tablets that meet Netflix certification for HDR10+</li><li>Source device (such as set-top boxes, streaming devices, MVPDs, etc.) that meets Netflix certification for HDR10+, connected to an HDR10+ compliant display via HDMI</li></ul><p>4. For TV or streaming devices, ensure that the HDR toggle is enabled in our Netflix application settings: <a href="https://help.netflix.com/en/node/100220">https://help.netflix.com/en/node/100220</a></p><p>Additional guidance: <a href="https://help.netflix.com/en/node/13444">https://help.netflix.com/en/node/13444</a></p><h3>Summary</h3><p>More HDR content is watched every day on Netflix. Expanding the Netflix HDR ecosystem to include HDR10+ increases the accessibility of HDR content with dynamic metadata to more members, improves the viewing experience, and preserves the creative intent of our content creators. The commitment to innovation and quality underscores our dedication to delivering an immersive and authentic viewing experience for all our members.</p><h3>Acknowledgements</h3><p>Launching HDR10+ was a collaborative effort involving multiple teams at Netflix, and we are grateful to everyone who contributed to making this idea a reality. We would like to extend our thanks to the following teams for their crucial roles in this launch:</p><ul><li>The various Client and Partner Engineering teams at Netflix that manage the Netflix experience across different device platforms.<br />Special acknowledgments: <a href="https://www.linkedin.com/in/akshaygarg05/">Akshay Garg</a>, <a href="https://www.linkedin.com/in/dashap/">Dasha Polyakova</a>, <a href="https://www.linkedin.com/in/wei-vivian-li/">Vivian Li</a>, <a href="https://www.linkedin.com/in/benjamintoofer/">Ben Toofer</a>, <a href="https://www.linkedin.com/in/allanzp/">Allan Zhou</a>, <a href="https://www.linkedin.com/in/artemdanylenko/">Artem Danylenko</a></li><li>The Encoding Technologies team that is responsible for producing optimized encodings to enable high-quality experiences for our members. Special acknowledgments: <a href="https://www.linkedin.com/in/adithyaprakash/">Adithya Prakash</a>, <a href="https://www.linkedin.com/in/carvalhovinicius/">Vinicius Carvalho</a></li><li>The Content Operations &amp; Innovation teams responsible for producing and delivering HDR content to Netflix, maintaining the intent of creative vision from production to streaming. Special acknowledgements: <a href="https://www.linkedin.com/in/michael-keegan-072a4950/">Michael Keegan</a></li><li>The Product Discover team that enables seamless UI discovery journey for our members. Special acknowledgments: <a href="https://www.linkedin.com/in/chad-mckee/">Chad McKee</a> <a href="https://www.linkedin.com/in/ramyasomaskandan/">Ramya Somaskandan</a></li><li>The Playback Experience team that delivers the best possible experience to our members. Special acknowledgments: <a href="https://www.linkedin.com/in/nate-santti/">Nate Santti</a></li></ul><h4>Footnotes</h4><ol><li>While we have enabled HDR10+ for distribution i.e., for what our members consume on their devices, we continue to accept only Dolby Vision masters on the ingest side, i.e., for all content delivery to Netflix as per our <a href="https://partnerhelp.netflixstudios.com/hc/en-us/sections/360012197873-Branded-Delivery-Specifications">delivery specification</a>. In addition to HDR10+, we continue to serve HDR10 and DolbyVision. Our encoding pipeline is designed with flexibility and extensibility where all these HDR formats could be derived from a single DolbyVision deliverable efficiently at scale.</li><li>We recognize that it is hard to convey visual improvements in HDR video using still photographs converted to SDR. We encourage the reader to stream Netflix content in HDR10+ and check for yourself!</li></ol><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c9ab1f4bd72b" width="1" /><hr /><p><a href="https://netflixtechblog.com/hdr10-now-streaming-on-netflix-c9ab1f4bd72b">HDR10+ Now Streaming on Netflix</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/8efe69ebd653</id>
            <title>Title Launch Observability at Netflix Scale</title>
            <link>https://netflixtechblog.com/title-launch-observability-at-netflix-scale-8efe69ebd653?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/8efe69ebd653</guid>
            <pubDate></pubDate>
            <updated>2025-03-05T01:24:53.778Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <h4>Part 3: System Strategies and Architecture</h4><p><strong>By:</strong> <a href="https://www.linkedin.com/in/varun-khaitan/">Varun Khaitan</a></p><p>With special thanks to my stunning colleagues: <a href="https://www.linkedin.com/in/mallikarao/">Mallika Rao</a>, <a href="https://www.linkedin.com/in/esmir-mesic/">Esmir Mesic</a>, <a href="https://www.linkedin.com/in/hugodesmarques/">Hugo Marques</a></p><p>This blog post is a continuation of <a href="https://netflixtechblog.com/title-launch-observability-at-netflix-scale-19ea916be1ed">Part 2</a>, where we cleared the ambiguity around title launch observability at Netflix. In this installment, we will explore the strategies, tools, and methodologies that were employed to achieve comprehensive title observability at scale.</p><h3>Defining the observability endpoint</h3><p>To create a comprehensive solution, we decided to introduce observability endpoints first. Each microservice involved in our <strong>Personalization stack</strong> that integrated with our observability solution had to introduce a new “Title Health” endpoint. Our goal was for each new endpoint to adhere to a few principles:</p><ol><li>Accurate reflection of production behavior</li><li>Standardization across all endpoints</li><li>Answering the Insight Triad: “Healthy” or not, why not and how to fix it.</li></ol><p><strong>Accurately Reflecting Production Behavior</strong></p><p>A key part of our solution is insights into production behavior, which necessitates our requests to the endpoint result in traffic to the real service functions that mimics the same pathways the traffic would take if it came from the usual callers.</p><p>In order to allow for this mimicking, many systems implement an “event” handling, where they convert our request into a call to the real service with properties enabled to log when titles are filtered out of their response and why. Building services that adhere to software best practices, such as Object-Oriented Programming (OOP), the SOLID principles, and modularization, is crucial to have success at this stage. Without these practices, service endpoints may become tightly coupled to business logic, making it challenging and costly to add a new endpoint that seamlessly integrates with the observability solution while following the same production logic.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/760/0*8s2gCb2Pqw2Q0Frq" /><figcaption><em>A service with modular business logic facilitates the seamless addition of an observability endpoint.</em></figcaption></figure><p><strong>Standardization</strong></p><p>To standardize communication between our observability service and the personalization stack’s observability endpoints, we’ve developed a stable proto request/response format. This centralized format, defined and maintained by our team, ensures all endpoints adhere to a consistent protocol. As a result, requests are uniformly handled, and responses are processed cohesively. This standardization enhances adoption within the personalization stack, simplifies the system, and improves understanding and debuggability for engineers.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*P-0nxUAHve77yBtv" /><figcaption><em>The request schema for the observability endpoint.</em></figcaption></figure><p><strong>The Insight Triad API</strong></p><p>To efficiently understand the health of a title and triage issues quickly, all implementations of the observability endpoint must answer: is the title eligible for this phase of promotion, if not — why is it not eligible, and what can be done to fix any problems.</p><p>The end-users of this observability system are Launch Managers, whose job it is to ensure smooth title launches. As such, they must be able to quickly see whether there is a problem, what the problem is, and how to solve it. Teams implementing the endpoint must provide as much information as possible so that a non-engineer (Launch Manager) can understand the root cause of the issue and fix any title setup issues as they arise. They must also provide enough information for partner engineers to identify the problem with the underlying service in cases of system-level issues.</p><p>These requirements are captured in the following protobuf object that defines the endpoint response.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*aeo7vs3h2Z5JKH5t" /><figcaption><em>The response schema for the observability endpoint.</em></figcaption></figure><h3>High level architecture</h3><p>We’ve distilled our comprehensive solution into the following key steps, capturing the essence of our approach:</p><ol><li>Establish observability endpoints across all services within our Personalization and Discovery Stack.</li><li>Implement proactive monitoring for each of these endpoints.</li><li>Track real-time title impressions from the Netflix UI.</li><li>Store the data in an optimized, highly distributed datastore.</li><li>Offer easy-to-integrate APIs for our dashboard, enabling stakeholders to track specific titles effectively.</li><li>“Time Travel” to validate ahead of time.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*1h2cwZDfmz8nis_h" /><figcaption><em>Observability stack high level architecture diagram</em></figcaption></figure><p>In the following sections, we will explore each of these concepts and components as illustrated in the diagram above.</p><h3>Key Features</h3><h4>Proactive monitoring through scheduled collectors jobs</h4><p>Our Title Health microservice runs a scheduled collector job every 30 minutes for most of our personalization stack.</p><p>For each Netflix row we support (such as Trending Now, Coming Soon, etc.), there is a dedicated collector. These collectors retrieve the relevant list of titles from our catalog that qualify for a specific row by interfacing with our catalog services. These services are informed about the expected subset of titles for each row, for which we are assessing title health.</p><p>Once a collector retrieves its list of candidate titles, it orchestrates batched calls to assigned row services using the above standardized schema to retrieve all the relevant health information of the titles. Additionally, some collectors will instead poll our kafka queue for impressions data.</p><h4>Real-time Title Impressions and Kafka Queue</h4><p>In addition to evaluating title health via our personalization stack services, we also keep an eye on how our recommendation algorithms treat titles by reviewing impressions data. It’s essential that our algorithms treat all titles equitably, for each one has limitless potential.</p><p>This data is processed from a real-time impressions stream into a Kafka queue, which our title health system regularly polls. Specialized collectors access the Kafka queue every two minutes to retrieve impressions data. This data is then aggregated in minute(s) intervals, calculating the number of impressions titles receive in near-real-time, and presented as an additional health status indicator for stakeholders.</p><h4>Data storage and distribution through Hollow Feeds</h4><p><a href="https://hollow.how/">Netflix Hollow</a> is an Open Source java library and toolset for disseminating in-memory datasets from a single producer to many consumers for high performance read-only access. Given the shape of our data, hollow feeds are an excellent strategy to distribute the data across our service boxes.</p><p>Once collectors gather health data from partner services in the personalization stack or from our impressions stream, this data is stored in a dedicated Hollow feed for each collector. Hollow offers numerous features that help us monitor the overall health of a Netflix row, including ensuring there are no large-scale issues across a feed publish. It also allows us to track the history of each title by maintaining a per-title data history, calculate differences between previous and current data versions, and roll back to earlier versions if a problematic data change is detected.</p><h4>Observability Dashboard using Health Check Engine</h4><p>We maintain several dashboards that utilize our title health service to present the status of titles to stakeholders. These user interfaces access an endpoint in our service, enabling them to request the current status of a title across all supported rows. This endpoint efficiently reads from all available Hollow Feeds to obtain the current status, thanks to Hollow’s in-memory capabilities. The results are returned in a standardized format, ensuring easy support for future UIs.</p><p>Additionally, we have other endpoints that can summarize the health of a title across subsets of sections to highlight specific member experiences.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*dBFS1pBlqNoCUHwV" /><figcaption>Message depicting a dashboard request.</figcaption></figure><h4>Time Traveling: Catching before launch</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Zz2Y8yjPAsbG5WVR" /></figure><p>Titles launching at Netflix go through several phases of pre-promotion before ultimately launching on our platform. For each of these phases, the first several hours of promotion are critical for the reach and effective personalization of a title, especially once the title has launched. Thus, to prevent issues as titles go through the launch lifecycle, our observability system needs to be capable of simulating traffic ahead of time so that relevant teams can catch and fix issues before they impact members. We call this capability <strong>“Time Travel”</strong>.</p><p>Many of the metadata and assets involved in title setup have specific timelines for when they become available to members. To determine if a title will be viewable at the start of an experience, we must simulate a request to a partner service as if it were from a future time when those specific metadata or assets are available. This is achieved by including a future timestamp in our request to the observability endpoint, corresponding to when the title is expected to appear for a given experience. The endpoint then communicates with any further downstream services using the context of that future timestamp.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*jrdqpJmp0lzna6Zc" /><figcaption>An example request with a future timestamp.</figcaption></figure><h3>Conclusion</h3><p>Throughout this series, we’ve explored the journey of enhancing title launch observability at Netflix. In <a href="https://netflixtechblog.com/title-launch-observability-at-netflix-scale-c88c586629eb">Part 1</a>, we identified the challenges of managing vast content launches and the need for scalable solutions to ensure each title’s success. <a href="https://netflixtechblog.com/title-launch-observability-at-netflix-scale-19ea916be1ed">Part 2</a> highlighted the strategic approach to navigating ambiguity, introducing “Title Health” as a framework to align teams and prioritize core issues. In this final part, we detailed the sophisticated system strategies and architecture, including observability endpoints, proactive monitoring, and “Time Travel” capabilities; all designed to ensure a thrilling viewing experience.</p><p>By investing in these innovative solutions, we enhance the discoverability and success of each title, fostering trust with content creators and partners. This journey not only bolsters our operational capabilities but also lays the groundwork for future innovations, ensuring that every story reaches its intended audience and that every member enjoys their favorite titles on Netflix.</p><p>Thank you for joining us on this exploration, and stay tuned for more insights and innovations as we continue to entertain the world.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8efe69ebd653" width="1" /><hr /><p><a href="https://netflixtechblog.com/title-launch-observability-at-netflix-scale-8efe69ebd653">Title Launch Observability at Netflix Scale</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/e2b67c88c9fb</id>
            <title>Introducing Impressions at Netflix</title>
            <link>https://netflixtechblog.com/introducing-impressions-at-netflix-e2b67c88c9fb?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/e2b67c88c9fb</guid>
            <pubDate></pubDate>
            <updated>2025-02-15T01:12:54.346Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <h4>Part 1: Creating the Source of Truth for Impressions</h4><p><strong>By:</strong> <a href="https://www.linkedin.com/in/tulikabhatt/">Tulika Bhatt</a></p><p>Imagine scrolling through Netflix, where each movie poster or promotional banner competes for your attention. Every image you hover over isn’t just a visual placeholder; it’s a critical data point that fuels our sophisticated personalization engine. At Netflix, we call these images ‘impressions,’ and they play a pivotal role in transforming your interaction from simple browsing into an immersive binge-watching experience, all tailored to your unique tastes.</p><p>Capturing these moments and turning them into a personalized journey is no simple feat. It requires a state-of-the-art system that can track and process these impressions while maintaining a detailed history of each profile’s exposure. This nuanced integration of data and technology empowers us to offer bespoke content recommendations.</p><p>In this multi-part blog series, we take you behind the scenes of our system that processes billions of impressions daily. We will explore the challenges we encounter and unveil how we are building a resilient solution that transforms these client-side impressions into a personalized content discovery experience for every Netflix viewer.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*T6tQiUj-VDtyEhd1" /><figcaption>Impressions on homepage</figcaption></figure><h3>Why do we need impression history?</h3><h4>Enhanced Personalization</h4><p>To tailor recommendations more effectively, it’s crucial to track what content a user has already encountered. Having impression history helps us achieve this by allowing us to identify content that has been displayed on the homepage but not engaged with, helping us deliver fresh, engaging recommendations.</p><h4>Frequency Capping</h4><p>By maintaining a history of impressions, we can implement frequency capping to prevent over-exposure to the same content. This ensures users aren’t repeatedly shown identical options, keeping the viewing experience vibrant and reducing the risk of frustration or disengagement.</p><h4>Highlighting New Releases</h4><p>For new content, impression history helps us monitor initial user interactions and adjust our merchandising efforts accordingly. We can experiment with different content placements or promotional strategies to boost visibility and engagement.</p><h4>Analytical Insights</h4><p>Additionally, impression history offers insightful information for addressing a number of platform-related analytics queries. Analyzing impression history, for example, might help determine how well a specific row on the home page is functioning or assess the effectiveness of a merchandising strategy.</p><h3>Architecture Overview</h3><p>The first pivotal step in managing impressions begins with the creation of a Source-of-Truth (SOT) dataset. This foundational dataset is essential, as it supports various downstream workflows and enables a multitude of use cases.</p><h4>Collecting Raw Impression Events</h4><p>As Netflix members explore our platform, their interactions with the user interface spark a vast array of raw events. These events are promptly relayed from the client side to our servers, entering a centralized event processing queue. This queue ensures we are consistently capturing raw events from our global user base.</p><p>After raw events are collected into a centralized queue, a custom event extractor processes this data to identify and extract all impression events. These extracted events are then routed to an Apache Kafka topic for immediate processing needs and simultaneously stored in an Apache Iceberg table for long-term retention and historical analysis. This dual-path approach leverages Kafka’s capability for low-latency streaming and Iceberg’s efficient management of large-scale, immutable datasets, ensuring both real-time responsiveness and comprehensive historical data availability.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*4NRQp10pg9KK_GKU" /><figcaption>Collecting raw impression events</figcaption></figure><h4>Filtering &amp; Enriching Raw Impressions</h4><p>Once the raw impression events are queued, a stateless Apache Flink job takes charge, meticulously processing this data. It filters out any invalid entries and enriches the valid ones with additional metadata, such as show or movie title details, and the specific page and row location where each impression was presented to users. This refined output is then structured using an Avro schema, establishing a definitive source of truth for Netflix’s impression data. The enriched data is seamlessly accessible for both real-time applications via Kafka and historical analysis through storage in an Apache Iceberg table. This dual availability ensures immediate processing capabilities alongside comprehensive long-term data retention.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Lhs-gvhMuIyKylHt" /><figcaption>Impression Source-of-Truth architecture</figcaption></figure><h4>Ensuring High Quality Impressions</h4><p>Maintaining the highest quality of impressions is a top priority. We accomplish this by gathering detailed column-level metrics that offer insights into the state and quality of each impression. These metrics include everything from validating identifiers to checking that essential columns are properly filled. The data collected feeds into a comprehensive quality dashboard and supports a tiered threshold-based alerting system. These alerts promptly notify us of any potential issues, enabling us to swiftly address regressions. Additionally, while enriching the data, we ensure that all columns are in agreement with each other, offering in-place corrections wherever possible to deliver accurate data.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/736/0*VWssCnOIabEqo02H" /><figcaption>Dashboard showing mismatch count between two columns- entityId and videoId</figcaption></figure><h3>Configuration</h3><p>We handle a staggering volume of 1 to 1.5 million impression events globally every second, with each event approximately 1.2KB in size. To efficiently process this massive influx in real-time, we employ Apache Flink for its low-latency stream processing capabilities, which seamlessly integrates both batch and stream processing to facilitate efficient backfilling of historical data and ensure consistency across real-time and historical analyses. Our Flink configuration includes 8 task managers per region, each equipped with 8 CPU cores and 32GB of memory, operating at a parallelism of 48, allowing us to handle the necessary scale and speed for seamless performance delivery. The Flink job’s sink is equipped with a data mesh connector, as detailed in our <a href="https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873">Data Mesh platform</a> which has two outputs: Kafka and Iceberg. This setup allows for efficient streaming of real-time data through Kafka and the preservation of historical data in Iceberg, providing a comprehensive and flexible data processing and storage solution.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*B-hm-UJMBV7-WOb6" /><figcaption>Raw impressions records per second</figcaption></figure><p>We utilize the ‘island model’ for deploying our Flink jobs, where all dependencies for a given application reside within a single region. This approach ensures high availability by isolating regions, so if one becomes degraded, others remain unaffected, allowing traffic to be shifted between regions to maintain service continuity. Thus, all data in one region is processed by the Flink job deployed within that region.</p><h3>Future Work</h3><h4>Addressing the Challenge of Unschematized Events</h4><p>Allowing raw events to land on our centralized processing queue unschematized offers significant flexibility, but it also introduces challenges. Without a defined schema, it can be difficult to determine whether missing data was intentional or due to a logging error. We are investigating solutions to introduce schema management that maintains flexibility while providing clarity.</p><h4>Automating Performance Tuning with Autoscalers</h4><p>Tuning the performance of our Apache Flink jobs is currently a manual process. The next step is to integrate with autoscalers, which can dynamically adjust resources based on workload demands. This integration will not only optimize performance but also ensure more efficient resource utilization.</p><h4>Improving Data Quality Alerts</h4><p>Right now, there’s a lot of business rules dictating when a data quality alert needs to be fired. This leads to a lot of false positives that require manual judgement. A lot of times it is difficult to track changes leading to regression due to inadequate data lineage information. We are investing in building a comprehensive data quality platform that more intelligently identifies anomalies in our impression stream, keeps track of data lineage and data governance, and also, generates alerts notifying producers of any regressions. This approach will enhance efficiency, reduce manual oversight, and ensure a higher standard of data integrity.</p><h3>Conclusion</h3><p>Creating a reliable source of truth for impressions is a complex but essential task that enhances personalization and discovery experience. Stay tuned for the next part of this series, where we’ll delve into how we use this SOT dataset to create a microservice that provides impression histories. We invite you to share your thoughts in the comments and continue with us on this journey of discovering impressions.</p><h3>Acknowledgments</h3><p>We are genuinely grateful to our amazing colleagues whose contributions were essential to the success of Impressions: Julian Jaffe, Bryan Keller, Yun Wang, Brandon Bremen, Kyle Alford, Ron Brown and Shriya Arora.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e2b67c88c9fb" width="1" /><hr /><p><a href="https://netflixtechblog.com/introducing-impressions-at-netflix-e2b67c88c9fb">Introducing Impressions at Netflix</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/19ea916be1ed</id>
            <title>Title Launch Observability at Netflix Scale</title>
            <link>https://netflixtechblog.com/title-launch-observability-at-netflix-scale-19ea916be1ed?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/19ea916be1ed</guid>
            <pubDate></pubDate>
            <updated>2025-01-07T01:26:33.650Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <h4>Part 2: Navigating Ambiguity</h4><p><strong>By:</strong> <a href="https://www.linkedin.com/in/varun-khaitan/">Varun Khaitan</a></p><p>With special thanks to my stunning colleagues: <a href="https://www.linkedin.com/in/mallikarao/">Mallika Rao</a>, <a href="https://www.linkedin.com/in/esmir-mesic/">Esmir Mesic</a>, <a href="https://www.linkedin.com/in/hugodesmarques/">Hugo Marques</a></p><p>Building on the foundation laid in <a href="https://medium.com/netflix-techblog/title-launch-observability-at-netflix-scale-c88c586629eb">Part 1</a>, where we explored the “what” behind the challenges of title launch observability at Netflix, this post shifts focus to the “how.” How do we ensure every title launches seamlessly and remains discoverable by the right audience?</p><p>In the dynamic world of technology, it’s tempting to leap into problem-solving mode. But the key to lasting success lies in taking a step back — understanding the broader context before diving into solutions. This thoughtful approach doesn’t just address immediate hurdles; it builds the resilience and scalability needed for the future. Let’s explore how this mindset drives results.</p><h3>Understanding the Bigger Picture</h3><p>Let’s take a comprehensive look at all the elements involved and how they interconnect. We should aim to address questions such as: What is vital to the business? Which aspects of the problem are essential to resolve? And how did we arrive at this point?</p><p>This process involves:</p><ol><li><strong>Identifying Stakeholders: </strong>Determine who is impacted by the issue and whose input is crucial for a successful resolution. In this case, the main stakeholders are:<br /><br />-<strong><em> Title Launch Operators<br />Role:</em></strong><em> Responsible for setting up the title and its metadata into our systems.<br /></em><strong><em>Challenge:</em></strong><em> Don’t understand the cascading effects of their setup on these perceived black box personalization systems<br /><br />-</em><strong><em> Personalization System Engineers</em></strong><em><br /> </em><strong><em>Role: </em></strong><em>Develop and operate the personalization systems.<br /></em><strong><em>Challenge:</em></strong><em> End up spending unplanned cycles on title launch and personalization investigations.<br /><br />- </em><strong><em>Product Managers </em></strong><em><br /></em><strong><em>Role: </em></strong><em>Ensure we put forward the best experience for our members.<br /></em><strong><em>Challenge: </em></strong><em>Members may not connect with the most relevant title.<br /><br />- </em><strong><em>Creative Representatives</em></strong><em> <br /></em><strong><em>Role:</em></strong><em> Mediator between the content creators and Netflix.<br /></em><strong><em>Challenge: </em></strong><em>Build trust in the Netflix brand with content creators.</em></li><li><strong>Mapping the Current Landscape:</strong> By charting the existing landscape, we can pinpoint areas ripe for improvement and steer clear of redundant efforts. Beyond the scattered solutions and makeshift scripts, it became evident that there was no established solution for title launch observability. This suggests that this area has been neglected for quite some time and likely requires significant investment. This situation presents both challenges and opportunities; while it may be more difficult to make initial progress, there are plenty of easy wins to capitalize on.</li><li><strong>Clarifying the Core Problem:</strong> By clearly defining the problem, we can ensure that our solutions address the root cause rather than just the symptoms. While there were many issues and problems we could address, the core problem here was to make sure every title was treated fairly by our personalization stack. If we can ensure fair treatment with confidence and bring that visibility to all our stakeholders, we can address all their challenges.</li><li><strong>Assessing Business Priorities: </strong>Understanding what is most important to the organization helps prioritize actions and resources effectively. In this context, we’re focused on developing systems that ensure successful title launches, build trust between content creators and our brand, and reduce engineering operational overhead. While this is a critical business need and we definitely should solve it, it’s essential to evaluate how it stacks up against other priorities across different areas of the organization.</li></ol><h3>Defining Title Health</h3><p>Navigating such an ambiguous space required a shared understanding to foster clarity and collaboration. To address this, we introduced the term “Title Health,” a concept designed to help us communicate effectively and capture the nuances of maintaining each title’s visibility and performance. This shared language became a foundation for discussing the complexities of this domain.</p><p><strong>“Title Health”</strong> encompasses various metrics and indicators that reflect how well a title is performing, in terms of discoverability and member engagement. The three main questions we try to answer are:</p><ol><li>Is this title visible at all to <strong>any</strong> <strong>member</strong>?</li><li>Is this title visible to an appropriate <strong>audience size</strong>?</li><li>Is this title reaching <strong>all the appropriate audiences</strong>?</li></ol><p>Defining Title Health provided a framework to monitor and optimize each title’s lifecycle. It allowed us to align with partners on principles and requirements before building solutions, ensuring every title reaches its intended audience seamlessly. This common language not only introduced the problem space effectively but also accelerated collaboration and decision-making across teams.</p><h3>Categories of issues</h3><p>To build a robust plan for title launch observability, we first needed to categorize the types of issues we encounter. This structured approach allows us to address all aspects of title health comprehensively.</p><p>Currently, these issues are grouped into three primary categories:</p><p><strong>1. Title Setup</strong></p><p>A title’s setup includes essential attributes like metadata (e.g., launch dates, audio and subtitle languages, editorial tags) and assets (e.g., artwork, trailers, supplemental messages). These elements are critical for a title’s eligibility in a row, accurate personalization, and an engaging presentation. Since these attributes feed directly into algorithms, any delays or inaccuracies can ripple through the system.</p><p>The observability system must ensure that title setup is complete and validated in a timely manner, identify potential bottlenecks and ensure a smooth launch process.</p><p><strong>2. Personalization Systems</strong></p><p>Titles are eligible to be recommended across multiple canvases on product — HomePage, Coming Soon, Messaging, Search and more. Personalization systems handle the recommendation and serving of titles on these canvases, leveraging a vast ecosystem of microservices, caches, databases, code, and configurations to build these product canvases.</p><p>We aim to validate that titles are eligible in all appropriate product canvases across the end to end personalization stack during all of the title’s launch phases.</p><p><strong>3. Algorithms</strong></p><p>Complex algorithms drive each personalized product experience, recommending titles tailored to individual members. Observability here means validating the accuracy of algorithmic recommendations for all titles.<br />Algorithmic performance can be affected by various factors, such as model shortcomings, incomplete or inaccurate input signals, feature anomalies, or interactions between titles. Identifying and addressing these issues ensures that recommendations remain precise and effective.</p><p>By categorizing issues into these areas, we can systematically address challenges and deliver a reliable, personalized experience for every title on our platform.</p><h3>Issue Analysis</h3><p>Let’s also learn more about how often we see each of these types of issues and how much effort it takes to fix them once they come up.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*YyCLwVKiGE_L6fWb" /></figure><p>From the above chart, we see that setup issues are the most common but they are also easy to fix since it’s relatively straightforward to go back and rectify a title’s metadata. System issues, which mostly manifest as bugs in our personalization microservices are not uncommon, and they take moderate effort to address. Algorithm issues, while rare, are really difficult to address since these often involve interpreting and retraining complex machine learning models.</p><h3>Evaluating Our Options</h3><p>Now that we understand more deeply about the problems we want to address and how we should go about prioritizing our resources. Lets go back to the two options we discussed in Part 1, and make an informed decision.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/756/1*5YRxJT3YI53wgtLs9zO6gg.png" /></figure><p>Ultimately, we realized this space demands the full spectrum of features we’ve discussed. But the question remained: <em>Where do we start?</em> <br />After careful consideration, we chose to focus on proactive issue detection first. Catching problems before launch offered the greatest potential for business impact, ensuring smoother launches, better member experiences, and stronger system reliability.</p><p>This decision wasn’t just about solving today’s challenges — it was about laying the foundation for a scalable, robust system that can grow with the complexities of our ever-evolving platform.</p><h3>Up next</h3><p>In the next iteration we will talk about how to design an observability endpoint that works for all personalization systems. What are the main things to keep in mind while creating a microservice API endpoint? How do we ensure standardization? What is the architecture of the systems involved?</p><p>Keep an eye out for our next binge-worthy episode!</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=19ea916be1ed" width="1" /><hr /><p><a href="https://netflixtechblog.com/title-launch-observability-at-netflix-scale-19ea916be1ed">Title Launch Observability at Netflix Scale</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://medium.com/p/e67f0aa82183</id>
            <title>Part 3: A Survey of Analytics Engineering Work at Netflix</title>
            <link>https://netflixtechblog.com/part-3-a-survey-of-analytics-engineering-work-at-netflix-e67f0aa82183?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/e67f0aa82183</guid>
            <pubDate></pubDate>
            <updated>2025-01-06T19:27:20.316Z</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p><em>This article is the last in a multi-part series sharing a breadth of Analytics Engineering work at Netflix, recently presented as part of our annual internal Analytics Engineering conference. Need to catch up? Check out </em><a href="https://research.netflix.com/publication/part-1-a-survey-of-analytics-engineering-work-at-netflix"><em>Part 1</em></a><em>, which detailed how we’re empowering Netflix to efficiently produce and effectively deliver high quality, actionable analytic insights across the company and </em><a href="https://research.netflix.com/publication/part-2-a-survey-of-analytics-engineering-work-at-netflix"><em>Part 2</em></a><em>, which stepped through a few exciting business applications for Analytics Engineering. This post will go into aspects of technical craft.</em></p><h3>Dashboard Design Tips</h3><p><a href="https://www.linkedin.com/in/rinachang">Rina Chang</a>, <a href="https://www.linkedin.com/in/shansusielu/">Susie Lu</a></p><p>What is design, and why does it matter? Often people think design is about how things look, but design is actually about how things work. Everything is designed, because we’re all making choices about how things work, but not everything is designed well. Good design doesn’t waste time or mental energy; instead, it helps the user achieve their goals.</p><p>When applying this to a dashboard application, the easiest way to use design effectively is to leverage existing patterns. (For example, people have learned that blue underlined text on a website means it’s a clickable link.) So knowing the arsenal of available patterns and what they imply is useful when making the choice of when to use which pattern.</p><p>First, to design a dashboard well, you need to understand your user.</p><ul><li>Talk to your users throughout the entire product lifecycle. Talk to them early and often, through whatever means you can.</li><li>Understand their needs, ask why, then ask why again. Separate symptoms from problems from solutions.</li><li>Prioritize and clarify — less is more! Distill what you can build that’s differentiated and provides the most value to your user.</li></ul><p>Here is a framework for thinking about what your users are trying to achieve. Where do your users fall on these axes? Don’t solve for multiple positions across these axes in a given view; if that exists, then create different views or potentially different dashboards.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ar0t2-zF5YVuXnUe" /></figure><p>Second, understanding your users’ mental models will allow you to choose how to structure your app to match. A few questions to ask yourself when considering the information architecture of your app include:</p><ul><li>Do you have different user groups trying to accomplish different things? Split them into different apps or different views.</li><li>What should go together on a single page? All the information needed for a single user type to accomplish their “job.” If there are multiple <a href="https://www.christenseninstitute.org/theory/jobs-to-be-done/">jobs to be done</a>, split each out onto its own page.</li><li>What should go together within a single section on a page? All the information needed to answer a single question.</li><li>Does your dashboard feel too difficult to use? You probably have too much information! When in doubt, keep it simple. If needed, hide complexity under an “Advanced” section.</li></ul><p>Here are some general guidelines for page layouts:</p><ul><li>Choose infinite scrolling vs. clicking through multiple pages depending on which option suits your users’ expectations better</li><li>Lead with the most-used information first, above the fold</li><li>Create signposts that cue the user to where they are by labeling pages, sections, and links</li><li>Use cards or borders to visually group related items together</li><li>Leverage nesting to create well-understood “scopes of control.” Specifically, users expect a controller object to affect children either: Below it (if horizontal) or To the right of it (if vertical)</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/816/0*KIqd6dZXD_NZyTKR" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/816/0*O52xqUnDsJ8kPCVZ" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/816/0*6qCQlTNyoabhrkVa" /></figure><p>Third, some tips and tricks can help you more easily tackle the unique design challenges that come with making interactive charts.</p><ul><li>Titles: Make sure filters are represented in the title or subtitle of the chart for easy scannability and screenshot-ability.</li><li>Tooltips: Core details should be on the page, while the context in the tooltip is for deeper information. Annotate multiple points when there are only a handful of lines.</li><li>Annotations: Provide annotations on charts to explain shifts in values so all users can access that context.</li><li>Color: Limit the number of colors you use. Be consistent in how you use colors. Otherwise, colors lose meaning.</li><li>Onboarding: Separate out onboarding to your dashboard from routine usage.</li></ul><p>Finally, it is important to note that these are general guidelines, but there is always room for interpretation and/or the use of good judgment to adapt them to suit your own product and use cases. At the end of the day, the most important thing is that a user can leverage the data insights provided by your dashboard to perform their work, and good design is a means to that end.</p><h3><strong>Learnings from Deploying an Analytics API at Netflix</strong></h3><p><a href="https://www.linkedin.com/in/devincarullo/">Devin Carullo</a></p><p>At Netflix Studio, we operate at the intersection of art and science. Data is a tool that enhances decision-making, complementing the deep expertise and industry knowledge of our creative professionals.</p><p>One example is in production budgeting — namely, determining how much we should spend to produce a given show or movie. Although there was already a process for creating and comparing budgets for new productions against similar past projects, it was highly manual. We developed a tool that automatically selects and compares similar Netflix productions, flagging any anomalies for Production Finance to review.</p><p>To ensure success, it was essential that results be delivered in real-time and integrated seamlessly into existing tools. This required close collaboration among product teams, DSE, and front-end and back-end developers. We developed a GraphQL endpoint using Metaflow, integrating it into the existing budgeting product. This solution enabled data to be used more effectively for real-time decision-making.</p><p>We recently launched our MVP and continue to iterate on the product. Reflecting on our journey, the path to launch was complex and filled with unexpected challenges. As an analytics engineer accustomed to crafting quick solutions, I underestimated the effort required to deploy a production-grade analytics API.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*KOgCUre0HvjZ82ZH" /><figcaption>Fig 1. My vague idea of how my API would work</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*BBEHaQdU_e57_sjD" /><figcaption>Fig 2: Our actual solution</figcaption></figure><p>With hindsight, below are my key learnings.</p><p><strong>Measure Impact and Necessity of Real-Time Results</strong></p><p>Before implementing real-time analytics, assess whether real-time results are truly necessary for your use case. This can significantly impact the complexity and cost of your solution. Batch processing data may provide a similar impact and take significantly less time. It’s easier to develop and maintain, and tends to be more familiar for analytics engineers, data scientists, and data engineers.</p><p>Additionally, if you are developing a proof of concept, the upfront investment may not be worth it. Scrappy solutions can often be the best choice for analytics work.</p><p><strong>Explore All Available Solutions</strong></p><p>At Netflix, there were multiple established methods for creating an API, but none perfectly suited our specific use case. Metaflow, a tool developed at Netflix for data science projects, already supported REST APIs. However, this approach did not align with the preferred workflow of our engineering partners. Although they could integrate with REST endpoints, this solution presented inherent limitations. Large response sizes rendered the API/front-end integration unreliable, necessitating the addition of filter parameters to reduce the response size.</p><p>Additionally, the product we were integrating into was using GraphQL, and deviating from this established engineering approach was not ideal. Lastly, given our goal to overlay results throughout the product, GraphQL features, such as federation, proved to be particularly advantageous.</p><p>After realizing there wasn’t an existing solution at Netflix for deploying python endpoints with GraphQL, we worked with the Metaflow team to build this feature. This allowed us to continue developing via Metaflow and allowed our engineering partners to stay on their paved path.</p><p><strong>Align on Performance Expectations</strong></p><p>A major challenge during development was managing API latency. Much of this could have been mitigated by aligning on performance expectations from the outset. Initially, we operated under our assumptions of what constituted an acceptable response time, which differed greatly from the actual needs of our users and our engineering partners.</p><p>Understanding user expectations is key to designing an effective solution. Our methodology resulted in a full budget analysis taking, on average, 7 seconds. Users were willing to wait for an analysis when they modified a budget, but not every time they accessed one. To address this, we implemented caching using Metaflow, reducing the API response time to approximately 1 second for cached results. Additionally, we set up a nightly batch job to pre-cache results.</p><p>While users were generally okay with waiting for analysis during changes, we had to be mindful of GraphQL’s 30-second limit. This highlighted the importance of continuously monitoring the impact of changes on response times, leading us to our next key learning: rigorous testing.</p><p><strong>Real-Time Analysis Requires Rigorous Testing</strong></p><p>Load Testing: We leveraged Locust to measure the response time of our endpoint and assess how the endpoint responded to reasonable and elevated loads. We were able to use FullStory, which was already being used in the product, to estimate expected calls per minute.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*xVjhqU2DZV7RYBD0" /><figcaption>Fig 3. Locust allows us to simulate concurrent calls and measure response time</figcaption></figure><p>Unit Tests &amp; Integration Tests: Code testing is always a good idea, but it can often be overlooked in analytics. It is especially important when you are delivering live analysis to circumvent end users from being the first to see an error or incorrect information. We implemented unit testing and full integration tests, ensuring that our analysis would return correct results.</p><p><strong>The Importance of Aligning Workflows and Collaboration</strong></p><p>This project marked the first time our team collaborated directly with our engineering partners to integrate a DSE API into their product. Throughout the process, we discovered significant gaps in our understanding of each other’s workflows. Assumptions about each other’s knowledge and processes led to misunderstandings and delays.</p><p>Deployment Paths: Our engineering partners followed a strict deployment path, whereas our approach on the DSE side was more flexible. We typically tested our work on feature branches using Metaflow projects and then pushed results to production. However, this lack of control led to issues, such as inadvertently deploying changes to production before the corresponding product updates were ready and difficulties in managing a test endpoint. Ultimately, we deferred to our engineering partners to establish a deployment path and collaborated with the Metaflow team and data engineers to implement it effectively.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*BaqggE2wQ2C9Svo8" /><figcaption>Fig 4. Our current deployment path</figcaption></figure><p>Work Planning: While the engineering team operated on sprints, our DSE team planned by quarters. This misalignment in planning cycles is an ongoing challenge that we are actively working to resolve.</p><p>Looking ahead, our team is committed to continuing this partnership with our engineering colleagues. Both teams have invested significant time in building this relationship, and we are optimistic that it will yield substantial benefits in future projects.</p><h3>External Speaker: Benn Stancil</h3><p>In addition to the above presentations, we kicked off our Analytics Summit with a keynote talk from <a href="https://www.linkedin.com/in/benn-stancil/">Benn Stancil</a>, Founder of Mode Analytics. Benn stepped through a history of the modern data stack, and the group discussed ideas on the future of analytics.</p><p>Analytics Engineering is a key contributor to building our deep data culture at Netflix, and we are proud to have a large group of stunning colleagues that are not only applying but advancing our analytical capabilities at Netflix. The 2024 Analytics Summit continued to be a wonderful way to give visibility to one another on work across business verticals, celebrate our collective impact, and highlight what’s to come in analytics practice at Netflix.</p><p>To learn more, follow the <a href="https://research.netflix.com/research-area/analytics">Netflix Research Site</a>, and if you are also interested in entertaining the world, have a look at <a href="https://explore.jobs.netflix.net/careers">our open roles</a>!</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e67f0aa82183" width="1" /><hr /><p><a href="https://netflixtechblog.com/part-3-a-survey-of-analytics-engineering-work-at-netflix-e67f0aa82183">Part 3: A Survey of Analytics Engineering Work at Netflix</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>