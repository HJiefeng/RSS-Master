<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>Cloud Blog</title>
        <link>https://cloud.google.com/blog/</link>
        
        <item>
            <id>https://cloud.google.com/blog/topics/inside-google-cloud/whats-new-google-cloud/</id>
            <title>What’s new with Google Cloud</title>
            <link>https://cloud.google.com/blog/topics/inside-google-cloud/whats-new-google-cloud/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/topics/inside-google-cloud/whats-new-google-cloud/</guid>
            <pubDate></pubDate>
            <updated>Fri, 16 May 2025 16:00:00 +0000</updated>
                
                
            <media:content url="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/52_-_Whats_new.jpg"/>
                
            <content:encoded>
                <![CDATA[
                    
                    
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/products/databases/techniques-for-improving-text-to-sql/</id>
            <title>Getting AI to write good SQL: Text-to-SQL techniques explained</title>
            <link>https://cloud.google.com/blog/products/databases/techniques-for-improving-text-to-sql/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/products/databases/techniques-for-improving-text-to-sql/</guid>
            <pubDate></pubDate>
            <updated>Fri, 16 May 2025 16:00:00 +0000</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Organizations depend on fast and accurate data-driven insights to make decisions, and SQL is at the core of how they access that data. With Gemini, Google can generate SQL directly from natural language — a.k.a. text-to-SQL. This capability increases developer and analysts’ productivity and empowers non-technical users to interact directly with the data they need.</span></p>
<p><span style="vertical-align: baseline;">Today, you can find text-to-SQL capabilities in many Google Cloud products:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">BigQuery Studio</strong><span style="vertical-align: baseline;"> in the </span><a href="https://cloud.google.com/bigquery/docs/write-sql-gemini#generate_sql_from_a_comment"><span style="text-decoration: underline; vertical-align: baseline;">SQL Editor</span></a><span style="vertical-align: baseline;"> and </span><a href="https://cloud.google.com/bigquery/docs/write-sql-gemini#use_the_sql_generation_tool"><span style="text-decoration: underline; vertical-align: baseline;">SQL Generation tool</span></a><span style="vertical-align: baseline;">, and within the </span><strong style="vertical-align: baseline;">Data Canvas</strong><span style="vertical-align: baseline;"> </span><a href="https://cloud.google.com/blog/products/data-analytics/using-bigquery-data-canvas-a-deep-dive?e=48754805#:~:text=powered%20by%20Gemini-,2.%20Generate%20SQL,-You%20can%20also"><span style="text-decoration: underline; vertical-align: baseline;">SQL node</span></a></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">"Help me code" functionality in </span><a href="https://cloud.google.com/sql/docs/mysql/write-sql-gemini"><strong style="text-decoration: underline; vertical-align: baseline;">Cloud SQL Studio</strong></a><strong style="vertical-align: baseline;"> (</strong><span style="vertical-align: baseline;">Postgres, MySQL and SQLServer), </span><strong style="vertical-align: baseline;">AlloyDB Studio</strong><span style="vertical-align: baseline;"> and </span><strong style="vertical-align: baseline;">Cloud Spanner Studio</strong></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">AlloyDB AI</strong><span style="vertical-align: baseline;"> with its direct natural language interface to the database, currently available as a public preview</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Through </span><strong style="vertical-align: baseline;">Vertex AI</strong><span style="vertical-align: baseline;">, which lets you access the Gemini models that are the basis for these products directly</span></p>
</li>
</ul>
<p><span style="vertical-align: baseline;">Recently, powerful large language models (LLMs) like Gemini, with their abilities to reason and synthesize, have driven remarkable advancements in the field of text-to-SQL. In this blog post, the first entry in a series, we explore the technical internals of Google Cloud's text-to-SQL agents. We will cover state-of-the-art approaches to context building and table retrieval, how to do effective evaluation of text-to-SQL quality with LLM-as-a-judge techniques, the best approaches to LLM prompting and post-processing, and how we approach techniques that allows the system to offer virtually certified correct answers.</span></p></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="1 Text-to-SQL at Google Cloud" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Text-to-SQL_at_Google_Cloud.max-1000x1000.jpg" />
        
        </a>
      
        <figcaption class="article-image__caption "><p>The ‘Help me code’ feature in Cloud SQL Studio generates SQL from a text prompt</p></figcaption>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">The challenges of text-to-SQL technology</strong></h3>
<p><span style="vertical-align: baseline;">Current state-of-the-art LLMs like Gemini 2.5 have reasoning capabilities that make them good at translating complex questions posed in natural language to functioning SQL, complete with joins, filters, aggregations and other difficult concepts.</span></p>
<p><span style="vertical-align: baseline;">To see this in action you can do a simple test in </span><a href="https://cloud.google.com/generative-ai-studio"><span style="text-decoration: underline; vertical-align: baseline;">Vertex AI Studio</span></a><span style="vertical-align: baseline;">. Given the prompt </span><span style="font-style: italic; vertical-align: baseline;">"I have a database schema that contains products and orders. Write a SQL query that shows the number of orders for shoes"</span><span style="vertical-align: baseline;">, Gemini produces SQL for a hypothetical schema:</span></p></div>
<div class="block-code"><dl>
    <dt>code_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;code&#x27;, &quot;SELECT COUNT(DISTINCT o.order_id) AS NumberOfShoeOrders FROM orders o JOIN order_items oi ON o.order_id = oi.order_id JOIN products p ON oi.product_id = p.product_id WHERE p.product_name LIKE &#x27;%shoe%&#x27;.&quot;), (&#x27;language&#x27;, &#x27;&#x27;), (&#x27;caption&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e753f062be0&gt;)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Great, this is a good looking query. But what happens when you move beyond this trivial example, and use Gemini for text-to-SQL against a real world database and on real-world user questions? It turns out that the problem is more difficult. The model needs to be complemented with methods to: </span></p>
<ol>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">provide business-specific context</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">understand user intent</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">manage differences in SQL dialects</span></p>
</li>
</ol>
<p><span style="vertical-align: baseline;">Let’s take a look at each of these challenges. </span></p>
<p><strong style="vertical-align: baseline;">Problem #1: Provide business-specific context</strong></p>
<p><span style="vertical-align: baseline;">Just like data analysts or engineers, LLMs need significant amounts of knowledge or "context" to generate accurate SQL. The context can be both explicit (what does the schema look like, what are the relevant columns, and what does the data itself look like?) or more implicit (what is the precise semantic meaning of a piece of data? what does it mean for the specific business case?).</span></p>
<p><span style="vertical-align: baseline;">Specialized model training, or fine tuning, is typically not a scalable solution to this problem. Training on the shape of every database or dataset, and keeping up with schema or data changes, is both difficult and cost-prohibitive. Business knowledge and semantics are often not well documented in the first place, and difficult to turn into training data.</span></p>
<p style="padding-left: 40px;"><span style="font-style: italic; vertical-align: baseline;">For example, even the best DBA in the world would not be able to write an accurate query to track shoe sales if they didn't know that </span><code style="font-style: italic; vertical-align: baseline;">cat_id2 = 'Footwear'</code><span style="font-style: italic; vertical-align: baseline;"> in a </span><code style="font-style: italic; vertical-align: baseline;">pcat_extension</code><span style="font-style: italic; vertical-align: baseline;"> table means that the product in question is a kind of shoe. The same is true for  LLMs.</span></p>
<p><strong style="vertical-align: baseline;">Problem #2: Understanding user intent</strong></p>
<p><span style="vertical-align: baseline;">Natural language is less precise than SQL. An engineer or analyst faced with an ambiguous question can detect that they need more information and go back and ask the right follow-up questions. An LLM, on the other hand, tends to try to give you an answer, and when the question is ambiguous, can be prone to hallucinating.</span></p>
<p style="padding-left: 40px;"><span style="font-style: italic; vertical-align: baseline;">Example: Take a question like "What are the best-selling shoes?" Here, one obvious point of ambiguity is what "best selling" actually means in the context of the business or application — the most ordered shoes? The shoe brand that brought in the most money? Further, should the SQL count returned orders? And how many kinds of shoes do you want to see in the report? etc. </span></p>
<p><span style="vertical-align: baseline;">Further, different users need different kinds of answers. If the user is a technical analyst or a developer asking a vague question, giving them a reasonable, but perhaps not 100% correct SQL query is a good starting point. On the other hand, if the user is less technical and does not understand SQL, providing precise, correct SQL is more important. Being able to reply with follow-up questions to disambiguate, explaining the reasoning that went into an answer, and guiding the user to what they are looking for is key.</span></p>
<p><strong style="vertical-align: baseline;">Problem #3: Limits of LLM generation</strong></p>
<p><span style="vertical-align: baseline;">Out of the box, LLMs are particularly good at tasks like creative writing, summarizing or extracting information from documents. But some models can struggle with following precise instructions and getting details exactly right, particularly when it comes to more obscure SQL features. To be able to produce correct SQL, the LLM needs to adhere closely to what can often turn into complex specifications.</span></p>
<p style="padding-left: 40px;"><span style="font-style: italic; vertical-align: baseline;">Example: Consider the differences between SQL dialects, which are more subtle than differences between programming languages like Python and Java. As a simple example, if you're using BigQuery SQL, the correct function for extracting a month from a timestamp column is </span><code style="font-style: italic; vertical-align: baseline;">EXTRACT(MONTH FROM timestamp_column)</code><span style="font-style: italic; vertical-align: baseline;">. But if you are using MySQL, you use </span><code style="font-style: italic; vertical-align: baseline;">MONTH(timestamp_column)</code><span style="font-style: italic; vertical-align: baseline;">.</span></p>
<h3><strong style="vertical-align: baseline;">Text-to-SQL techniques</strong></h3>
<p><span style="vertical-align: baseline;">At Google Cloud, we’re constantly evolving our text-to-SQL agents to improve their quality. To address the problems listed above, we apply a number of techniques.<br /><br /></span></p>
<div align="left">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;"><table><colgroup><col /><col /></colgroup>
<tbody>
<tr>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><strong style="vertical-align: baseline;">Problem</strong></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><strong style="vertical-align: baseline;">Solutions</strong></p>
</td>
</tr>
<tr>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">Understanding schema, data and business concepts</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<ul>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Intelligent retrieval</strong><span style="vertical-align: baseline;"> and ranking of datasets, tables and columns, based on semantic similarity.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">In-context-learning</strong><span style="vertical-align: baseline;"> with business specific examples</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Data linking and sampling</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Semantic layer over raw data. This provides a bridge between complex data structures and the everyday language used by the customer</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Usage pattern analysis and query history</span></p>
</li>
</ul>
</td>
</tr>
<tr>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">Understanding user intent</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><strong style="vertical-align: baseline;">Disambiguation using LLMs</strong></p>
<ul>
<ul>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Entity resolution</span></p>
</li>
</ul>
</ul>
<p><strong style="vertical-align: baseline;">SQL-aware foundation models</strong></p>
</td>
</tr>
<tr>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">Limits of LLM generation</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><strong style="vertical-align: baseline;">Self-consistency</strong></p>
<p><strong style="vertical-align: baseline;">Validation and rewriting</strong></p>
<ul>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Strong foundation models</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">In-context-learning with dialect specific examples</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Model finetuning</span></p>
</li>
</ul>
</td>
</tr>
</tbody>
</table></div>
</div>
</div>
</div>
</div>
</div>
</div></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="2 Text-to-SQL at Google Cloud" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Text-to-SQL_at_Google_Cloud.max-1000x1000.jpg" />
        
        </a>
      
        <figcaption class="article-image__caption "><p>The text-to-SQL architecture</p></figcaption>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Let’s take a closer look at some of these techniques.</span></p>
<p><strong style="vertical-align: baseline;">SQL-aware models<br /></strong><span style="vertical-align: baseline;">Strong LLMs are the foundation of text-to-SQL solutions, and the Gemini family of models has a proven track record of high-quality code and SQL generation. Depending on the particular SQL generation task, we mix and match model versions, including some cases where we employ customized fine-tuning, for example to ensure that models provide sufficiently good SQL for certain dialects.</span></p>
<p><strong style="vertical-align: baseline;">Disambiguation using LLMs<br /></strong><span style="vertical-align: baseline;">Disambiguation involves getting the system to respond with a clarifying question when faced with a question that is not clear enough (in the example above of "What is the best selling shoe?" should lead to a follow-up question like "Would you like to see the shoes ordered by order quantity or revenue?" from the text-to-SQL agent). Here we typically orchestrate LLM calls to first try to identify if a question can actually be answered given the available schema and data, and if not, to generate the necessary follow-up questions to clarify the user's intent.</span></p>
<p><strong style="vertical-align: baseline;">Retrieval and in-context-learning<br /></strong><span style="vertical-align: baseline;">As mentioned above, providing models with the context they need to generate SQL is critical. We use a variety of indexing and retrieval techniques — first to identify relevant datasets, tables and columns, typically using vector search for multi-stage semantic matching, then to load additional useful context. Depending on the product, this may include things like user-provided schema annotations, examples of similar SQL or how to apply specific business rules, or samples of recent queries that a user has run against the same datasets. All of this data is organized into prompts then passed to the model. Gemini's support for long context windows unlocks new capabilities here by allowing the model to handle large schemas and other contextual information.</span></p>
<p><strong style="vertical-align: baseline;">Validation and reprompting<br /></strong><span style="vertical-align: baseline;">Even with a high-quality model, there is still some level of non-determinism or unpredictability involved in LLM-driven SQL generation. To address this we have found that non-AI approaches like query parsing or doing a dry run of the generated SQL complements model-based workflows well. We can get a clear, deterministic signal if the LLM has missed something crucial, which we then pass back to the model for a second pass. When provided an example of a mistake and some guidance, models can typically address what they got wrong.</span></p>
<p><strong style="vertical-align: baseline;">Self-consistency<br /></strong><span style="vertical-align: baseline;">The idea of self-consistency is to not depend on a single round of generation, but to generate multiple queries for the same user question, potentially using different prompting techniques or model variants, and picking the best one from all candidates. If several models agree that one answer looks particularly good, there is a greater chance that the final SQL query will be accurate and matches what the user is looking for.</span></p>
<h3><strong style="vertical-align: baseline;">Evaluation and measuring improvements</strong></h3>
<p><span style="vertical-align: baseline;">Improving AI-driven capabilities depends on robust evaluation. The text-to-SQL benchmarks developed in the academic community, like the popular </span><a href="https://bird-bench.github.io/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">BIRD-bench</span></a><span style="vertical-align: baseline;">, have been a very useful baseline to understand model and end-to-end system performance. However, these benchmarks are often lacking when it comes to representing broad real-world schemas and workloads. To address this we have developed our own suite of synthetic benchmarks that augment the baseline in many ways.</span></p>
<p style="padding-left: 40px;"><strong style="vertical-align: baseline;">Coverage:</strong><span style="vertical-align: baseline;"> We make sure to have benchmarks that cover a broad list of SQL engines and products, both dialects and engine-specific features. This includes not only queries, but also DDL, DML and other administrative needs, and questions that are representative for common usage patterns, including more complex queries and schemas.</span></p>
<p style="padding-left: 40px;"><strong style="vertical-align: baseline;">Metrics:</strong><span style="vertical-align: baseline;"> We combine user metrics and offline eval metrics, and employ both human and automated evaluation, particularly using LLM-as-a-judge techniques, which reduce cost but still allow us to understand performance on ambiguous and unclear tasks.</span></p>
<p style="padding-left: 40px;"><strong style="vertical-align: baseline;">Continuous evals:</strong><span style="vertical-align: baseline;"> Our engineering and research teams use evals to quickly be able to test out new models, new prompting techniques and other improvements. It can give us signals quickly to tell if an approach is showing promise and is worth pursuing.</span></p>
<p><span style="vertical-align: baseline;">Taken together, using these techniques are driving the remarkable improvements in text-to-SQL that we are witnessing in our labs, as well as in customers’ environments. As you get ready to incorporate text-to-SQL in your own environments, stay tuned for more deep dives into our text-to-SQL solutions. Try Gemini text-to-SQL in </span><a href="https://cloud.google.com/bigquery/docs/write-sql-gemini#use_the_sql_generation_tool"><span style="text-decoration: underline; vertical-align: baseline;">BigQuery Studio</span></a><span style="vertical-align: baseline;">, </span><a href="https://cloud.google.com/sql/docs/mysql/write-sql-gemini"><span style="text-decoration: underline; vertical-align: baseline;">CloudSQL, AlloyDB and Spanner Studio</span></a><span style="vertical-align: baseline;">, and in </span><a href="https://cloud.google.com/blog/products/databases/alloydb-ai-drives-innovation-from-the-database"><span style="text-decoration: underline; vertical-align: baseline;">AlloyDB AI</span></a><span style="vertical-align: baseline;"> today.</span></p></div>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/products/compute/ai-hypercomputer-enhancements-for-the-developer/</id>
            <title>AI Hypercomputer developer experience enhancements from Q1 25: build faster, scale bigger</title>
            <link>https://cloud.google.com/blog/products/compute/ai-hypercomputer-enhancements-for-the-developer/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/products/compute/ai-hypercomputer-enhancements-for-the-developer/</guid>
            <pubDate></pubDate>
            <updated>Fri, 16 May 2025 16:00:00 +0000</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Building cutting-edge AI models is exciting, whether you’re iterating in your notebook or orchestrating large clusters. However, scaling up training can present significant challenges, including navigating complex infrastructure, configuring software and dependencies across numerous instances, and pinpointing performance bottlenecks. </span></p>
<p><span style="vertical-align: baseline;">At Google Cloud, we're focused on making AI training easier, whatever your scale. We're continuously evolving our AI Hypercomputer system, not just with powerful hardware like </span><a href="https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">TPUs</span></a><span style="vertical-align: baseline;"> and </span><a href="https://cloud.google.com/blog/products/compute/a3-ultra-with-nvidia-h200-gpus-are-ga-on-ai-hypercomputer"><span style="text-decoration: underline; vertical-align: baseline;">GPUs</span></a><span style="vertical-align: baseline;">, but with a suite of </span><a href="https://cloud.google.com/blog/products/compute/whats-new-with-ai-hypercomputer"><span style="text-decoration: underline; vertical-align: baseline;">tools and features</span></a><span style="vertical-align: baseline;"> designed to make </span><span style="font-style: italic; vertical-align: baseline;">you</span><span style="vertical-align: baseline;">, the developer, more productive. Let's dive into some recent enhancements that can help streamline your workflows, from interactive development to optimized training and easier deployment.</span></p>
<h3><strong style="vertical-align: baseline;">Scale from your notebook with Pathways on Cloud</strong></h3>
<p><span style="vertical-align: baseline;">You love the rapid iteration that Jupyter notebooks provide, but scaling to thousands of accelerators means leaving that familiar environment behind. At the same time, having to learn different tools for running workloads at scale isn't practical; nor is tying up large clusters of accelerators for weeks for iterative experiments that might run only for a short time.</span></p>
<p><span style="vertical-align: baseline;">You shouldn't have to choose between ease-of-use and massive scale. With </span><a href="https://jax-ml.github.io/scaling-book/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">JAX</span></a><span style="vertical-align: baseline;">, it’s easy to write code for one accelerator and scale it up to thousands of accelerators. </span><a href="https://cloud.google.com/ai-hypercomputer/docs/workloads/pathways-on-cloud/pathways-intro"><span style="text-decoration: underline; vertical-align: baseline;">Pathways on Cloud</span></a><span style="vertical-align: baseline;">, an orchestration system for creating large-scale, multi-task, and sparsely activated machine learning systems, takes this concept further, making </span><a href="https://cloud.google.com/ai-hypercomputer/docs/workloads/pathways-on-cloud/pathways-interactive-mode"><span style="text-decoration: underline; vertical-align: baseline;">interactive supercomputing</span></a><span style="font-style: italic; vertical-align: baseline;"> </span><span style="vertical-align: baseline;">a reality. Pathways dynamically manages pools of accelerators </span><span style="font-style: italic; vertical-align: baseline;">for you</span><span style="vertical-align: baseline;">, orchestrating data movement and computation across potentially thousands of devices. The result? You can launch an experiment on just one accelerator directly from your Jupyter notebook, refine it, and then scale it to thousands of accelerators </span><span style="font-style: italic; vertical-align: baseline;">within the same interactive session</span><span style="vertical-align: baseline;">. Now you can quickly iterate on research and development without sacrificing scale.</span></p>
<p><span style="vertical-align: baseline;">With Pathways on Cloud, you can finally stop rewriting code for different scales. Stop over-provisioning hardware for weeks when your experiments only need a few hours. Stay focused on your science, iterate faster, and leverage supercomputing power on demand. Watch </span><a href="https://www.youtube.com/watch?v=VdrQIm-nvss" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">this video</span></a><span style="vertical-align: baseline;"> to see how Pathways on Cloud delivers true interactive scaling — far beyond just running JupyterHub on a Google Kubernetes Engine (GKE) cluster.</span></p></div>
<div class="block-video">



<div class="article-module article-video ">
  <figure>
    <a class="h-c-video h-c-video--marquee" href="https://youtube.com/watch?v=VdrQIm-nvss">

      
        

        <div class="article-video__aspect-image">
          <span class="h-u-visually-hidden">Scale AI workloads without leaving your notebook with Pathways on Cloud</span>
        </div>
      
      <svg class="h-c-video__play h-c-icon h-c-icon--color-white" xmlns="http://www.w3.org/2000/svg">
        <use xlink:href="#mi-youtube-icon" xmlns:xlink="http://www.w3.org/1999/xlink"></use>
      </svg>
    </a>

    
  </figure>
</div>

<div class="h-c-modal--video">
   <a class="glue-yt-video" href="https://youtube.com/watch?v=VdrQIm-nvss">
   </a>
</div>

</div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">Debug faster, optimize smarter with Xprofiler</strong></h3>
<p><span style="vertical-align: baseline;">When scaling up a job, simply knowing that your accelerators are being used isn't enough. You need to understand </span><span style="font-style: italic; vertical-align: baseline;">how</span><span style="vertical-align: baseline;"> they're being used and </span><span style="font-style: italic; vertical-align: baseline;">why</span><span style="vertical-align: baseline;"> things might be slow or crashing. How else would you find that pesky out-of-memory error that takes down your entire run?</span></p>
<p><span style="vertical-align: baseline;">Meet the </span><a href="https://github.com/AI-Hypercomputer/cloud-diagnostics-xprof/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Xprofiler library</span></a><span style="vertical-align: baseline;">, your tool for deep performance analysis on Google Cloud accelerators. It lets you profile and trace your code execution, giving you critical insights, especially into the high level operations (HLO) generated by the XLA compiler. Getting actionable insights using Xprofiler is easy. Simply launch an Xprofiler instance from the command line to capture detailed profile and trace logs during your run. Then, use TensorBoard to quickly analyze this data. You can visualize performance bottlenecks, understand hardware limits with roofline analysis (is your workload compute- or memory-bound?), and quickly pinpoint the root cause of errors. Xprofiler helps you optimize your code for peak performance, so you can get the most out of your AI infrastructure.</span></p></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="2" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_hGTFtHn.max-1000x1000.png" />
        
        </a>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">Skip the setup hassle with container images</strong></h3>
<p><span style="vertical-align: baseline;">You have the choice of many powerful AI frameworks and libraries, but configuring them correctly — with the right drivers and dependencies — can be complex and time-consuming. Getting it wrong, especially when scaling to hundreds or thousands of instances, can lead to costly errors and delays. To help you bypass these headaches, we provide pre-built, optimized container images designed for common AI development needs.</span></p>
<p><span style="vertical-align: baseline;">For PyTorch on GPUs, our </span><a href="https://cloud.google.com/ai-hypercomputer/docs/software-stack"><span style="text-decoration: underline; vertical-align: baseline;">GPU-accelerated instance container images</span></a><span style="vertical-align: baseline;"> offer a ready-to-run environment. We partnered closely with NVIDIA to include tested versions of essential software like the </span><a href="https://developer.nvidia.com/cuda-toolkit" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">NVIDIA CUDA Toolkit</span></a><span style="vertical-align: baseline;">, NCCL, and frameworks such as </span><a href="https://www.nvidia.com/en-us/ai-data-science/products/nemo/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">NVIDIA NeMo</span></a><span style="vertical-align: baseline;">. Thanks to </span><a href="https://canonical.com/?utm_source=gcp&amp;utm_medium=ainewsletter" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Canonical</span></a><span style="vertical-align: baseline;">, these run on optimized Ubuntu LTS. Now you can get started quickly with a stable environment that’s tuned for performance, avoiding compatibility challenges and saving significant setup time.</span></p>
<p><span style="vertical-align: baseline;">And if you’re working with JAX (on either TPUs or GPUs), our curated </span><span style="vertical-align: baseline;">container images</span><span style="vertical-align: baseline;"> and recipes for JAX for AI on Google Cloud streamline getting started. Avoid the hassle of manual dependency tracking and configuration with these tested and ready-to-use JAX environments. </span></p></div>
<div class="block-aside"><dl>
    <dt>aside_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;title&#x27;, &#x27;$300 in free credit to try Google Cloud infrastructure&#x27;), (&#x27;body&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e753fb97070&gt;), (&#x27;btn_text&#x27;, &#x27;Start building for free&#x27;), (&#x27;href&#x27;, &#x27;http://console.cloud.google.com/freetrial?redirectPath=/compute&#x27;), (&#x27;image&#x27;, None)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">Boost GPU training efficiency with proven recipes</strong></h3>
<p><span style="vertical-align: baseline;">Beyond setup, maximizing useful compute time ("ML Goodput") during training is crucial, especially at scale. Wasted cycles due to job failures can significantly inflate costs and delay results. To help, we provide techniques and ready-to-use recipes to tackle these challenges. </span></p>
<p><span style="vertical-align: baseline;">Techniques like asynchronous and multi-tier checkpointing increase checkpoint frequency without slowing down training and speed up save/restore operations. AI Hypercomputer can automatically handle interruptions, choosing intelligently between resets, hot-swaps, or scaling actions. Our </span><a href="https://github.com/AI-Hypercomputer/gpu-recipes/tree/main/training/a3mega/llama3-1-70b/nemo-pretraining-gke-resiliency" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">ML Goodput recipe</span></a><span style="vertical-align: baseline;">, created in partnership with NVIDIA, bundles these techniques, integrating NVIDIA NeMo and the </span><a href="https://github.com/NVIDIA/nvidia-resiliency-ext" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">NVIDIA Resiliency Extension</span></a><span style="vertical-align: baseline;"> (NVRx) for a comprehensive solution to boost the efficiency and reliability of your PyTorch training on Google Cloud.</span></p>
<p><span style="vertical-align: baseline;">We also added optimized recipes (complete with checkpointing) for you to benchmark training performance for different storage options like </span><a href="https://github.com/AI-Hypercomputer/gpu-recipes/tree/main/training/a3mega/llama3-1-70b/nemo-pretraining-gke-gcs" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Google Cloud Storage</span></a><span style="vertical-align: baseline;"> and </span><a href="https://github.com/AI-Hypercomputer/gpu-recipes/blob/main/training/a3mega/llama3-1-70b/nemo-pretraining-gke-parallelstore" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Parallelstore</span></a><span style="vertical-align: baseline;">. Lastly, we added </span><a href="https://github.com/AI-Hypercomputer/gpu-recipes/tree/main/training/a4" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">recipes for our A4</span></a><span style="vertical-align: baseline;"> NVIDIA accelerated instance (built on NVIDIA Blackwell). The training recipes include sparse and dense model training up to 512 Blackwell GPUs with PyTorch and JAX.</span></p>
<h3><strong style="vertical-align: baseline;">Cutting-edge JAX LLM development with MaxText</strong></h3>
<p><span style="vertical-align: baseline;">For developers who use JAX for LLMs on Google Cloud, </span><a href="https://github.com/AI-Hypercomputer/maxtext/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">MaxText</span></a><span style="vertical-align: baseline;"> provides advanced training, tuning, and serving on both TPUs and GPUs. Recently, we added support for key fine-tuning techniques like Supervised Fine Tuning (SFT) and Direct Preference Optimization (DPO), alongside </span><a href="https://cloud.google.com/ai-hypercomputer/docs/workloads/pathways-on-cloud/resilient-training"><span style="text-decoration: underline; vertical-align: baseline;">resilient training capabilities</span></a><span style="vertical-align: baseline;"> such as suspend-resume and elastic training. MaxText leverages JAX optimizations and pipeline parallelism techniques that we developed in collaboration with NVIDIA to improve training efficiency across tens of thousands of NVIDIA GPUs. And we also added support and recipes for the latest open models: Gemma 3, Llama 4 </span><a href="https://github.com/AI-Hypercomputer/maxtext/blob/main/end_to_end/tpu/llama4/Run_Llama4.md" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">training</span></a><span style="vertical-align: baseline;"> and inference (</span><a href="https://github.com/AI-Hypercomputer/tpu-recipes/tree/main/inference/trillium/JetStream-Maxtext/Llama-4-Scout-17B-16E" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Scout</span></a><span style="vertical-align: baseline;"> and </span><a href="https://github.com/AI-Hypercomputer/tpu-recipes/tree/main/inference/trillium/JetStream-Maxtext/Llama-4-Maverick-17B-128E" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Maverick</span></a><span style="vertical-align: baseline;">), and DeepSeek v3 </span><a href="https://github.com/AI-Hypercomputer/maxtext/blob/main/end_to_end/tpu/deepseek/Run_DeepSeek.md" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">training</span></a><span style="vertical-align: baseline;"> and </span><a href="https://github.com/AI-Hypercomputer/tpu-recipes/tree/main/inference/trillium/JetStream-Maxtext/DeepSeek-R1-671B" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">inference</span></a><span style="vertical-align: baseline;">.</span></p>
<p><span style="vertical-align: baseline;">To help you get the best performance with Trillium TPU, we added </span><a href="https://github.com/AI-Hypercomputer/tpu-recipes/tree/main/microbenchmarks" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">microbenchmarking recipes</span></a><span style="vertical-align: baseline;"> including matrix multiplication, collective compute, and high-bandwidth memory (HBM) tests scaling up to multiple slices with hundreds of accelerators. These metrics are particularly useful for performance optimization. For production workloads on GKE, be sure to take a look at </span><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/configure-automatic-application-monitoring"><span style="text-decoration: underline; vertical-align: baseline;">automatic application monitoring</span></a><span style="vertical-align: baseline;">.</span></p>
<h3><strong style="vertical-align: baseline;">Harness PyTorch on TPU with PyTorch/XLA 2.7 and torchprime</strong></h3>
<p><span style="vertical-align: baseline;">We're committed to providing an integrated, high-performance experience for PyTorch users on TPUs. To that end, the recently released </span><a href="https://github.com/pytorch/xla/releases/tag/v2.7.0" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">PyTorch/XLA 2.7</span></a><span style="vertical-align: baseline;"> includes notable performance improvements, particularly benefiting users working with </span><a href="https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-vllm-tpu"><span style="text-decoration: underline; vertical-align: baseline;">vLLM on TPU</span></a><span style="vertical-align: baseline;"> for inference. This version also adds an important new flexibility and interoperability capability: you can now call JAX functions directly from within your PyTorch/XLA code.</span></p>
<p><span style="vertical-align: baseline;">Then, to help you harness the power of PyTorch/XLA on TPUs, we introduced </span><a href="https://github.com/AI-Hypercomputer/torchprime" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">torchprime</span></a><span style="vertical-align: baseline;">, a reference implementation for training PyTorch models on TPUs. Torchprime is designed to showcase best practices for large-scale, high-performance model training, making it a great starting point for your PyTorch/XLA development journey.</span></p>
<h3><strong style="vertical-align: baseline;">Build cutting-edge recommenders with RecML</strong></h3>
<p><span style="vertical-align: baseline;">While generative AI often captures the spotlight, highly effective recommender systems remain a cornerstone of many applications, and TPUs offer unique advantages for training them at scale. Deep-learning recommender models frequently rely on massive embedding tables to represent users, items, and their features, and processing these embeddings efficiently is crucial. This is where TPUs shine, particularly with </span><a href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#sparsecore"><span style="text-decoration: underline; vertical-align: baseline;">SparseCore</span></a><span style="vertical-align: baseline;">, a specialized integrated dataflow processor. SparseCore is purpose-built to accelerate the lookup and processing of the vast, sparse embeddings that are typical in recommenders, dramatically speeding up training compared to alternatives.</span></p>
<p><span style="vertical-align: baseline;">To help you leverage this power, we now offer </span><a href="https://github.com/AI-Hypercomputer/RecML" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">RecML</span></a><span style="vertical-align: baseline;">: an easy-to-use, high-performance, large-scale deep-learning recommender system library optimized for TPUs. It provides reference implementations for training state-of-the-art recommender models such as BERT4Rec, Mamba4Rec, SASRec, and HSTU. RecML uses SparseCore to maximize performance, making it easy for you to efficiently utilize the TPU hardware for faster training and scaling of your recommender models.</span></p>
<h3><strong style="vertical-align: baseline;">Build with us!</strong></h3>
<p><span style="vertical-align: baseline;">Improving the AI developer experience on Google Cloud is an ongoing mission. From scaling your interactive experiments with Pathways, to pinpointing bottlenecks with Xprof, to getting started faster with optimized containers and framework recipes, these AI Hypercomputer improvements help to remove friction so you can innovate faster, and build on the other </span><a href="https://cloud.google.com/blog/products/compute/whats-new-with-ai-hypercomputer"><span style="text-decoration: underline; vertical-align: baseline;">AI Hypercomputer innovations</span></a><span style="vertical-align: baseline;"> we announced at Google Cloud Next 25:</span></p></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="3" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/3_LuxKaS3.max-1000x1000.png" />
        
        </a>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Explore these new features, spin up the container images, try the JAX and PyTorch recipes, and contribute back to open-source projects like MaxText, torchprime, and RecML. Your feedback shapes the future of AI development on Google Cloud. Let's build it together.</span></p></div>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/products/containers-kubernetes/gke-data-cache-now-ga-accelerates-stateful-apps/</id>
            <title>Supercharge data access performance with GKE Data Cache</title>
            <link>https://cloud.google.com/blog/products/containers-kubernetes/gke-data-cache-now-ga-accelerates-stateful-apps/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/products/containers-kubernetes/gke-data-cache-now-ga-accelerates-stateful-apps/</guid>
            <pubDate></pubDate>
            <updated>Fri, 16 May 2025 16:00:00 +0000</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Today, we’re excited to announce the general availability (GA) of </span><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/data-cache"><span style="text-decoration: underline; vertical-align: baseline;">GKE Data Cache</span></a><span style="vertical-align: baseline;">, a powerful new solution for Google Kubernetes Engine to accelerate the performance of read-heavy stateful or stateless applications that rely on persistent storage via network attached disks. By intelligently utilizing high-speed local SSDs as a cache layer for persistent disks, GKE Data Cache helps you achieve lower read latency and higher queries per second (QPS), without complex manual configuration. </span></p></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="image 1" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image_1_jQhq4Bt.max-1000x1000.jpg" />
        
        </a>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Using GKE Data Cache with Postgres we have seen:</span></p>
<ul>
<li><strong style="vertical-align: baseline;">Up to a 480% increase in transactions per second for PostgreSQL on GKE</strong></li>
<li><strong style="vertical-align: baseline;">Up to a 80% latency reduction for PostgreSQL on GKE</strong></li>
</ul>
<p style="padding-left: 40px;"><span style="font-style: italic; vertical-align: baseline;">“The launch of GKE Persistent Disk with Data Cache enables significant improvements in vector search performance. Specifically, we've observed that Qdrant search response times are a remarkable 10x faster compared to balanced disks, and 2.5x faster compared to premium SSDs, particularly when operating directly from disk without caching all data and indexes in RAM. Qdrant Hybrid Cloud users on Google Cloud can leverage this advancement to efficiently handle massive datasets, delivering unmatched scalability and speed without relying on full in-memory caching.”</span><span style="vertical-align: baseline;"> - </span><span style="vertical-align: baseline;">Bastian Hofmann</span><span style="vertical-align: baseline;">, Director of Engineering, Qdrant </span></p>
<p><span style="vertical-align: baseline;">Stateful applications like databases, analytics platforms, and content management systems are critical to many businesses. However, their performance can often be limited by the I/O speed of the underlying storage. While persistent disks provide durability and flexibility, read-intensive workloads can experience bottlenecks, impacting application responsiveness and scalability. </span></p>
<p><span style="vertical-align: baseline;">GKE Data Cache addresses this challenge head-on, providing a managed block storage solution that integrates with your existing Persistent Disk or Hyperdisk volumes. When you enable GKE Data Cache on your node pools and configure your workloads to use it, frequently accessed data is automatically cached on the low-latency local SSDs attached to your GKE nodes.</span></p>
<p><span style="vertical-align: baseline;">This caching layer serves read requests directly from the local SSDs whenever the data is available, significantly reducing the need to access the underlying persistent disk and potentially allowing for the use of less system memory cache (RAM) to service requests in a timely manner.</span></p></div>
<div class="block-aside"><dl>
    <dt>aside_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;title&#x27;, &#x27;$300 in free credit to try Google Cloud containers and Kubernetes&#x27;), (&#x27;body&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e753c56bd30&gt;), (&#x27;btn_text&#x27;, &#x27;Start building for free&#x27;), (&#x27;href&#x27;, &#x27;http://console.cloud.google.com/freetrial?redirectpath=/marketplace/product/google/container.googleapis.com&#x27;), (&#x27;image&#x27;, None)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">The result is a substantial improvement in read performance, leading to:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Lower read latency:</strong><span style="vertical-align: baseline;"> Applications experience faster data retrieval, improving the user experience and application responsiveness.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Higher throughput and QPS:</strong><span style="vertical-align: baseline;"> The ability to serve more read requests in parallel allows your applications to handle increased load and perform more intensive data operations.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Potential cost optimization:</strong><span style="vertical-align: baseline;"> By accelerating reads, you may be able to utilize smaller or lower-IOPS persistent disks for your primary storage while still achieving high performance through the cache. Additionally you may be able to reduce the required memory of the machine’s paging cache by pushing the read latency to the local SSD. Memory capacity is more expensive than capacity on a local SSD.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Simplified management:</strong><span style="vertical-align: baseline;"> As a managed feature, GKE Data Cache simplifies the process of implementing and managing a high-performance caching solution for your stateful workloads.</span></p>
</li>
</ul>
<p style="padding-left: 40px;"><span style="font-style: italic; vertical-align: baseline;">"Nothing elevates developer experience like an instant feedback loop. Thanks to GKE Data Cache, developers can spin up pre-warmed Coder Workspaces on demand, blending local-speed coding with the consistency of ephemeral Kubernetes infrastructure."</span><span style="vertical-align: baseline;"> - Ben Potter, VP of Product, Coder</span></p>
<p><span style="vertical-align: baseline;">GKE Data Cache supports all read/write Persistent Disk and Hyperdisk types as backing storage, so you can choose the right persistent storage for your needs while leveraging the performance benefits of local SSDs for reads. You can configure your node pools to dedicate a specific amount of local SSD space for data caching.</span></p>
<p><span style="vertical-align: baseline;">For data consistency, GKE Data Cache offers two write modes: writethrough (recommended for most production workloads to ensure data is written to both the cache and the persistent disk synchronously) and writeback (for workloads prioritizing write speed, with data written to the persistent disk asynchronously).</span></p>
<h3><strong style="vertical-align: baseline;">Getting started</strong></h3>
<p><span style="vertical-align: baseline;">Getting started with GKE Data Cache is straightforward. You'll need a GKE Standard cluster running a compatible version (1.32.3-gke.1440000 or later), node pools configured with local SSDs, the data cache feature enabled, and a StorageClass that specifies the use of data cache acceleration. Your stateful workloads can then request storage with caching using PersistentVolumeClaims that reference this StorageClass. The amount of data to store in a cache for each disk is defined in the StorageClass. </span></p>
<p><span style="vertical-align: baseline;">Here’s how to create a data cache-enabled node pool in an existing cluster:</span></p></div>
<div class="block-code"><dl>
    <dt>code_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;code&#x27;, &#x27;gcloud container node-pools create datacache-node-pool \\\r\n    --cluster=$CLUSTER_NAME \\\r\n    --location=$LOCATION \\\r\n    --data-cache-count=$DATA_CACHE_COUNT \\\r\n    --machine-type=$MACHINE_TYPE&#x27;), (&#x27;language&#x27;, &#x27;&#x27;), (&#x27;caption&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e753c56b7c0&gt;)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">When you create a node pool with the `</span><code style="vertical-align: baseline;">data-cache-count</code><span style="vertical-align: baseline;">` flag, local SSDs (LSSDs) are reserved for the data cache feature. This feature uses those LSSDs to cache data for all pods that have caching enabled and are scheduled onto that node pool.</span></p>
<p><span style="vertical-align: baseline;">The LSSDs not reserved for caching can be used as </span><a href="https://cloud.google.com/kubernetes-engine/docs/concepts/local-ssd#ephemeral"><span style="text-decoration: underline; vertical-align: baseline;">ephemeral storage</span></a><span style="vertical-align: baseline;">. Note that we do not currently support using the remaining LSSDs as </span><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/local-ssd-raw#cluster"><span style="text-decoration: underline; vertical-align: baseline;">raw block storage</span></a><span style="vertical-align: baseline;">.</span></p>
<p><span style="vertical-align: baseline;">Once you reserve the required local SSDs for caching, you set up the cache configuration in the StorageClass with `</span><code style="vertical-align: baseline;">data-cache-mode</code><span style="vertical-align: baseline;">` and `</span><code style="vertical-align: baseline;">data-cache-size</code><span style="vertical-align: baseline;">` then reference that StorageClass in a PersistentVolumeClaim.</span></p>
<p><strong style="font-style: italic; vertical-align: baseline;">Setting up data cache with StorageClass </strong></p></div>
<div class="block-code"><dl>
    <dt>code_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;code&#x27;, &#x27;apiVersion: storage.k8s.io/v1\r\nkind: StorageClass\r\nmetadata:\r\n  name: pd-balanced-data-cache-sc\r\nprovisioner: pd.csi.storage.gke.io\r\nparameters:\r\n  type: pd-balanced\r\n  data-cache-mode: writethrough\r\n  data-cache-size: &quot;100Gi&quot;\r\nvolumeBindingMode: WaitForFirstConsumer\r\nallowVolumeExpansion: true&#x27;), (&#x27;language&#x27;, &#x27;&#x27;), (&#x27;caption&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e753c56b6d0&gt;)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">To learn more about GKE Data Cache and how to implement it for your stateful workloads, please refer to the</span><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/data-cache"><span style="vertical-align: baseline;"> </span><span style="text-decoration: underline; vertical-align: baseline;">official documentation</span></a><span style="vertical-align: baseline;">. </span></p></div>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/products/identity-security/whats-new-with-google-clouds-risk-protection-program/</id>
            <title>Expanding our Risk Protection Program with new insurance partners and AI coverage</title>
            <link>https://cloud.google.com/blog/products/identity-security/whats-new-with-google-clouds-risk-protection-program/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/products/identity-security/whats-new-with-google-clouds-risk-protection-program/</guid>
            <pubDate></pubDate>
            <updated>Thu, 15 May 2025 18:00:00 +0000</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Today’s businesses have a vital need to manage and, when appropriate, transfer cyber risk in their cloud environments — even with robust security measures in place. At Google Cloud Next last month, we unveiled significant advancements to our Risk Protection Program, an industry-first collaboration between Google and leading cyber insurers that provides competitively priced cyber-insurance and broad coverage for Google Cloud customers. </span></p>
<p><span style="vertical-align: baseline;">These updates make the program more accessible to more organizations, expand our specialized cyber insurance offerings, and can help increase your overall confidence in your ability to successfully manage the cyber risk your organization faces. </span><span style="vertical-align: baseline;">The </span><a href="https://cloud.google.com/security/products/risk-protection-program"><span style="text-decoration: underline; vertical-align: baseline;">Risk Protection Program</span></a><span style="vertical-align: baseline;"> (RPP) is based on your cloud security posture as reported through Security Command Center (SCC), and reinforces our </span><a href="https://cloud.google.com/blog/transform/why-shared-fate-shows-us-a-better-cloud-roadmap"><span style="text-decoration: underline; vertical-align: baseline;">shared fate</span></a><span style="vertical-align: baseline;"> model as we actively partner with you for risk management.</span></p>
<h3><strong style="vertical-align: baseline;">New in 2025: Broader access, more choice</strong></h3>
<p><span style="vertical-align: baseline;">The Risk Protection Program has two main components:</span></p>
<ol>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Cyber Insurance Hub, provided by Google:</strong><span style="vertical-align: baseline;"> This tool enables a quick assessment of your risk posture across Google Cloud and facilitates a streamlined process for procuring cyber insurance.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Tailored Cyber Insurance Policies, provided by partners:</strong><span style="vertical-align: baseline;"> These are available through a modern, low-friction underwriting process, and use data submitted by customers to enable personalized pricing based on their specific use and risk posture.</span></p>
</li>
</ol>
<p><span style="vertical-align: baseline;">To further enhance the value and accessibility of these core components, we're introducing several key updates.</span></p>
<h3><strong style="vertical-align: baseline;">New insurance partners, expanded global reach</strong></h3>
<p><span style="vertical-align: baseline;">Giving Google Cloud customers a wider array of choices for tailored cyber insurance, we're excited to welcome </span><a href="https://www.beazley.com/en-us/fullspectrumcyber/google-cloud/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Beazley</span></a><span style="vertical-align: baseline;"> and </span><a href="https://www.chubb.com/risk-protection-program.html" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Chubb</span></a><span style="vertical-align: baseline;">, two of the world's largest cyber-insurers, as new program partners. We are also expanding our collaboration with our founding partner, </span><a href="https://www.munichre.com/en/solutions/for-industry-clients/cloud-protection-plus.html" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Munich Re</span></a><span style="vertical-align: baseline;">, through onboarding Munich Re Specialty and</span><a href="https://www.munichre.com/en/solutions/for-industry-clients/cloud-protection-plus.html" rel="noopener" target="_blank"><span style="vertical-align: baseline;"> their </span></a><span style="vertical-align: baseline;">SMB-focused subsidiary HSB. </span></p></div>
<div class="block-video">



<div class="article-module article-video ">
  <figure>
    <a class="h-c-video h-c-video--marquee" href="https://youtube.com/watch?v=Gr3KHKLl8PM">

      
        

        <div class="article-video__aspect-image">
          <span class="h-u-visually-hidden">How Etsy weaves together security and cyber insurance through Google Cloud&#x27;s Risk Protection Program</span>
        </div>
      
      <svg class="h-c-video__play h-c-icon h-c-icon--color-white" xmlns="http://www.w3.org/2000/svg">
        <use xlink:href="#mi-youtube-icon" xmlns:xlink="http://www.w3.org/1999/xlink"></use>
      </svg>
    </a>

    
  </figure>
</div>

<div class="h-c-modal--video">
   <a class="glue-yt-video" href="https://youtube.com/watch?v=Gr3KHKLl8PM">
   </a>
</div>

</div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">This collaboration will make the Risk Protection Program available to more Google Cloud customers, extending our reach to include businesses of all sizes. We're also expanding the program’s availability internationally, with coverage rolling out in phases, ensuring that more organizations can take advantage of the unique benefits of the program.</span></p>
<h3><strong style="vertical-align: baseline;">Streamlined insurance procurement and data-driven pricing</strong></h3>
<p><span style="vertical-align: baseline;">The </span><a href="https://cloud.google.com/risk-manager/docs"><span style="text-decoration: underline; vertical-align: baseline;">Cyber Insurance Hub</span></a><span style="vertical-align: baseline;"> can help you measure and manage your risk directly on Google Cloud. It assesses your Google Cloud workloads and provides proactive security recommendations. It also generates reports that map your cloud configurations against the industry-standard </span><a href="https://www.cisecurity.org/cis-benchmarks" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">CIS Benchmarks</span></a><span style="vertical-align: baseline;">. </span></p>
<p><span style="vertical-align: baseline;">The hub allows you to directly share your actual risk posture data — derived from your real-world cloud setup — with participating cyber insurers through a streamlined interface. This represents a significant improvement over traditional, often lengthy, and manual insurance applications, crafting a more efficient underwriting process that can deliver potentially more competitive and appropriate pricing.</span></p></div>
<div class="block-aside"><dl>
    <dt>aside_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;title&#x27;, &#x27;$300 in free credit to try Google Cloud security products&#x27;), (&#x27;body&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e75413ae7f0&gt;), (&#x27;btn_text&#x27;, &#x27;Start building for free&#x27;), (&#x27;href&#x27;, &#x27;http://console.cloud.google.com/freetrial?redirectPath=/welcome&#x27;), (&#x27;image&#x27;, None)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">Accelerated onboarding for digital natives</strong></h3>
<p><span style="vertical-align: baseline;">Additionally, for qualified Google Cloud digital native customers, Beazley is removing the traditional, often complex insurance application altogether and replacing it with a single-page attestation. This means an even faster, simpler path to securing tailored cyber insurance for organizations deeply integrated with Google Cloud.</span></p>
<h3><strong style="vertical-align: baseline;">Coverage for emerging AI risks</strong></h3>
<p><span style="vertical-align: baseline;">The Risk Protection Program is further evolving to address the rapidly changing technological landscape, including the increasing adoption of AI. A key enhancement now available is the inclusion of Affirmative AI insurance coverage for your Google-related AI workloads. Building upon Google's existing </span><a href="https://cloud.google.com/blog/products/ai-machine-learning/protecting-customers-with-generative-ai-indemnification"><span style="text-decoration: underline; vertical-align: baseline;">AI indemnification</span></a><span style="vertical-align: baseline;"> commitment, this explicit coverage provides clarity and reduces uncertainty around the unique risks associated with generative AI.</span></p>
<h3><strong style="vertical-align: baseline;">Preparedness against future quantum threats </strong></h3>
<p><span style="vertical-align: baseline;">Demonstrating a commitment to future-forward risk management, Chubb's offerings will include coverage specifically against potential future quantum exploit risks. This proactive step aligns with Google’s ongoing work in </span><a href="https://cloud.google.com/blog/products/identity-security/cloud-ciso-perspectives-prepare-early-for-PQC-resilient-cryptographic-threats/"><span style="text-decoration: underline; vertical-align: baseline;">post-quantum cryptography</span></a><span style="vertical-align: baseline;"> (PQC) and helps customers prepare for potential security challenges posed by future quantum computing advancements.</span></p>
<h3><strong style="vertical-align: baseline;">Threat intelligence, advisory and incident preparedness with Mandiant</strong></h3>
<p><span style="vertical-align: baseline;">Risk Protection Program clients underwritten by Munich Re receive access to Mandiant's expertise in threat intelligence, advisory, and incident preparedness. </span></p>
<p><span style="vertical-align: baseline;">This expertise includes near real-time updates on high-priority exploited vulnerabilities informed by Mandiant Consulting and Google Threat Intelligence Group, technical coaching from experienced Mandiant incident responders during a cybersecurity incident, and tabletop exercises facilitated by Munich Re and Mandiant Consulting to test and enhance incident readiness with your teams.</span></p>
<h3><strong style="vertical-align: baseline;">Translating security investments into financial returns</strong></h3>
<p><span style="vertical-align: baseline;">The enhanced Risk Protection Program helps translate your dedication to security into tangible business value. By using the Cyber Insurance Hub and demonstrating strong security practices on Google Cloud, you can potentially achieve more favorable terms and pricing on cyber insurance premiums — the return on investment is clear. </span></p>
<p><span style="vertical-align: baseline;">We understand that navigating the complexities of cloud risk can be new to some insurers, which is why Google actively works with our cyber insurance partners to ensure they understand the nuances of the cloud, simplifying the process for you.</span></p>
<p style="padding-left: 40px;"><span style="font-style: italic; vertical-align: baseline;">“Google Cloud’s Risk Protection Program has streamlined our insurance underwriting process while providing valuable insight into quantitative risks through standardized reporting, enhanced compliance monitoring, and simplified reporting to our insurance provider. This partnership underscores our shared commitment to safeguarding trust and ensuring the responsible use of data,” said Frank Caserta, CISO, LiveRamp.</span></p>
<p style="padding-left: 40px;"><span style="font-style: italic; vertical-align: baseline;">“We were early adopters of RPP and it’s been a great change in our process. The program has helped us have a more efficient renewal … I just go and I press one button and it goes off to insurance partners. Because I know what data the insurance underwriters are going to be using, I can proactively respond to any questions and therefore cut down time dramatically. To organizations looking to adopt RPP, I would say do it. It makes your insurance renewal process so much more efficient,” said Justinian Fortenberry, CISO and VP of Engineering, Etsy.</span></p>
<h3><strong style="vertical-align: baseline;">It’s easy to get started</strong></h3>
<p><span style="vertical-align: baseline;">At Google Cloud, we firmly believe that the cloud is more than just a risk to be managed, it is a platform for managing risk effectively. Our new enhancements to the Risk Protection Program mark a significant step forward in helping customers navigate the complexities of cloud security and risk management. </span></p>
<p><span style="vertical-align: baseline;">To get started, you can follow these four steps:</span></p>
<ol>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Discuss the Risk Protection Program with your insurance lead and broker.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Onboard to Google's </span><a href="https://cloud.google.com/risk-manager/docs"><span style="text-decoration: underline; vertical-align: baseline;">Cyber Insurance Hub</span></a><span style="vertical-align: baseline;">, run a report, and share the report with your chosen insurance partner(s).</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Work with your insurance broker to complete the cyber insurance underwriting process, and evaluate any quotes.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><a href="https://docs.google.com/forms/d/e/1FAIpQLSd-DUMctn7J2_jLM2YMGzmsjt9Y6ZCrNW0X6v6wModh4puL5A/viewform" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Contact us</span></a><span style="vertical-align: baseline;"> for more information.</span></p>
</li>
</ol></div>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/products/databases/introducing-cassandra-compatible-api-in-spanner/</id>
            <title>Introducing Cassandra-compatible APIs in Spanner for zero-code-change migration</title>
            <link>https://cloud.google.com/blog/products/databases/introducing-cassandra-compatible-api-in-spanner/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/products/databases/introducing-cassandra-compatible-api-in-spanner/</guid>
            <pubDate></pubDate>
            <updated>Thu, 15 May 2025 16:00:00 +0000</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">You heard at Next ‘25 that native support for the Cassandra Query Language (CQL) API in Spanner is available in preview, providing near-zero code-change application migration, along with a suite of complementary tools for zero-downtime data migration. </span></p>
<p><a href="https://cloud.google.com/spanner/docs"><span style="text-decoration: underline; vertical-align: baseline;">Spanner</span></a><span style="vertical-align: baseline;"> is known for its robust SQL support, ACID transactions, and change data capture (CDC), provided as a fully managed service. At the same time, Spanner excels at the basics of NoSQL: single-digit-millisecond read and write latencies, virtually unlimited horizontal scalability, and industry leading high availability. With the CQL addition to Spanner, migrating your application is typically a one-line code change — your application and CQL statements stay the same — while new data migration tooling simplifies live and bulk migration of data. </span></p>
<h3><strong style="vertical-align: baseline;">Why migrate from Cassandra to Spanner?</strong></h3>
<p><span style="vertical-align: baseline;">But first, let’s talk about why an organization might want to migrate from Cassandra to Spanner in the first place. </span></p>
<p><span style="vertical-align: baseline;">NoSQL databases like Apache Cassandra gained popularity for their horizontal scalability and high availability — attributes that are essential for modern, web-scale applications that manage large datasets and user traffic. But while Cassandra's masterless architecture and tunable consistency provides flexibility, but some key challenges persist:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Cassandra suffers from high total cost of ownership (TCO) via inelastic scaling and the substantial operational overhead it requires.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Queries that use secondary indexes may suffer from significant performance impact. </span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">CQL lacks complex joins and sophisticated aggregation functionality.</span></p>
</li>
</ul>
<p><span style="vertical-align: baseline;">In contrast, Spanner offers several key advantages for demanding NoSQL workloads:</span></p>
<ol>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Easy operations: </strong><span style="vertical-align: baseline;">Managing Cassandra requires dedicated expertise and effort for tasks such as hardware/VM provisioning, configuration tuning, scaling, and patching and upgrades. With Spanner, Google Cloud manages these operational tasks, dramatically reducing TCO and freeing engineers to focus on your application, not database administration.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Optimized elastic scale: </strong><span style="vertical-align: baseline;">Spanner offers truly elastic scalability via an autoscaler that automatically adjusts capacity in response to real-time load, helping avoid overprovisioning for peak workloads. Spanner handles data rebalancing in the background, so as not to impact availability or performance of production workloads.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Global ACID transactions:</strong><span style="vertical-align: baseline;"> Cassandra's Lightweight Transactions (LWTs) offer limited compare-and-set functionality within a single partition and cannot guarantee atomicity for multi-partition or multi-step operations without complex, often brittle, application-level workarounds. Spanner supports fully ACID (atomic, consistent, isolated, durable) transactions, simplifying development and helping to ensure data integrity. In fact, </span><a href="https://cloud.google.com/resources/content/critical-capabilities-dbms"><span style="text-decoration: underline; vertical-align: baseline;">Gartner ranks Spanner</span></a><span style="vertical-align: baseline;"> as #1 in the Lightweight Transactions Use Case and #3 in the OLTP Transactions Use Case in its Critical Capabilities for Cloud Database Management Systems for Operational Use Cases</span><span style="vertical-align: baseline;"> </span><span style="vertical-align: baseline;">report.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Strongly consistent global secondary indexes</strong><span style="vertical-align: baseline;">: Cassandra secondary indexes are local indexes, stored as hidden tables on each node that contains the primary data to be indexed. This means that queries on a secondary index that accesses multiple nodes can greatly impact your application’s performance. Spanner offers global secondary indexes that can be used to efficiently query data across the entire database. In addition, Spanner’s secondary indexes are strongly consistent, so your different queries all return results that are consistent and up to date. Spanner also offers </span><a href="https://cloud.google.com/spanner/docs/index-advisor"><span style="text-decoration: underline; vertical-align: baseline;">an index advisor</span></a><span style="vertical-align: baseline;"> that analyzes your queries to recommend new or altered indexes that can improve query performance.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Normalized data for data integrity: </strong><span style="vertical-align: baseline;">Data in Cassandra is often arranged as one query per table, and data is repeated amongst many tables, a process known as denormalization. Data denormalization has several drawbacks: increased data redundancy, inconsistencies between datasets, and increased storage and maintenance requirements as the number of tables grows. In contrast, data in Spanner is normalized, with full support for relational semantics, including enforced foreign keys and rich SQL query capabilities with efficient server-side aggregation.</span></p>
</li>
</ol></div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">Using Spanner’s new Cassandra-compatible APIs</strong></h3>
<p><span style="vertical-align: baseline;">The new APIs and the migration tools are meant to provide a seamless experience for your Cassandra to Spanner migrations. Here’s a quick guide:</span></p></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="cassandra_to_spanner_@2x" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/cassandra_to_spanner_2x.max-1000x1000.png" />
        
        </a>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><ol>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Set up Spanner:</strong><span style="vertical-align: baseline;"> Begin by creating a Spanner instance — regional or multi-regional. For each Cassandra keyspace you intend to migrate, create a corresponding database in your Spanner instance. Using the same name as the Cassandra keyspace is recommended to minimize application code modifications.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Convert your schema:</strong><span style="vertical-align: baseline;"> Utilize the </span><a href="https://github.com/cloudspannerecosystem/spanner-cassandra-schema-tool" rel="noopener" target="_blank"><code style="text-decoration: underline; vertical-align: baseline;">spanner-cassandra-schema-tool</code></a><span style="vertical-align: baseline;"> to automate schema conversion and Spanner tables creation. This tool processes your CQL table definitions, maps Cassandra data types to their Spanner equivalents (e.g., </span><code style="vertical-align: baseline;">text</code><span style="vertical-align: baseline;"> to </span><code style="vertical-align: baseline;">STRING(MAX)</code><span style="vertical-align: baseline;">, </span><code style="vertical-align: baseline;">bigint</code><span style="vertical-align: baseline;"> to </span><code style="vertical-align: baseline;">INT64</code><span style="vertical-align: baseline;">, </span><code style="vertical-align: baseline;">map</code><span style="vertical-align: baseline;"> to </span><code style="vertical-align: baseline;">JSON</code><span style="vertical-align: baseline;">), and automatically generates the transformed Spanner tables.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Migrate data:</strong><span style="vertical-align: baseline;"> Migrate your data from Cassandra to Spanner and subsequently verify its integrity. We typically recommend a two-phased approach to minimize downtime:</span></p>
</li>
<ul>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Live migration (for zero downtime):</strong><span style="vertical-align: baseline;"> To achieve a near zero-downtime migration, implement live migration for incoming data </span><span style="font-style: italic; vertical-align: baseline;">before</span><span style="vertical-align: baseline;"> initiating the bulk data transfer, via the </span><a href="https://github.com/GoogleCloudPlatform/spanner-migration-tool/tree/master/sources/cassandra" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">included Docker file or Terraform template</span></a><span style="vertical-align: baseline;">. The tool leverages the ZDM Proxy for dual-writes and the </span><a href="https://github.com/googleapis/go-spanner-cassandra?tab=readme-ov-file#sidecar-proxy" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Cassandra-Spanner Proxy</span></a><span style="vertical-align: baseline;"> to transform CQL to Spanner API.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Bulk data migration:</strong><span style="vertical-align: baseline;"> Use the </span><a href="https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/main/v2/sourcedb-to-spanner/README_Sourcedb_to_Spanner.md#cassandra-to-spanner-bulk-migration" rel="noopener" target="_blank"><strong style="text-decoration: underline; vertical-align: baseline;">SourceDB to Spanner Dataflow template</strong></a><span style="vertical-align: baseline;"> for a highly parallelized transfer. This template reads data from Cassandra, performs the necessary transformations, and writes it to Spanner.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Data validation:</strong><span style="vertical-align: baseline;"> After the bulk data migration is complete, it’s time to </span><a href="https://cloud.google.com/spanner/docs/non-relational/migrate-from-cassandra-to-spanner#compare-data"><span style="text-decoration: underline; vertical-align: baseline;">validate data accuracy and integrity</span></a><span style="vertical-align: baseline;">. Common methods include comparing row counts, sampling row data, or performing an exhaustive comparison between the Cassandra and Spanner data.</span></p>
</li>
</ul>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Switch your application to use the native endpoint for Cassandra:</strong><span style="vertical-align: baseline;"> We provide two options for doing this:</span></p>
</li>
</ol>
<ul>
<li>
<ul>
<li><strong style="vertical-align: baseline;">Embed the Spanner Cassandra Client Library:</strong><span style="vertical-align: baseline;"> For applications written in Java or Go, you can include the </span><a href="https://cloud.google.com/spanner/docs/non-relational/connect-cassandra-adapter"><span style="text-decoration: underline; vertical-align: baseline;">Spanner Cassandra client library</span></a><span style="vertical-align: baseline;"> as a dependency. The primary code modification involves changing how you build the </span><code style="vertical-align: baseline;">CqlSession</code><span style="vertical-align: baseline;"> object to use the </span><code style="vertical-align: baseline;">SpannerCqlSession.builder()</code><span style="vertical-align: baseline;"> and providing the Spanner database URI. Your existing data access logic using the </span><code style="vertical-align: baseline;">CqlSession</code><span style="vertical-align: baseline;"> interface remains unchanged.</span></li>
</ul>
</li>
</ul></div>
<div class="block-code"><dl>
    <dt>code_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;code&#x27;, &#x27;CqlSession session =\r\n    SpannerCqlSession.builder() \r\n        .setDatabaseUri(&quot;projects/your_gcp_project/instances/your_spanner_instance/databases/your_spanner_database&quot;)\r\n        .withKeyspace(&quot;your_spanner_database&quot;) // Corresponds to the Spanner database name\r\n        .withConfigLoader(\r\n            DriverConfigLoader.fromFile(new File(&quot;/path/to/config/file&quot;))) // Optional: path to driver config\r\n        .build();\r\n\r\n// Your existing code using the \&#x27;session\&#x27; object remains unchanged.&#x27;), (&#x27;language&#x27;, &#x27;&#x27;), (&#x27;caption&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e753f0f3280&gt;)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><ul>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Use the Spanner Cassandra sidecar proxy:</strong><span style="vertical-align: baseline;"> For applications where embedding the client library is not feasible or for connecting standard tools like </span><code style="vertical-align: baseline;">cqlsh</code><span style="vertical-align: baseline;">, you can deploy the </span><a href="https://github.com/googleapis/go-spanner-cassandra?tab=readme-ov-file#sidecar-proxy" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Spanner Cassandra sidecar proxy</span></a><span style="vertical-align: baseline;">. This proxy runs as a separate process, often containerized, alongside your application and handles the translation between the Cassandra wire protocol and Spanner's API.</span></p>
</li>
</ul>
<p><span style="vertical-align: baseline;">At this point, your application and data should be successfully migrated to Spanner. We have more detailed migration instructions available in </span><a href="https://cloud.google.com/spanner/docs/non-relational/migrate-from-cassandra-to-spanner"><span style="text-decoration: underline; vertical-align: baseline;">the documentation</span></a><span style="vertical-align: baseline;">.</span></p>
<h3><strong style="vertical-align: baseline;">Spanner is cost-effective for your Cassandra workloads</strong></h3>
<p><span style="vertical-align: baseline;">Now that you’ve migrated your workloads to Spanner, you may wonder about costs. In fact, Spanner is a cost-effective alternative to Cassandra. Here’s why:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Fully managed service: </strong><span style="vertical-align: baseline;">This reduces your operational costs.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">License-Free, Usage-Based Billing:</strong><span style="vertical-align: baseline;"> Unlike Cassandra, where per-node licensing fees can accumulate, Spanner has no licensing fees. Spanner usage is billed hourly, meaning no upfront costs, and you only pay for what you use.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Elastic scaling with Autoscaler:</strong><span style="vertical-align: baseline;"> </span><a href="https://cloud.google.com/spanner/docs/managed-autoscaler"><span style="text-decoration: underline; vertical-align: baseline;">Spanner’s Autoscaler</span></a><span style="vertical-align: baseline;"> allows precise scaling according to workload requirements. Avoid over-provisioning for peak performance and pay only for the resources you consume.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Granular instances:</strong><span style="vertical-align: baseline;"> Start small with a Spanner granular instance (100 processing units) for as little as $65/month.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Built-in observability dashboards: </strong><span style="vertical-align: baseline;">No need to pay extra to store and analyze your usage logs. The Spanner console has everything you need.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Tiered storage:</strong><span style="vertical-align: baseline;"> For storage-heavy workloads, Spanner offers tiered storage options that can be up to 80% cheaper than solid-state drive (SSD) storage.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Incremental backups:</strong><span style="vertical-align: baseline;"> Benefit from substantial cost savings on backups with Spanner's incremental backups.</span></p>
</li>
</ul>
<p><span style="vertical-align: baseline;">A recent </span><a href="https://cloud.google.com/resources/content/forrester-spanner-tei-study"><span style="text-decoration: underline; vertical-align: baseline;">Total Economic Impact study by Forrester Consulting</span></a><span style="vertical-align: baseline;"> found that Spanner delivered a 132% ROI and $7.74 million in total benefits over three years for a composite organization representative of interviewed customers. These gains largely stemmed from retiring self-managed databases and leveraging Spanner’s elastic scalability and built-in, hands-free high availability operations.</span></p>
<h3><strong style="vertical-align: baseline;">Migrating to Spanner is just the starting point</strong></h3>
<p><span style="vertical-align: baseline;">Migrating your Cassandra application to Spanner is the first step to unlocking a world of new possibilities for your data. While your application continues interacting with Cassandra via CQL, you can build new microservices that access the same data. You can even consider moving your connecting ETL pipelines to Spanner leveraging Spanner's full multi-model capabilities. You can also implement sophisticated </span><strong style="vertical-align: baseline;">graph</strong><span style="vertical-align: baseline;"> queries to analyze relationships between users, utilize </span><strong style="vertical-align: baseline;">full-text search</strong><span style="vertical-align: baseline;"> across user profiles or product catalogs, or add </span><strong style="vertical-align: baseline;">vector embedding</strong><span style="vertical-align: baseline;"> support for recommendation engines or similarity searches, combining transactional data with AI/ML workloads directly. Lastly, Spanner is tightly integrated with BigQuery, allowing you to unlock valuable insights and drive better decision-making by connecting operational and analytical workloads. </span></p>
<p><span style="vertical-align: baseline;">Get started today by testing our migration kit via the </span><a href="https://cloud.google.com/spanner"><span style="text-decoration: underline; vertical-align: baseline;">Spanner free trial</span></a><span style="vertical-align: baseline;">. We believe this will empower next-gen application builders like you to modernize your Cassandra workloads and unlock the full potential of your data on Google Cloud with Spanner.</span></p></div>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/products/databases/database-center-is-now-generally-available/</id>
            <title>Simplify database fleet management with AI-powered Database Center, now GA</title>
            <link>https://cloud.google.com/blog/products/databases/database-center-is-now-generally-available/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/products/databases/database-center-is-now-generally-available/</guid>
            <pubDate></pubDate>
            <updated>Thu, 15 May 2025 16:00:00 +0000</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">At Google Cloud Next 25, we </span><a href="https://cloud.google.com/blog/products/databases/whats-new-for-google-cloud-databases-at-next25?e=48754805"><span style="text-decoration: underline; vertical-align: baseline;">announced</span></a><span style="vertical-align: baseline;"> the general availability of </span><a href="https://cloud.google.com/database-center/docs/overview"><span style="text-decoration: underline; vertical-align: baseline;">Database Center</span></a><span style="vertical-align: baseline;">, an AI-powered unified fleet management solution that simplifies all aspects of database fleet management including monitoring, optimization, and security. </span></p>
<p><span style="vertical-align: baseline;">Database Center replaces fragmented tools, complex scripts, APIs, and other error-prone workflows that companies use to monitor their database fleets, delivering an integrated experience that uses Google's foundation AI models for an intuitive chat interface and to make intelligent recommendations. </span></p>
<p><a href="https://cloud.google.com/blog/products/databases/database-center-preview-now-open-to-all-customers"><span style="text-decoration: underline; vertical-align: baseline;">In a previous post</span></a><span style="vertical-align: baseline;">, we described how Database Center is designed to bring order to the chaos of your database fleet, and unlock the true potential of your data. Its single, intuitive interface lets you:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Gain a comprehensive view of your entire database fleet. No more silos of information or hunting through bespoke tools and spreadsheets.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Proactively de-risk your fleet with intelligent performance, reliability, cost, compliance and security recommendations.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Optimize your database fleet with AI-powered assistance. Use a natural-language chat interface to ask questions and quickly resolve fleet issues and get optimization recommendations</span></p>
</li>
</ul></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="1" src="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/1_zYPWCL2.gif" />
        
        </a>
      
        <figcaption class="article-image__caption "><p>Single pane of glass, recent issues and custom views</p></figcaption>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">What’s new in Database Center</strong></h3>
<p><span style="vertical-align: baseline;">With general availability, Database Center now provides enhanced performance and health monitoring for all Google Cloud databases, including Cloud SQL, AlloyDB, Spanner, Bigtable, Memorystore, and Firestore. It delivers richer metrics and actionable recommendations, helps you to optimize database performance and reliability, and customize your experience. Database Center leverages Gemini to deliver assistive performance troubleshooting experience. Finally, you can track the weekly progress of your database inventory and health issues. </span></p>
<p><span style="vertical-align: baseline;">Here’s a detailed list of new features included in Database Center GA release: </span></p>
<ul>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Gemini assisted performance troubleshooting:</strong><span style="vertical-align: baseline;"> </span></p>
</li>
<ul>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Inefficient query/index advisor for Cloud SQL</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Analyze option for high resource utilization for Cloud SQL and AlloyDB</span></p>
</li>
</ul>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Enhanced recommendations </strong><span style="vertical-align: baseline;">across Bigtable, Spanner, MemoryStore, Firestore and Cloud SQL. List of new recommendations:</span></p>
</li>
<ul>
<li style="vertical-align: baseline;">
<p><strong style="font-style: italic; vertical-align: baseline;">Spanner </strong><span style="font-style: italic; vertical-align: baseline;">-</span><span style="font-style: italic; vertical-align: baseline;"> </span><span style="font-style: italic; vertical-align: baseline;">Deletion protection not enabled</span><span style="vertical-align: baseline;">, </span><span style="font-style: italic; vertical-align: baseline;">No automated backup policy, </span><span style="font-style: italic; vertical-align: baseline;">Last backup older than 24hrs</span></p>
</li>
<li style="font-style: italic; vertical-align: baseline;">
<p><strong style="font-style: italic; vertical-align: baseline;">Bigtable</strong><span style="font-style: italic; vertical-align: baseline;"> - Hotspot detected, High resource utilization, Nearing or at storage capacity, Deletion protection not enabled, Suspended resource </span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="font-style: italic; vertical-align: baseline;">Memorystore</strong><span style="vertical-align: baseline;"> - </span><span style="font-style: italic; vertical-align: baseline;">High resource utilization, Expensive command, Maintenance policy not set</span></p>
</li>
<li style="font-style: italic; vertical-align: baseline;">
<p><strong style="font-style: italic; vertical-align: baseline;">Firestore</strong><span style="font-style: italic; vertical-align: baseline;"> - PITR not enabled, No automated back-up policy</span></p>
</li>
<li style="font-style: italic; vertical-align: baseline;">
<p><strong style="font-style: italic; vertical-align: baseline;">Cloud SQL</strong><span style="font-style: italic; vertical-align: baseline;"> - No user password policy, Inefficient queries</span></p>
</li>
</ul>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Savable views: </strong><span style="vertical-align: baseline;">Let users create, save and share custom Database Center views for specific personas.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Historical data:</strong><span style="vertical-align: baseline;"> Enable users to track new database resources generated and issues occurred in the last week.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Alerting</strong><span style="vertical-align: baseline;">: Access a centralized interface to view all database altering policies and incidents.</span></p>
</li>
</ul></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="2" src="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/2_76V34q6.gif" />
        
        </a>
      
        <figcaption class="article-image__caption "><p>Assistive experience to detect and analyse performance issues</p></figcaption>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">What Database Center customers are saying </strong></h3>
<p><span style="vertical-align: baseline;">Customers who used the Database Center preview saw significant value in the unified fleet management solution.</span></p>
<p style="padding-left: 40px;"><span style="font-style: italic; vertical-align: baseline;">“As we continue to accelerate digital transformations, our databases must meet the pace of our development and the growing complexity of database environments that power business applications. We manage thousands of databases across hundreds of projects that are critical for Ford. Database Center has given us unparalleled insight into our database landscape. The robust monitoring and observability tools have allowed us to proactively identify and address potential issues before they impact our users. Now, we can get answers on fleet health in seconds and proactively mitigate risks to our applications more swiftly than before.” </span><span style="vertical-align: baseline;">- Bogdan Capatina, Technical Expert in Database Technologies, Ford Motor Company.</span></p>
<p style="padding-left: 40px;"><span style="font-style: italic; vertical-align: baseline;">“Database Center offers a comprehensive overview of the entire database landscape and serves as a central hub for navigating and taking action on any database within TELUS.” </span><span style="vertical-align: baseline;">- Kurt Harsft, Manager Information Services, Telus</span></p>
<p style="padding-left: 40px;"><span style="font-style: italic; vertical-align: baseline;">“Database Center perfectly aligns with our vision of database observability — delivering a single, unified view just like we monitor our on-prem databases. It simplifies management, strengthens security compliance, and sets the standard for how database oversight should be: seamless and comprehensive.” </span><span style="vertical-align: baseline;">- Sangeeth Talluri, Senior Principal Architect, Home Depot</span></p>
<p style="padding-left: 40px;"><span style="font-style: italic; vertical-align: baseline;">“The ability to effortlessly access comprehensive infrastructure health, security, compliance, alerts, and data protection across the entire fleet through the Database Center is truly a game-changer, providing exceptional value and peace of mind.” </span><span style="vertical-align: baseline;">- Rahul Jain, Engineering Manager, UKG</span></p>
<h3><strong style="vertical-align: baseline;">How you can access Database Center</strong></h3>
<p><span style="vertical-align: baseline;">Database Center is </span><strong style="vertical-align: baseline;">accessible from Google Cloud managed database services console:</strong><span style="vertical-align: baseline;"> Cloud SQL, AlloyDB, Spanner and Bigtable. Database Center is </span><strong style="vertical-align: baseline;">enabled by default</strong><span style="vertical-align: baseline;"> for users with the necessary IAM permissions at the desired hierarchy.</span></p>
<p><span style="vertical-align: baseline;">Google Cloud customers </span><strong style="vertical-align: baseline;">do not have to pay anything additional </strong><span style="vertical-align: baseline;">to use Database Center. Certain premium features, including those that are Gemini-backed, like few performance recommenders, cost recommenders and natural language chat require customers to enable </span><a href="https://cloud.google.com/products/gemini/cloud-assist"><span style="text-decoration: underline; vertical-align: baseline;">Gemini Cloud Assist</span></a><span style="vertical-align: baseline;">. Advanced security and compliance monitoring requires a Google Security Command Central (SCC) subscription.</span><strong style="vertical-align: baseline;"> </strong></p>
<p><strong style="vertical-align: baseline;">Get started with Database Center today:</strong></p>
<ul>
<li style="vertical-align: baseline;">
<p><a href="https://console.cloud.google.com/database-center"><span style="text-decoration: underline; vertical-align: baseline;">Access Database Center in Google Cloud console </span></a></p>
</li>
<li style="vertical-align: baseline;">
<p><a href="https://cloud.google.com/database-center/docs/overview"><span style="text-decoration: underline; vertical-align: baseline;">Review the documentation to learn more</span></a></p>
</li>
</ul></div>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/topics/partners/upgrades-to-google-cloud-marketplace-for-partners/</id>
            <title>Google Cloud Marketplace simplifies deals and improves economics</title>
            <link>https://cloud.google.com/blog/topics/partners/upgrades-to-google-cloud-marketplace-for-partners/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/topics/partners/upgrades-to-google-cloud-marketplace-for-partners/</guid>
            <pubDate></pubDate>
            <updated>Thu, 15 May 2025 16:00:00 +0000</updated>
                
                
            <media:content url="https://storage.googleapis.com/gweb-cloudblog-publish/images/google_cloud_marketplace.max-600x600.jpg"/>
                
            <content:encoded>
                <![CDATA[
                    
                    
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/topics/developers-practitioners/read-doras-latest-research-on-software-excellence/</id>
            <title>Unlock software delivery excellence and quality with Gemini Code Assist agents</title>
            <link>https://cloud.google.com/blog/topics/developers-practitioners/read-doras-latest-research-on-software-excellence/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/topics/developers-practitioners/read-doras-latest-research-on-software-excellence/</guid>
            <pubDate></pubDate>
            <updated>Wed, 14 May 2025 16:00:00 +0000</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">According to DORA’s latest research – the </span><a href="https://dora.dev/dora-report-gen-ai/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Impact of Generative AI in Software Development report</span></a><span style="vertical-align: baseline;"> – AI tools are making software developers feel more productive, focused, and satisfied. They're even writing better code and documentation more quickly. But the research uncovered a paradox: these individual gains </span><span style="font-style: italic; vertical-align: baseline;">do not</span><span style="vertical-align: baseline;"> translate directly into improved system-level performance. In fact, DORA's platform-agnostic research found that increased AI adoption is correlated with:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Decreased software delivery throughput:</strong><span style="vertical-align: baseline;"> A 25% increase in AI adoption is associated with a 1.5% reduction in delivery throughput.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Lower software delivery stability:</strong><span style="vertical-align: baseline;"> A 25% increase in AI adoption is associated with a 7.2% reduction in delivery stability.</span></p>
</li>
</ul>
<p><span style="vertical-align: baseline;">This presents a puzzle. How can AI simultaneously boost individual productivity and negatively impact overall system performance?</span></p>
<p><span style="vertical-align: baseline;">According to the research, the key is this: more code isn’t always better. This doesn't mean AI is inherently detrimental to software delivery. It highlights the importance of </span><span style="font-style: italic; vertical-align: baseline;">strategic</span><span style="vertical-align: baseline;"> AI adoption. Focusing solely on code generation, without addressing other constraints in the system, is like adding more lanes to a highway that ends in a single-lane tunnel. Traffic will simply pile up.</span></p>
<p><span style="vertical-align: baseline;"> In this post, we’ll explore how the research suggests organizations can unlock software delivery excellence. </span></p></div>
<div class="block-aside"><dl>
    <dt>aside_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;title&#x27;, &#x27;$300 in free credit to try Google Cloud developer tools&#x27;), (&#x27;body&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e75413284f0&gt;), (&#x27;btn_text&#x27;, &#x27;Start building for free&#x27;), (&#x27;href&#x27;, &#x27;http://console.cloud.google.com/freetrial?redirectPath=/welcome&#x27;), (&#x27;image&#x27;, None)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">Gemini Code Assist agents</strong></h3>
<p><a href="https://developers.google.com/gemini-code-assist/docs/review-github-code" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Gemini Code Assist for GitHub</span></a><span style="vertical-align: baseline;">, released on February 25 in a public preview as part of </span><a href="https://developers.google.com/gemini-code-assist/docs/write-code-gemini" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Gemini Code Assist for Individuals</span></a><span style="vertical-align: baseline;">, meets developers in the pull request workflow. This code review agent brings AI-assistance to your code review process, enhancing </span><a href="https://dora.dev/capabilities/code-maintainability/" rel="noopener" target="_blank"><span style="font-style: italic; text-decoration: underline; vertical-align: baseline;">code maintainability</span></a><span style="vertical-align: baseline;">, reviewing code against team-specific best practices, and supporting </span><a href="https://dora.dev/capabilities/continuous-integration/" rel="noopener" target="_blank"><span style="font-style: italic; text-decoration: underline; vertical-align: baseline;">continuous integration</span></a><span style="vertical-align: baseline;">. This means developers have more benefits, such as:</span></p>
<ol>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Improved documentation quality</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Fast feedback</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Easy migration of code bases</span></p>
</li>
</ol>
<p><span style="vertical-align: baseline;">Any developer hosting repositories on GitHub can </span><a href="https://github.com/marketplace/gemini-code-assist" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">sign-up and install the Gemini Code Assist for GitHub</span></a><span style="vertical-align: baseline;"> from the GitHub marketplace today and learn more about the agent in our </span><a href="https://developers.google.com/gemini-code-assist/docs/review-github-code" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">documentation</span></a><span style="vertical-align: baseline;">. </span></p>
<p><strong style="vertical-align: baseline;">Improve documentation quality</strong></p>
<p><a href="https://dora.dev/capabilities/documentation-quality/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">High-quality documentation</span></a><span style="vertical-align: baseline;"> is consistently linked in DORA research to improved team performance, organizational performance, and developer well-being. The agent helps improve documentation quality by:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Automating documentation generation and updates.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Providing instant answers to developer questions about the codebase.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Facilitating knowledge sharing and accelerates onboarding.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Reducing tech debt (as the agent documents, it improves understanding).</span></p>
</li>
</ul>
<p><span style="vertical-align: baseline;">By automating this often-tedious task, developers can focus on higher-value work, improving flow and job satisfaction.</span></p>
<p><strong style="vertical-align: baseline;">Provide fast feedback</strong></p>
<p><span style="vertical-align: baseline;">Fast, quality feedback loops help build confidence and trust in new tools and processes. Feedback also helps individuals, teams, and organizations change tactics when they see that things aren't working as expected. Improving </span><a href="https://dora.dev/capabilities/test-automation/" rel="noopener" target="_blank"><span style="font-style: italic; text-decoration: underline; vertical-align: baseline;">test automation</span></a><span style="vertical-align: baseline;"> and </span><a href="https://dora.dev/capabilities/test-data-management/" rel="noopener" target="_blank"><span style="font-style: italic; text-decoration: underline; vertical-align: baseline;">test data management</span></a><span style="vertical-align: baseline;"> helps teams get feedback faster and "shift left" on quality by:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Generating comprehensive test cases.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Prioritizing tests based on risk assessment.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Identifying and flagging flaky tests.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Reducing the reliance on manual testing.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Increasing code coverage</span></p>
</li>
</ul>
<p><span style="vertical-align: baseline;">Automated testing can reduce change failure rates and improve overall software reliability.</span></p>
<p><strong style="vertical-align: baseline;">Migrate code bases</strong></p>
<p><span style="vertical-align: baseline;">Keeping applications updated is required to maintain security, reliability, and maintainability. However, upgrades and migrations can be risky, time consuming, and difficult to prioritize. The agent can streamline the transition from old systems and dependencies by:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Rewriting code from one language, framework, or version to another.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Providing a full set of test cases to ensure the new implementation is correct.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Offering comprehensive review to catch any errors that might have been missed.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Delivering a set of documentation for the new implementation, saving you time and effort.</span></p>
</li>
</ul>
<h3><strong style="vertical-align: baseline;">Use Gemini Code Assist agents to reduce technical debt </strong></h3>
<p><span style="vertical-align: baseline;">Now that we have introduced you to some ways Gemini Code Assist agents can impact your SDLC, let’s look at a specific example every software team faces: Managing technical debt. </span></p>
<p><span style="vertical-align: baseline;">We've built Gemini Code Assist to be more than just a coding companion—it's a strategic partner in conquering technical debt and building a foundation for sustainable software delivery.</span></p>
<p><span style="vertical-align: baseline;">Technical debt isn't about </span><span style="font-style: italic; vertical-align: baseline;">bad</span><span style="vertical-align: baseline;"> code, per se. It's about the </span><span style="font-style: italic; vertical-align: baseline;">consequences</span><span style="vertical-align: baseline;"> of prioritizing speed over perfect design or taking shortcuts to meet immediate needs. It can be the old library that no one has time to update, the duplicated code that's faster to copy-paste than refactor, the "temporary" workaround that becomes permanent, or the documentation that never gets written. It can arise from a lack of understanding, poor training, or even the unexpected retirement of a tool or service. It manifests in many forms. In fact, </span><a href="https://ieeexplore.ieee.org/document/10109339" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">researchers from Google identified ten key categories</span></a><span style="vertical-align: baseline;">.</span></p>
<p><span style="vertical-align: baseline;">The impact of this accumulated debt is profound. </span><a href="https://dora.dev/research/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">DORA's research</span></a><span style="vertical-align: baseline;"> consistently demonstrates a strong correlation between technical debt and </span><span style="font-style: italic; vertical-align: baseline;">decreased</span><span style="vertical-align: baseline;"> </span><a href="https://dora.dev/guides/dora-metrics-four-keys/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">software delivery performance</span></a><span style="vertical-align: baseline;">. Technical debt doesn't just slow down development; it actively undermines an organization's ability to innovate, adapt, and compete.</span></p>
<h2><span style="vertical-align: baseline;">Strategies for paying down technical debt</span></h2>
<p><span style="vertical-align: baseline;">Just as financial debt requires a deliberate repayment plan, technical debt needs a strategic approach to remediation. There's no magic bullet, but DORA's research and industry practices point to several key strategies:</span></p>
<ol>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Prioritize and visualize:</strong><span style="vertical-align: baseline;"> The first step is making the debt </span><span style="font-style: italic; vertical-align: baseline;">visible</span><span style="vertical-align: baseline;">. Use tools like code analysis platforms, dependency trackers, and documentation coverage reports to quantify the problem. </span><a href="https://dora.dev/guides/value-stream-management/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Value Stream Mapping</span></a><span style="vertical-align: baseline;"> (VSM) is invaluable for identifying bottlenecks and areas of waste caused by technical debt.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Embrace continuous improvement:</strong><span style="vertical-align: baseline;"> Technical debt reduction should be an ongoing process, not a one-time project. Integrate it into your daily work.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Empower teams:</strong><span style="vertical-align: baseline;"> Give teams the autonomy and resources they need to address technical debt. </span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Modernize teams and architecture:</strong><span style="vertical-align: baseline;"> Migrate to </span><a href="https://dora.dev/capabilities/loosely-coupled-teams/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">loosely coupled teams</span></a><span style="vertical-align: baseline;"> and architecture to reduce dependencies and make it easier to isolate and address technical debt in specific components.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Establish clear change management practices.</strong><span style="vertical-align: baseline;"> Reduce bottlenecks and wait times associated with change by following a streamlined, clear, and collaborative change process, focused on reviews rather than blocking.</span></p>
</li>
</ol>
<p><span style="vertical-align: baseline;">These strategies, when implemented consistently, create a virtuous cycle. Reducing technical debt frees up developer time and energy, allowing them to focus on building new features and further improving the system.</span></p>
<h3><strong style="vertical-align: baseline;">Gemini Code Assist:  Your ally in the fight against technical debt</strong></h3>
<p><span style="vertical-align: baseline;">By focusing on the entire SDLC, not just code generation, and by providing specialized capabilities for tasks like code review, documentation, and testing, Gemini Code Assist helps ensure that AI-generated code is thoroughly vetted and integrated responsibly. This reduces the likelihood of introducing errors and accumulating technical debt, leading to higher quality and more maintainable code.</span></p>
<p><span style="vertical-align: baseline;">You can customize the behavior of Gemini Code Assist to align with your organization's specific coding environments, standards, preferred styles, and established best practices. Google Cloud is committed to a continuous improvement approach, guided by DORA's research. We encourage our customers to:</span></p>
<ol>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Identify bottlenecks:</strong><span style="vertical-align: baseline;"> Use </span><a href="https://dora.dev/guides/dora-metrics-four-keys/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">DORA's four key metrics</span></a><span style="vertical-align: baseline;"> and other performance indicators to pinpoint the constraints in your SDLC.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Plan targeted improvements:</strong><span style="vertical-align: baseline;"> Select specific </span><a href="https://dora.dev/capabilities/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">DORA capabilities</span></a><span style="vertical-align: baseline;"> to enhance, leveraging Gemini AI agents where appropriate.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Experiment and measure:</strong><span style="vertical-align: baseline;"> Implement changes and rigorously track their impact on your performance metrics.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Adapt and iterate:</strong><span style="vertical-align: baseline;"> Based on the data, refine your approach, continuously seeking improvement.</span></p>
</li>
</ol>
<p><span style="vertical-align: baseline;">This iterative, data-driven approach, combined with the power of Gemini Code Assist agents, allows organizations to move beyond the hype and realize the true potential of AI in software development.</span></p>
<h3><strong style="vertical-align: baseline;">Try Gemini Code Assist today</strong></h3>
<p><span style="vertical-align: baseline;">The future of software development is a partnership between human ingenuity and artificial intelligence. Gemini AI agents are designed to be that partner.</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Try</strong><a href="https://codeassist.google/" rel="noopener" target="_blank"><strong style="text-decoration: underline; vertical-align: baseline;"> Gemini Code Assist today</strong></a><span style="vertical-align: baseline;">. Experience how our AI agents can transform your software delivery process.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><a href="https://cloud.google.com/gemini/docs/codeassist/overview"><strong style="text-decoration: underline; vertical-align: baseline;">Explore Gemini Code Assist further with our documentation</strong></a><strong style="vertical-align: baseline;">.</strong><span style="vertical-align: baseline;"> See the product's capabilities.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><a href="https://dora.dev/publications/" rel="noopener" target="_blank"><strong style="text-decoration: underline; vertical-align: baseline;">Read the DORA research</strong></a><span style="vertical-align: baseline;"> to dive deeper into the science behind high-performing teams.</span></p>
</li>
</ul>
<p><span style="vertical-align: baseline;">Don't just write code faster. Build better software, deliver it more reliably, and empower your teams to achieve their full potential.</span></p></div>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/topics/developers-practitioners/use-google-adk-and-mcp-with-an-external-server/</id>
            <title>A guide to Google ADK and MCP integration with an external server</title>
            <link>https://cloud.google.com/blog/topics/developers-practitioners/use-google-adk-and-mcp-with-an-external-server/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/topics/developers-practitioners/use-google-adk-and-mcp-with-an-external-server/</guid>
            <pubDate></pubDate>
            <updated>Wed, 14 May 2025 16:00:00 +0000</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">For AI-powered agents to perform useful, real-world tasks, they need to reliably access tools and up-to-the-minute information that lives outside the base model. Anthropic’s Model Context Protocol (MCP) is designed to address this, providing a standardized way for agents to retrieve that crucial, external context needed to inform their responses and actions.</span></p>
<p><span style="vertical-align: baseline;">This is vital for developers who need to build and deploy sophisticated agents that can leverage enterprise data or public tools. But integrating agents built with Google's Agent Development Kit (ADK) to communicate effectively with an MCP server, especially one hosted externally, might present some integration challenges.</span></p>
<p><span style="vertical-align: baseline;">Today, we’ll guide you through developing ADK agents that connect to external MCP servers, initially using Server-Sent Events (SSE). We’ll take an example of an ADK agent leveraging MCP to access Wikipedia articles, which is a common use case to retrieve external specialised data. We will also introduce Streamable HTTP, the next-generation transport protocol designed to succeed SSE for MCP communications.</span></p>
<h3><strong style="vertical-align: baseline;">A quick refresher</strong></h3>
<p><span style="vertical-align: baseline;">Before we start, let's make sure we all understand the following terms:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">SSE </strong><span style="vertical-align: baseline;">enables servers to push data to clients over a persistent HTTP connection. In a typical setup for MCP, this involved using two distinct endpoints: one for the client to send requests to the server (usually via HTTP POST) and a separate endpoint where the client would establish an SSE connection (HTTP GET) to receive streaming responses and server-initiated messages.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">MCP</strong><span style="vertical-align: baseline;"> is an open standard designed to standardize how Large Language Models (LLMs) interact with external data sources, APIs and resources as agent tools, MCP aims to replace the current landscape of fragmented, custom integrations with a universal, standardized framework.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Streamable HTTP</strong><span style="vertical-align: baseline;"> utilizes a single HTTP endpoint for both sending requests from the client to the server, and receiving responses and notifications from the server to the client.</span></p>
</li>
</ul></div>
<div class="block-aside"><dl>
    <dt>aside_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;title&#x27;, &#x27;$300 in free credit to try Google Cloud developer tools&#x27;), (&#x27;body&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e753fffc100&gt;), (&#x27;btn_text&#x27;, &#x27;Start building for free&#x27;), (&#x27;href&#x27;, &#x27;http://console.cloud.google.com/freetrial?redirectPath=/welcome&#x27;), (&#x27;image&#x27;, None)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">Step 1: Create an MCP server </strong></h3>
<p><span style="vertical-align: baseline;">You need the following python packages installed in your virtual environment before proceeding. We will be using the </span><a href="https://docs.astral.sh/uv/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">uv tool</span></a><span style="vertical-align: baseline;"> in this blog.</span></p></div>
<div class="block-code"><dl>
    <dt>code_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;code&#x27;, &#x27;&quot;beautifulsoup4==4.12.3&quot;,\r\n&quot;google-adk==0.3.0&quot;,\r\n&quot;html2text==2024.2.26&quot;,\r\n&quot;mcp[cli]==1.5.0&quot;,\r\n&quot;requests==2.32.3&quot;&#x27;), (&#x27;language&#x27;, &#x27;lang-py&#x27;), (&#x27;caption&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e753fffceb0&gt;)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Here’s an explanation of the Python code </span><code style="vertical-align: baseline;">server.py</code><span style="vertical-align: baseline;">:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">It creates an instance of an MCP server using </span><code style="vertical-align: baseline;">FastMCP</code></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">It defines a tool called </span><code style="vertical-align: baseline;">extract_wikipedia_article</code><span style="vertical-align: baseline;"> decorated with </span><code style="vertical-align: baseline;">@mcp.tool</code></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">It configures an SSE transport mechanism </span><code style="vertical-align: baseline;">SseServerTransport</code><span style="vertical-align: baseline;"> to enable real-time communication, typically for the MCP server interactions.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">It creates a web application instance using the </span><a href="https://www.starlette.io/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Starlette framework</span></a><span style="vertical-align: baseline;"> and defines two routes, </span><code style="vertical-align: baseline;">message</code><span style="vertical-align: baseline;"> and </span><code style="vertical-align: baseline;">sse</code><span style="vertical-align: baseline;">.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">You can read more about SSE transport protocol </span><a href="https://modelcontextprotocol.io/specification/2024-11-05/basic/transports#http-with-sse" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">here</span></a><span style="vertical-align: baseline;">.</span></p>
</li>
</ul></div>
<div class="block-code"><dl>
    <dt>code_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;code&#x27;, &#x27;# File server.py\r\n\r\nimport requests\r\nfrom requests.exceptions import RequestException\r\nfrom bs4 import BeautifulSoup\r\nfrom html2text import html2text\r\n\r\nimport uvicorn\r\nfrom starlette.applications import Starlette\r\nfrom starlette.requests import Request\r\nfrom starlette.routing import Route, Mount\r\n\r\nfrom mcp.server.fastmcp import FastMCP\r\nfrom mcp.shared.exceptions import McpError\r\nfrom mcp.types import ErrorData, INTERNAL_ERROR, INVALID_PARAMS\r\nfrom mcp.server.sse import SseServerTransport\r\n\r\n# Create an MCP server instance with an identifier (&quot;wiki&quot;)\r\nmcp = FastMCP(&quot;wiki&quot;)\r\n\r\n@mcp.tool()\r\ndef extract_wikipedia_article(url: str) -&gt; str:\r\n    &quot;&quot;&quot;\r\n    Retrieves and processes a Wikipedia article from the given URL, extracting\r\n    the main content and converting it to Markdown format.\r\n\r\n    Usage:\r\n        extract_wikipedia_article(&quot;https://en.wikipedia.org/wiki/Gemini_(chatbot)&quot;)\r\n    &quot;&quot;&quot;\r\n    try:\r\n        if not url.startswith(&quot;http&quot;):\r\n            raise ValueError(&quot;URL must begin with http or https protocol.&quot;)\r\n\r\n        response = requests.get(url, timeout=8)\r\n        if response.status_code != 200:\r\n            raise McpError(\r\n                ErrorData(\r\n                    code=INTERNAL_ERROR,\r\n                    message=f&quot;Unable to access the article. Server returned status: {response.status_code}&quot;\r\n                )\r\n            )\r\n        soup = BeautifulSoup(response.text, &quot;html.parser&quot;)\r\n        content_div = soup.find(&quot;div&quot;, {&quot;id&quot;: &quot;mw-content-text&quot;})\r\n        if not content_div:\r\n            raise McpError(\r\n                ErrorData(\r\n                    code=INVALID_PARAMS,\r\n                    message=&quot;The main article content section was not found at the specified Wikipedia URL.&quot;\r\n                )\r\n            )\r\n        markdown_text = html2text(str(content_div))\r\n        return markdown_text\r\n\r\n    except Exception as e:\r\n        raise McpError(ErrorData(code=INTERNAL_ERROR, message=f&quot;An unexpected error occurred: {str(e)}&quot;)) from e\r\n\r\nsse = SseServerTransport(&quot;/messages/&quot;)\r\n\r\nasync def handle_sse(request: Request) -&gt; None:\r\n    _server = mcp._mcp_server\r\n    async with sse.connect_sse(\r\n        request.scope,\r\n        request.receive,\r\n        request._send,\r\n    ) as (reader, writer):\r\n        await _server.run(reader, writer, _server.create_initialization_options())\r\n\r\napp = Starlette(\r\n    debug=True,\r\n    routes=[\r\n        Route(&quot;/sse&quot;, endpoint=handle_sse),\r\n        Mount(&quot;/messages/&quot;, app=sse.handle_post_message),\r\n    ],\r\n)\r\n\r\nif __name__ == &quot;__main__&quot;:\r\n    uvicorn.run(app, host=&quot;localhost&quot;, port=8001)&#x27;), (&#x27;language&#x27;, &#x27;lang-py&#x27;), (&#x27;caption&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e7540b11820&gt;)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><p><span><span style="vertical-align: baseline;">To start the server, you can run the command </span><code style="vertical-align: baseline;">uv run server.py</code><span style="vertical-align: baseline;">.</span></span></p>
<p><span style="vertical-align: baseline;">Bonus tip, to debug the server using </span><a href="https://modelcontextprotocol.io/docs/tools/inspector" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">MCP Inspector</span></a><span style="vertical-align: baseline;">, execute the command </span><code style="vertical-align: baseline;">uv run mcp dev server.py</code><span style="vertical-align: baseline;">.</span></p></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="01-mcp-inspector" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/01-mcp-inspector.max-1000x1000.png" />
        
        </a>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">Step 2: Attach the MCP server while creating ADK agents</strong></h3>
<p><span style="vertical-align: baseline;">The following explains the Python code in the file </span><code style="vertical-align: baseline;">agent.py</code><span style="vertical-align: baseline;">:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Uses </span><code style="vertical-align: baseline;">MCPToolset.from_server</code><span style="vertical-align: baseline;"> with </span><code style="vertical-align: baseline;">SseServerParams</code><span style="vertical-align: baseline;"> to establish a SSE connection to a URI endpoint. For this demo we will use </span><code style="vertical-align: baseline;">http://localhost:8001/sse</code><span style="vertical-align: baseline;">, but in production this would be a remote server.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Create an ADK Agent and call </span><code style="vertical-align: baseline;">get_tools_async</code><span style="vertical-align: baseline;"> to get the tools from the MCP server.</span></p>
</li>
</ul></div>
<div class="block-code"><dl>
    <dt>code_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;code&#x27;, &#x27;# File agent.py\r\n\r\nimport asyncio\r\nimport json\r\nfrom typing import Any\r\n\r\nfrom dotenv import load_dotenv\r\nfrom google.adk.agents.llm_agent import LlmAgent\r\nfrom google.adk.artifacts.in_memory_artifact_service import (\r\n    InMemoryArtifactService,  # Optional\r\n)\r\nfrom google.adk.runners import Runner\r\nfrom google.adk.sessions import InMemorySessionService\r\nfrom google.adk.tools.mcp_tool.mcp_toolset import (\r\n    MCPToolset,\r\n    SseServerParams,\r\n)\r\nfrom google.genai import types\r\nfrom rich import print\r\nload_dotenv()\r\n\r\nasync def get_tools_async():\r\n    &quot;&quot;&quot;Gets tools from the File System MCP Server.&quot;&quot;&quot;\r\n    tools, exit_stack = await MCPToolset.from_server(\r\n        connection_params=SseServerParams(\r\n            url=&quot;http://localhost:8001/sse&quot;,\r\n        )\r\n    )\r\n    print(&quot;MCP Toolset created successfully.&quot;)\r\n    return tools, exit_stack\r\n\r\nasync def get_agent_async():\r\n    &quot;&quot;&quot;Creates an ADK Agent equipped with tools from the MCP Server.&quot;&quot;&quot;\r\n    tools, exit_stack = await get_tools_async()\r\n    print(f&quot;Fetched {len(tools)} tools from MCP server.&quot;)\r\n    root_agent = LlmAgent(\r\n        model=&quot;gemini-2.0-flash&quot;,\r\n        name=&quot;assistant&quot;,\r\n        instruction=&quot;&quot;&quot;Help user extract and summarize the article from wikipedia link.\r\n        Use the following tools to extract wikipedia article:\r\n        - extract_wikipedia_article\r\n\r\n        Once you retrieve the article, always summarize it in a few sentences for the user.\r\n        &quot;&quot;&quot;,\r\n        tools=tools,\r\n    )\r\n    return root_agent, exit_stack\r\n\r\nroot_agent = get_agent_async()&#x27;), (&#x27;language&#x27;, &#x27;lang-py&#x27;), (&#x27;caption&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e7540b11730&gt;)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">Step 3: Test your agent</strong></h3>
<p><span style="vertical-align: baseline;">We will use the ADK developer tool to test the agent.</span></p>
<p><span style="vertical-align: baseline;">Create the following directory structure:</span></p></div>
<div class="block-code"><dl>
    <dt>code_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;code&#x27;, &#x27;. # &lt;--Your current directory\r\n├── adk-agent\r\n│   ├── __init__.py\r\n│   └── agent.py\r\n├── .env&#x27;), (&#x27;language&#x27;, &#x27;lang-py&#x27;), (&#x27;caption&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e7540b118b0&gt;)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">The content for </span><code style="vertical-align: baseline;">__init__.py</code><span style="vertical-align: baseline;"> and </span><code style="vertical-align: baseline;">.env</code><span style="vertical-align: baseline;"> are as follows:</span></p></div>
<div class="block-code"><dl>
    <dt>code_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;code&#x27;, &#x27;# .env\r\nGOOGLE_GENAI_USE_VERTEXAI=&quot;True&quot;\r\nGOOGLE_CLOUD_PROJECT=&lt;YOUR_PROJECT_ID&gt;\r\nGOOGLE_CLOUD_LOCATION=&quot;us-central1&quot;&#x27;), (&#x27;language&#x27;, &#x27;lang-py&#x27;), (&#x27;caption&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e7540b11e50&gt;)])]&gt;</dd>
</dl></div>
<div class="block-code"><dl>
    <dt>code_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;code&#x27;, &#x27;# __init__.py\r\nfrom . import agent&#x27;), (&#x27;language&#x27;, &#x27;lang-py&#x27;), (&#x27;caption&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e7540b11a00&gt;)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Start the UI with the following command:</span></p></div>
<div class="block-code"><dl>
    <dt>code_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;code&#x27;, &#x27;uv run adk web&#x27;), (&#x27;language&#x27;, &#x27;lang-py&#x27;), (&#x27;caption&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e7540b111c0&gt;)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">This will open up the ADK developer tool interface as shown below:</span></p></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="02-adk-web" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/02-adk-web.max-1000x1000.png" />
        
        </a>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">Streamable HTTP</strong></h3>
<p><span style="vertical-align: baseline;">It is worth noting that in March 2025, MCP released a new transport protocol called</span><a href="https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;"> Streamable HTTP</span></a><span style="vertical-align: baseline;">. The Streamable HTTP transport allows a server to function as an independent process managing multiple client connections via HTTP POST and GET requests. Servers can optionally implement Server-Sent Events (SSE) for streaming multiple messages, enabling support for basic MCP servers as well as more advanced servers with streaming and server-initiated communication.</span></p>
<p><span style="vertical-align: baseline;">The following code demonstrates how to implement a Streamable HTTP server, where the tool </span><code style="vertical-align: baseline;">extract_wikipedia_article</code><span style="vertical-align: baseline;"> will return a dummy string to simplify the code.</span></p></div>
<div class="block-code"><dl>
    <dt>code_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;code&#x27;, &#x27;# File server.py\r\n\r\nimport contextlib\r\nimport logging\r\nfrom collections.abc import AsyncIterator\r\n\r\nimport anyio\r\nimport mcp.types as types\r\nfrom mcp.server.lowlevel import Server\r\nfrom mcp.server.streamable_http_manager import StreamableHTTPSessionManager\r\nfrom starlette.applications import Starlette\r\nfrom starlette.routing import Mount\r\nfrom starlette.types import Receive, Scope, Send\r\nimport uvicorn\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\napp = Server(&quot;mcp-streamable-http-stateless-demo&quot;)\r\n\r\n\r\n@app.call_tool()\r\nasync def call_tool(\r\n    name: str, arguments: dict\r\n) -&gt; list[types.TextContent | types.ImageContent | types.EmbeddedResource]:\r\n    # Check if the tool is extract-wikipedia-article\r\n    if name == &quot;extract-wikipedia-article&quot;:\r\n        # Return dummy content for the Wikipedia article\r\n        return [\r\n            types.TextContent(\r\n                type=&quot;text&quot;,\r\n                text=&quot;This is the article ...&quot;,\r\n            )\r\n        ]\r\n\r\n    # For other tools, keep the existing notification logic\r\n    ctx = app.request_context\r\n    interval = arguments.get(&quot;interval&quot;, 1.0)\r\n    count = arguments.get(&quot;count&quot;, 5)\r\n    caller = arguments.get(&quot;caller&quot;, &quot;unknown&quot;)\r\n\r\n    # Send the specified number of notifications with the given interval\r\n    for i in range(count):\r\n        await ctx.session.send_log_message(\r\n            level=&quot;info&quot;,\r\n            data=f&quot;Notification {i + 1}/{count} from caller: {caller}&quot;,\r\n            logger=&quot;notification_stream&quot;,\r\n            related_request_id=ctx.request_id,\r\n        )\r\n        if i &lt; count - 1:  # Don\&#x27;t wait after the last notification\r\n            await anyio.sleep(interval)\r\n\r\n    return [\r\n        types.TextContent(\r\n            type=&quot;text&quot;,\r\n            text=(\r\n                f&quot;Sent {count} notifications with {interval}s interval&quot;\r\n                f&quot; for caller: {caller}&quot;\r\n            ),\r\n        )\r\n    ]\r\n\r\n\r\n@app.list_tools()\r\nasync def list_tools() -&gt; list[types.Tool]:\r\n    return [\r\n        types.Tool(\r\n            name=&quot;extract-wikipedia-article&quot;,\r\n            description=(&quot;Extracts the main content of a Wikipedia article&quot;),\r\n            inputSchema={\r\n                &quot;type&quot;: &quot;object&quot;,\r\n                &quot;required&quot;: [&quot;url&quot;],\r\n                &quot;properties&quot;: {\r\n                    &quot;url&quot;: {\r\n                        &quot;type&quot;: &quot;string&quot;,\r\n                        &quot;description&quot;: &quot;URL of the Wikipedia article to extract&quot;,\r\n                    },\r\n                },\r\n            },\r\n        )\r\n    ]\r\n\r\n\r\nsession_manager = StreamableHTTPSessionManager(\r\n    app=app,\r\n    event_store=None,\r\n    stateless=True,\r\n)\r\n\r\n\r\nasync def handle_streamable_http(scope: Scope, receive: Receive, send: Send) -&gt; None:\r\n    await session_manager.handle_request(scope, receive, send)\r\n\r\n\r\n@contextlib.asynccontextmanager\r\nasync def lifespan(app: Starlette) -&gt; AsyncIterator[None]:\r\n    &quot;&quot;&quot;Context manager for session manager.&quot;&quot;&quot;\r\n    async with session_manager.run():\r\n        logger.info(&quot;Application started with StreamableHTTP session manager!&quot;)\r\n        try:\r\n            yield\r\n        finally:\r\n            logger.info(&quot;Application shutting down...&quot;)\r\n\r\n\r\napp = Starlette(\r\n    debug=True,\r\n    routes=[\r\n        Mount(&quot;/mcp&quot;, app=handle_streamable_http),\r\n    ],\r\n    lifespan=lifespan,\r\n)\r\n\r\nif __name__ == &quot;__main__&quot;:\r\n    uvicorn.run(app, host=&quot;localhost&quot;, port=3000)&#x27;), (&#x27;language&#x27;, &#x27;lang-py&#x27;), (&#x27;caption&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e7540b117f0&gt;)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">You can start the Streamable HTTP MCP server using by running the following:</span></p></div>
<div class="block-code"><dl>
    <dt>code_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;code&#x27;, &#x27;# Start the server\r\nuv run server.py&#x27;), (&#x27;language&#x27;, &#x27;lang-py&#x27;), (&#x27;caption&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e7540b11670&gt;)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">To debug with </span><a href="https://modelcontextprotocol.io/docs/tools/inspector" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">MCP Inspector</span></a><span style="vertical-align: baseline;">, select </span><code style="vertical-align: baseline;">Streamable HTTP</code><span style="vertical-align: baseline;"> and fill in the MCP Server URL </span><a href="http://localhost:3000/mcp" rel="noopener" target="_blank"><code style="text-decoration: underline; vertical-align: baseline;">http://localhost:3000/mcp</code></a><span style="vertical-align: baseline;">.</span></p></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="03-streamable-http" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/03-streamable-http.max-1000x1000.png" />
        
        </a>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">Authentication</strong></h3>
<p><span style="vertical-align: baseline;">For production deployments of MCP servers, robust authentication is a critical security consideration. As this field is under active development at the time of writing, we recommend referring to the </span><a href="https://modelcontextprotocol.io/specification/2025-03-26/basic/authorization" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">MCP specification on Authentication</span></a><span style="vertical-align: baseline;"> for more information.</span></p>
<p><span style="vertical-align: baseline;">For an enterprise grade API governance system which, similar to MCP, can generate agent tools:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Apigee centralizes and manages any APIs, with full control, versioning, and governance </span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">API Hub organizes metadata for any API and documentation </span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Application Integrations support many existing API connections with user access control support </span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">ADK supports these </span><a href="https://google.github.io/adk-docs/tools/google-cloud-tools/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Google Cloud Managed Tools</span></a><span style="vertical-align: baseline;"> with about the same number of lines of code</span></p>
</li>
</ul>
<h3><span style="vertical-align: baseline;">Get started </span></h3>
<p><span style="vertical-align: baseline;">To get started today, read the </span><a href="https://google.github.io/adk-docs/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">documentation</span></a><span style="vertical-align: baseline;"> for ADK. You can create your own Agent with access to available MCP servers in the open community with ADK.</span></p></div>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/topics/training-certifications/new-google-cloud-certification-in-generative-ai/</id>
            <title>Google Cloud announces first-of-its-kind generative AI leader certification</title>
            <link>https://cloud.google.com/blog/topics/training-certifications/new-google-cloud-certification-in-generative-ai/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/topics/training-certifications/new-google-cloud-certification-in-generative-ai/</guid>
            <pubDate></pubDate>
            <updated>Wed, 14 May 2025 13:00:00 +0000</updated>
                
                
            <media:content url="https://storage.googleapis.com/gweb-cloudblog-publish/images/GAIL_Blog_header.max-600x600.png"/>
                
            <content:encoded>
                <![CDATA[
                    
                    
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/topics/customers/cool-stuff-google-cloud-customers-built-monthly-round-up/</id>
            <title>Cool stuff customers built, May edition: Visual scouts, racing agents, agile ads &amp; more</title>
            <link>https://cloud.google.com/blog/topics/customers/cool-stuff-google-cloud-customers-built-monthly-round-up/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/topics/customers/cool-stuff-google-cloud-customers-built-monthly-round-up/</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 May 2025 16:00:00 +0000</updated>
                
                
            <media:content url="https://storage.googleapis.com/gweb-cloudblog-publish/images/cool_things_customers_built_may25.max-600x600.jpg"/>
                
            <content:encoded>
                <![CDATA[
                    
                    
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/products/ai-machine-learning/evaluate-your-gen-media-models-on-vertex-ai/</id>
            <title>Evaluate your gen media models with multimodal evaluation on Vertex AI</title>
            <link>https://cloud.google.com/blog/products/ai-machine-learning/evaluate-your-gen-media-models-on-vertex-ai/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/products/ai-machine-learning/evaluate-your-gen-media-models-on-vertex-ai/</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 May 2025 16:00:00 +0000</updated>
                
                
            <media:content url="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/blog_KnxMxBa.gif"/>
                
            <content:encoded>
                <![CDATA[
                    
                    
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/products/ai-machine-learning/open-source-enhancements-to-langchain-postgresql/</id>
            <title>Announcing open-source enhancements to LangChain PostgreSQL</title>
            <link>https://cloud.google.com/blog/products/ai-machine-learning/open-source-enhancements-to-langchain-postgresql/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/products/ai-machine-learning/open-source-enhancements-to-langchain-postgresql/</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 May 2025 16:00:00 +0000</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">At Google Cloud Next ‘25, we announced upgrades to the core </span><a href="https://github.com/langchain-ai/langchain-postgres" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">LangChain Postgres package</span><span style="vertical-align: baseline;"> and became a major contributor to the library.</span></a><span style="vertical-align: baseline;"> These improvements underscore our vision that every application developer is a gen AI developer – one that is empowered to build database-backed agentic gen AI applications leveraging open source tools. </span></p>
<p><span style="vertical-align: baseline;">LangChain is an open-source framework designed to simplify the development of agentic gen AI applications powered by large language models (LLMs). It provides interfaces for connecting LLMs to external data sources, enabling more context-aware and powerful AI applications. To effectively manage and retrieve information from structured data, LangChain often needs to interact with databases. The langchain-postgres package specifically provides integrations that allow LangChain to connect to and utilize PostgreSQL databases for tasks such as storing chat history, acting as a vector store for embeddings, and loading documents. This integration is crucial for building chatbots with memory, performing semantic searches, and leveraging existing relational data within LLM-powered applications.</span></p>
<p><span style="vertical-align: baseline;">Our updates bring optimized performance through asynchronous PostgreSQL drivers, faster SQL filtering via relational metadata columns, and robust connection pooling for enterprise-level scalability. Furthermore, we've integrated:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Vector index support so that developers can set up their vector database from LangChain</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Support for flexible database schemas to build more powerful applications that are easier to maintain </span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Enhanced LangChain vector store APIs with a clear separation of database setup and usage, adhering to the principle of least privilege, for improved security.</span></p>
</li>
</ul></div>
<div class="block-aside"><dl>
    <dt>aside_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;title&#x27;, &#x27;$300 in free credit to try Google Cloud AI and ML&#x27;), (&#x27;body&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e754136dd90&gt;), (&#x27;btn_text&#x27;, &#x27;Start building for free&#x27;), (&#x27;href&#x27;, &#x27;http://console.cloud.google.com/freetrial?redirectPath=/vertex-ai/&#x27;), (&#x27;image&#x27;, None)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">What’s new</strong></h3>
<p><strong style="vertical-align: baseline;">Enhanced security and connectivity</strong></p>
<p><span style="vertical-align: baseline;">Building robust and secure generative AI applications requires careful consideration of how your application interacts with the underlying data infrastructure. With our contributions to the LangChain Postgres package, we've prioritized enhanced security and efficient connectivity through several key improvements.</span></p>
<p><span style="vertical-align: baseline;">We have focused on adhering to the </span><a href="https://en.wikipedia.org/wiki/Principle_of_least_privilege" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">principle of least privilege</span></a><span style="vertical-align: baseline;">. The enhanced API now clearly separates the permissions required for database schema creation from those needed for routine application usage. This separation allows you to grant restrictive permissions to the application layer, limiting its ability to modify the underlying database structure. By isolating these responsibilities, you significantly reduce the potential attack surface and enhance the overall security posture of your AI applications.</span></p>
<p><span style="vertical-align: baseline;">Furthermore, by maintaining a pool of active database connections, we minimize the overhead of establishing new connections for each query. This not only leads to significant performance improvements, especially in high-throughput environments, but also contributes to the stability of your application by managing resource utilization effectively, ensuring that you don’t end up with dozens of unused active PostgreSQL connections. </span></p>
<p><strong style="vertical-align: baseline;">Improved schema design</strong></p>
<p><span style="vertical-align: baseline;">Previously, the langchain-postgres package only allowed for new schema creation with fixed table names and a single json metadata column, emulating the data model of purpose built vector databases. But one of the benefits of using PostgreSQL databases as a vector database is that you can leverage PostgreSQL’s rich querying capabilities to improve the quality of your vector search by using filters on non-vector columns. With our improvements to the LangChain postgres package, you can define distinct metadata columns so that you are able to combine vector search queries with SQL filters when you query your vector store. </span></p>
<p><span style="vertical-align: baseline;">If you have a pre-existing database schema for PostgreSQL, you can now leverage that data with the new LangChain PostgreSQL package without needing to migrate your data to a new schema, allowing you to transform your operational workload to an AI workload with just a few lines of code!</span></p></div>
<div class="block-code"><dl>
    <dt>code_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;code&#x27;, &#x27;from langchain_postgres import PGEngine, PGVectorStore\r\nfrom langchain_google_vertexai import VertexAIEmbeddings\r\n\r\nvectorstore = PGVectorStore.create_sync(\r\n    engine=engine,\r\n    table_name=&quot;products&quot;,\r\n    embedding_service=VertexAIEmbeddings(model_name=&quot;text-embedding-005&quot;),\r\n    metadata_columns=[&quot;color&quot;, &quot;price&quot;]\r\n)\r\n\r\nresults = vectorstore.similarity_search(&quot;maroon puffer jacket&quot;)\r\n# Filter your vector search using metadata fields\r\nresults = vectorstore.similarity_search(&quot;maroon puffer jacket&quot;, filter={&quot;price&quot;: {&quot;$lt&quot;: 200.0}})&#x27;), (&#x27;language&#x27;, &#x27;lang-py&#x27;), (&#x27;caption&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e754136d8b0&gt;)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><p><strong style="vertical-align: baseline;">Production-ready features</strong></p>
<p><span style="vertical-align: baseline;">To support LangChain applications that can scale to production we created first class integrations for asynchronous drivers in the LangChain package and introduced vector index management. Asynchronous drivers allow you to leverage non-blocking I/O operations, leading to significant performance gains. This allows your application to handle a greater volume of concurrent requests with efficiency, scaling effectively while minimizing resource consumption and maximizing responsiveness.</span></p>
<p><span style="vertical-align: baseline;">Furthermore, we've integrated the ability to create and maintain vector indexes directly from within LangChain. This empowers you to adopt an infrastructure-as-code approach for your vector search, enabling you to define and deploy your entire application stack, from database schema to vector index configuration, using LangChain. This end-to-end integration streamlines the development process, allowing for seamless setup and management of AI-powered applications leveraging the speed of asynchronous operations and the power of vector search all from LangChain.</span></p>
<p><span style="vertical-align: baseline;">These are enhancements we previously made to our own LangChain packages for Google Cloud databases, We extracted them from our packages, upstreamed and published those changes into the LangChain PostgreSQL package, allowing developers on any platform to leverage our improvements. As databases have become more and more crucial to Generative AI applications, allowing users to ground LLMs, serving as knowledge bases for RAG applications, and powering high quality vector search, it is increasingly important for software libraries to have high quality integrations with databases so you can capitalize on your data. </span></p>
<h3><strong style="vertical-align: baseline;">Get started</strong></h3>
<p><span style="vertical-align: baseline;">Download the </span><a href="https://github.com/langchain-ai/langchain-postgres" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">langchain-postgres</span></a><span style="vertical-align: baseline;"> package today, and get started with a </span><a href="https://github.com/langchain-ai/langchain-postgres/blob/main/examples/pg_vectorstore.ipynb" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">quickstart application</span></a><span style="vertical-align: baseline;">! </span><a href="https://github.com/langchain-ai/langchain-postgres/blob/main/examples/migrate_pgvector_to_pgvectorstore.ipynb" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Follow this tutorial </span></a><span style="vertical-align: baseline;">to migrate from the prior version of the langchain-postgres to Google’s langchain-postgres package. Use </span><a href="https://github.com/googleapis/langchain-google-alloydb-pg-python" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">AlloyDB</span></a><span style="vertical-align: baseline;"> and </span><a href="https://github.com/googleapis/langchain-google-cloud-sql-pg-python" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Cloud SQL for PostgreSQL</span></a><span style="vertical-align: baseline;">’s LangChain package to leverage GCP specific features such as AlloyDB AI’s </span><a href="https://cloud.google.com/blog/products/databases/understanding-the-scann-index-in-alloydb"><span style="text-decoration: underline; vertical-align: baseline;">ScaNN index</span></a><span style="vertical-align: baseline;">. Get started building agentic applications with </span><a href="https://github.com/googleapis/genai-toolbox" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">MCP Toolbox</span></a><span style="vertical-align: baseline;">.</span></p>
<p><span style="vertical-align: baseline;">Get started with Google Cloud databases with a free trial for </span><a href="http://goo.gle/try_alloydb" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">AlloyDB</span></a><span style="vertical-align: baseline;"> or </span><a href="http://goo.gle/try_cloudsql" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Cloud SQL</span></a><span style="vertical-align: baseline;">.</span></p></div>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/products/databases/inside-ai-assisted-troubleshooting-for-databases/</id>
            <title>Democratizing database observability with AI-assisted troubleshooting</title>
            <link>https://cloud.google.com/blog/products/databases/inside-ai-assisted-troubleshooting-for-databases/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/products/databases/inside-ai-assisted-troubleshooting-for-databases/</guid>
            <pubDate></pubDate>
            <updated>Tue, 13 May 2025 16:00:00 +0000</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">As organizations adopt DevOps practices, application developers are increasingly expected to not only build applications but also manage and operate the databases they use. This added responsibility can prolong the application development process and time to market, primarily because developers must often manage and operate these critical systems without specialized database expertise.</span></p>
<p><span style="vertical-align: baseline;">To help, we introduced </span><a href="https://cloud.google.com/sql/docs/postgres/observe-troubleshoot-with-ai"><span style="text-decoration: underline; vertical-align: baseline;">AI-assisted troubleshooting</span></a><span style="vertical-align: baseline;"> for Cloud SQL and AlloyDB at Google Cloud Next 25. This feature is a game-changer, equipping developers with the self-sufficiency needed for effective database operations, allowing them to manage and troubleshoot databases confidently even without deep database expertise.</span></p>
<p><span style="vertical-align: baseline;">Imagine a database service that can proactively predict query performance issues, identify the root-cause and recommend actionable solutions — this is precisely what AI-assisted troubleshooting offers. AI-assisted troubleshooting</span><span style="vertical-align: baseline;"> predicts potential problems like slow queries, and provides actionable recommendations before they escalate into critical issues. AI-assisted troubleshooting works across Cloud SQL for MySQL, Cloud SQL for PostgreSQL, Cloud SQL for SQL Server and AlloyDB, and is integrated with Cloud SQL Studio simplifying the process of applying the recommendations. With assisted query optimization and automated performance monitoring, developers can channel their energy into application innovation rather than database maintenance and troubleshooting. </span></p>
<p><span style="vertical-align: baseline;">AI-assisted troubleshooting addresses database challenges head-on by leveraging the power of generative AI and machine learning. It provides automatic identification of hotspots, intelligent situational analyses that quickly identify root causes, and shares prescriptive remedial paths. With AI-assisted troubleshooting, you can:</span></p>
<ul>
<li><span style="vertical-align: baseline;">Identify inefficient queries, potential resource bottlenecks, and other performance issues before they impact your applications</span></li>
<li><span style="vertical-align: baseline;">Get clear, actionable recommendations to optimize database performance, including indexing strategies, partitioning suggestions, and more</span></li>
<li><span style="vertical-align: baseline;">Execute prescribed steps easily thanks to integration with Cloud SQL Studio</span></li>
</ul></div>
<div class="block-aside"><dl>
    <dt>aside_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;title&#x27;, &#x27;$300 in free credit to try Google Cloud databases&#x27;), (&#x27;body&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e75406c05e0&gt;), (&#x27;btn_text&#x27;, &#x27;Start building for free&#x27;), (&#x27;href&#x27;, &#x27;http://console.cloud.google.com/freetrial?redirectPath=/products?#databases&#x27;), (&#x27;image&#x27;, None)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">A real-world example</strong></h3>
<p><span style="vertical-align: baseline;">Imagine you're a developer for a real-time inventory tracking app that uses Cloud SQL. Recently, you've noticed a significant slowdown in data retrieval, impacting the app's responsiveness. How can you solve this quickly with AI-assisted troubleshooting?</span></p>
<ol>
<li><span style="vertical-align: baseline;">You navigate to Query Insights, where AI-assisted troubleshooting has already identified the problematic query and has provided an ‘Analyze’ button.</span></li>
<li><span style="vertical-align: baseline;">Click the ‘Analyze’ button, gaining immediate insights into the query's performance, including a spike in execution time. AI-assisted troubleshooting pinpoints the root cause - an increase in data volume.</span></li>
<li><span style="vertical-align: baseline;">It also provides a specific recommendation for resolving the issue, e.g. creating an index. </span></li>
<li><span style="vertical-align: baseline;">You implement the recommendations, via Cloud SQL Studio to restore the application's performance.</span></li>
</ol></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="01 slow query animated" src="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/01_slow_query_animated_ZvJ0EPo.gif" />
        
        </a>
      
        <figcaption class="article-image__caption "><p>Identifying a slow query, finding root-cause and fix recommendations with AI-assisted troubleshooting</p></figcaption>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="02 Analyzing database performance" src="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/02_Analyzing_database_performance_l9VwufY.gif" />
        
        </a>
      
        <figcaption class="article-image__caption "><p>Troubleshooting database performance, finding root-cause and fix recommendation with AI-assisted troubleshooting</p></figcaption>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">These scenarios highlight the power of AI-assisted troubleshooting. Instead of spending hours manually analyzing logs and metrics, AI provides a clear diagnosis and actionable solution in seconds.</span></p>
<p style="padding-left: 40px;"><span style="vertical-align: baseline;">“</span><span style="font-style: italic; vertical-align: baseline;">We're always looking for ways to leverage cutting-edge technology to improve our database operations. Cloud SQL's AI-assisted troubleshooting represents a significant leap forward in database management. By automating performance analysis and providing intelligent guidance, it frees up our team to focus on strategic initiatives like application innovation and new feature development. This is the kind of AI-powered solution that can truly transform how we operate.”</span><span style="vertical-align: baseline;"> - Kristofer Sikora, Exec Director, Cloud Data Engineering, CME Group</span></p>
<p><span style="vertical-align: baseline;">AI-assisted troubleshooting also bridges the skill gap and fosters a culture of independence, aligning with DevOps principles of automation and collaboration to streamline workflows and reduce silos between development and operations teams. Ultimately, AI-assisted troubleshooting not only improves database performance but also speeds-up overall software delivery by making database management more accessible and efficient for developers.</span></p>
<p><span style="vertical-align: baseline;">In short, AI-assisted troubleshooting is an exciting new way to manage and optimize your databases, and is now available in preview for AlloyDB, Cloud SQL for PostgreSQL and Cloud SQL for MySQL and Cloud SQL for SQL Server. Jumpstart your journey to optimized database performance by accessing AI-assisted troubleshooting in the Cloud SQL and AlloyDB consoles. For detailed guidance, see the documentation:</span><span style="vertical-align: baseline;"> </span><a href="https://cloud.google.com/sql/docs/postgres/observe-troubleshoot-with-ai"><span style="text-decoration: underline; vertical-align: baseline;">Cloud SQL for PostgreSQL</span></a><span style="vertical-align: baseline;">, </span><a href="https://cloud.google.com/sql/docs/mysql/observe-troubleshoot-with-ai"><span style="text-decoration: underline; vertical-align: baseline;">Cloud SQL for MySQL</span></a><span style="vertical-align: baseline;">, and </span><a href="https://cloud.corp.google.com/sql/docs/sqlserver/observe-troubleshoot-with-ai" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Cloud SQL for SQL Server</span></a><span style="vertical-align: baseline;">  and </span><a href="https://cloud.google.com/alloydb/docs/troubleshoot/slow-queries-ai"><span style="text-decoration: underline; vertical-align: baseline;">AlloyDB</span></a><span style="vertical-align: baseline;">.</span></p></div>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/products/data-analytics/bigquery-ml-contribution-analysis-models-now-ga/</id>
            <title>Understand why your metrics moved with contribution analysis in BigQuery ML, now GA</title>
            <link>https://cloud.google.com/blog/products/data-analytics/bigquery-ml-contribution-analysis-models-now-ga/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/products/data-analytics/bigquery-ml-contribution-analysis-models-now-ga/</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 May 2025 16:00:00 +0000</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">The key to effective data-driven decision making is quickly processing and extracting insights from large amounts of data. However, doing this efficiently and at scale is a challenge. </span></p>
<p><span style="vertical-align: baseline;">Imagine a retail scenario where you’re trying to identify the highest performing promotions by analyzing sales data across products, stores, locations, and a large customer base, in conjunction with other marketing events and discount sales. Pinpointing these highest performing promotions is essential to making targeted decisions and achieving better results. With current tooling, you’d need to manually inspect the data and use a trial and error approach to visualization and querying to uncover interesting insights. This task becomes difficult due to the volume of data and the number of dimensions involved, as this could create a large amount of possible combinations to explore.</span></p>
<p><span style="vertical-align: baseline;">BigQuery ML </span><a href="https://cloud.google.com/bigquery/docs/contribution-analysis"><span style="text-decoration: underline; vertical-align: baseline;">contribution analysis</span></a><span style="vertical-align: baseline;">, now generally available, performs this type of analysis at scale, automating insight generation and outputting key change drivers from multidimensional data so you can quickly pinpoint the areas where you need to take action. </span></p></div>
<div class="block-aside"><dl>
    <dt>aside_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;title&#x27;, &#x27;$300 in free credit to try Google Cloud data analytics&#x27;), (&#x27;body&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e753fbcbd00&gt;), (&#x27;btn_text&#x27;, &#x27;Start building for free&#x27;), (&#x27;href&#x27;, &#x27;http://console.cloud.google.com/freetrial?redirectPath=/bigquery/&#x27;), (&#x27;image&#x27;, None)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Originally </span><a href="https://cloud.google.com/blog/products/data-analytics/introducing-a-new-contribution-analysis-model-in-bigquery?e=48754805"><span style="text-decoration: underline; vertical-align: baseline;">announced in September 2024</span></a><span style="vertical-align: baseline;">, the preview of contribution analysis generates key drivers for a metric of interest across test and control datasets. With the GA, contribution analysis now include several new features to help you identify the most important insights faster:</span></p>
<p><span style="vertical-align: baseline;">1. </span><strong style="vertical-align: baseline;">Automated support tuning with top-k insights by apriori support</strong></p>
<p><span style="vertical-align: baseline;">In preview, contribution analysis provided an option for apriori support pruning, which would remove small data segments to decrease query execution time. For GA, as an alternative to specifying your own apriori support pruning threshold, you can now let the model set the threshold for you. Simply provide the number of insights you’d like returned and the model automatically retrieves the top insights by data segment size, which we refer to as apriori support. Both the </span><code style="vertical-align: baseline;">min_apriori_support</code><span style="vertical-align: baseline;"> and </span><code style="vertical-align: baseline;">top_k_insights_by_apriori_support</code><span style="vertical-align: baseline;"> option reduce query latency compared to returning all possible insights, </span><span style="vertical-align: baseline;">which can consist of millions of rows</span><span style="vertical-align: baseline;">.</span></p>
<p><span style="vertical-align: baseline;">2. </span><strong style="vertical-align: baseline;">Improved insight readability with redundant insight pruning</strong></p>
<p><span style="vertical-align: baseline;">Insight redundancy occurs when multiple related insights share the same output metrics, which is especially prevalent in correlated data. For example, consider the table below with sales data that contains city and store name dimensions. Because all sales from the General Store also occurred in Iowa City, the underlying data for rows 1 and 2 are the same, which leads to the equal metric values for both rows.<br /><br /></span></p>
<div align="center">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;"><table style="width: 99.2167%;"><colgroup><col style="width: 62.1094%;" /><col style="width: 17.1875%;" /><col style="width: 20.7031%;" /></colgroup>
<tbody>
<tr>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p style="text-align: center;"><strong style="vertical-align: baseline;">contributors</strong></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p style="text-align: center;"><strong style="vertical-align: baseline;">metric_test</strong></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p style="text-align: center;"><strong style="vertical-align: baseline;">metric_control</strong></p>
</td>
</tr>
<tr>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">[city=’Iowa City’, store_name=’General Store / Iowa City’]</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p style="text-align: center;"><span style="vertical-align: baseline;">640047.08</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p style="text-align: center;"><span style="vertical-align: baseline;">214317.46</span></p>
</td>
</tr>
<tr>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">[store_name=’General Store / Iowa City’]</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p style="text-align: center;"><span style="vertical-align: baseline;">640047.08</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p style="text-align: center;"><span style="vertical-align: baseline;">214317.46</span></p>
</td>
</tr>
</tbody>
</table></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><strong style="vertical-align: baseline;"> </strong><span style="vertical-align: baseline;">With the new </span><code style="vertical-align: baseline;">pruning_method</code><span style="vertical-align: baseline;"> option, you can choose to prune redundant insights, so you only see unique insights in the output table with the most descriptive segment. For the above table, you’d only see the first row that contains both the city and store name, rather than two rows with the same metrics.</span></p>
<p><span style="vertical-align: baseline;">3. </span><strong style="vertical-align: baseline;">Expanded metric support with the summable by category metric</strong></p>
<p><span style="vertical-align: baseline;">For preview, contribution analysis initially offered two contribution metric types: a summable metric and a summable ratio metric. Specifying a </span><a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-contribution-analysis#use_a_summable_metric"><span style="text-decoration: underline; vertical-align: baseline;">summable metric</span></a><span style="vertical-align: baseline;"> allows you to aggregate a single measure of interest, while the </span><a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-contribution-analysis#use_a_summable_ratio_metric"><span style="text-decoration: underline; vertical-align: baseline;">summable ratio metric</span></a><span style="vertical-align: baseline;"> lets you aggregate a ratio across two measures. </span></p>
<p><span style="vertical-align: baseline;">For GA, in addition to the </span><code style="vertical-align: baseline;">summable</code><span style="vertical-align: baseline;"> and </span><code style="vertical-align: baseline;">summable ratio</code><span style="vertical-align: baseline;"> metrics, you can now analyze the sum of a metric of interest normalized by the unique values of a categorical variable with the new </span><code style="vertical-align: baseline;">summable by category</code><span style="vertical-align: baseline;"> metric. This allows you to analyze metrics like sales per customer or site visits per day. </span></p>
<p><span style="vertical-align: baseline;">The summable by category metric can help adjust for outliers in your data. Take the example in the table below, where within the test set we have an outlier in row 1 with a high amount of active minutes on the website product. If you were using the </span><code style="vertical-align: baseline;">summable</code><span style="vertical-align: baseline;"> metric on active minutes, you’d see that active minutes on the website in the test set were 2.5 times higher than active minutes on the app in the test set. But, if you adjust for the number of users on each product with the </span><code style="vertical-align: baseline;">summable by category</code><span style="vertical-align: baseline;"> metric, you’d see that active minutes per user on the website in the test set was only 1.6 times higher than active minutes per user on the app in the test set.<br /><br /></span></p>
<div align="center">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;">
<div style="color: #5f6368; width: 100%;"><table style="width: 99.7389%;"><colgroup><col style="width: 35.7616%;" /><col style="width: 21.3245%;" /><col style="width: 23.0464%;" /><col style="width: 20%;" /></colgroup>
<tbody>
<tr>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><strong style="vertical-align: baseline;">Active Minutes</strong></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><strong style="vertical-align: baseline;">User Id</strong></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><strong style="vertical-align: baseline;">Product</strong></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><strong style="vertical-align: baseline;">Is Test</strong></p>
</td>
</tr>
<tr>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">100</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">user_a</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">Website</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">TRUE</span></p>
</td>
</tr>
<tr>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">23</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">user_b</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">Website</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">TRUE</span></p>
</td>
</tr>
<tr>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">15</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">user_c</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">Website</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">TRUE</span></p>
</td>
</tr>
<tr>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">31</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">user_b</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">App</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">TRUE</span></p>
</td>
</tr>
<tr>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">24</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">user_c</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">App</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">TRUE</span></p>
</td>
</tr>
<tr>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">39</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">user_b</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">Website</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p><span style="vertical-align: baseline;">FALSE</span></p>
</td>
</tr>
<tr>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p style="text-align: center;"><span style="vertical-align: baseline;">…</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p style="text-align: center;"><span style="vertical-align: baseline;">…</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p style="text-align: center;"><span style="vertical-align: baseline;">…</span></p>
</td>
<td style="vertical-align: top; border: 1px solid #000000; padding: 16px;">
<p style="text-align: center;"><span style="vertical-align: baseline;">…</span></p>
</td>
</tr>
</tbody>
</table></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p><code style="vertical-align: baseline;">Summable by category</code><span style="vertical-align: baseline;"> metrics are also helpful when you have different numbers of test and control rows. Say you want to compare revenue data from 2024 to 2025, but you only have five months of data from 2025 compared to twelve months of data in 2024. With the </span><code style="vertical-align: baseline;">summable by category</code><span style="vertical-align: baseline;"> metric, you can analyze the differences in revenue per month to get a more direct comparison between the two groups. </span></p>
<h3><strong style="vertical-align: baseline;">Contribution analysis in action</strong></h3>
<p><span style="vertical-align: baseline;">To get started, let’s walk through a retail sales example with the new model options. Say you want to find the key contributors to why sales dropped between 2020 and 2021 for apparel products on the Google Merchandise Store public e-commerce dataset.</span></p>
<p><span style="vertical-align: baseline;">To start, create an input data table that contains the sales metric to aggregate over, the categorical user_id column to normalize by, the dimension columns to slice by, and the boolean is_test value to determine whether the data row is in the test or control subset. </span></p></div>
<div class="block-paragraph_advanced"><pre class="language-plain"><code>CREATE OR REPLACE TABLE bqml_tutorial.ecommerce_data AS 
(SELECT 
  event_value_in_usd AS sales,
  user_pseudo_id AS user_id,
  device.category AS device_category,
  geo.country AS country,
  traffic_source.medium as site_traffic_source,
  FALSE AS is_test
FROM `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`
WHERE event_name IN ('in_app_purchase', 'purchase') and 
traffic_source.medium IN ('referral', 'organic', 'cpc', '&lt;Other&gt;')
AND _TABLE_SUFFIX BETWEEN '20200101' AND '20201231')
UNION ALL
(SELECT 
  event_value_in_usd AS sales,
  user_pseudo_id AS user_id,
  device.category AS device_category,
  geo.country AS country,
  traffic_source.medium as site_traffic_source,
  TRUE AS is_test
FROM `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`
WHERE event_name IN ('in_app_purchase', 'purchase') AND 
traffic_source.medium IN ('referral', 'organic', 'cpc', '&lt;Other&gt;')
AND _TABLE_SUFFIX BETWEEN '20210101' AND '20211231');
</code></pre></div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Then, create a summable by category metric contribution analysis model. Using the </span><code style="vertical-align: baseline;">top_k_insights_by_apriori_support</code><span style="vertical-align: baseline;"> option, you get the top 15 insights with the largest apriori support, and using the </span><code style="vertical-align: baseline;">prune_redundant_insights</code><span style="vertical-align: baseline;"> pruning method, you ensure unique insights are returned.</span></p></div>
<div class="block-paragraph_advanced"><pre class="language-plain"><code>CREATE OR REPLACE MODEL bqml_tutorial.ecommerce_summable_category_model 
OPTIONS(
  MODEL_TYPE='CONTRIBUTION_ANALYSIS',
  DIMENSION_ID_COLS=['device_category', 'country', 'site_traffic_source'],
  IS_TEST_COL='is_test',
  CONTRIBUTION_METRIC='SUM(sales)/COUNT(DISTINCT user_id)',
  TOP_K_INSIGHTS_BY_APRIORI_SUPPORT = 15,
  PRUNING_METHOD='PRUNE_REDUNDANT_INSIGHTS'
) AS SELECT * FROM bqml_tutorial.ecommerce_data;</code></pre></div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">And finally, you get the insights from the model. </span></p></div>
<div class="block-paragraph_advanced"><pre class="language-plain"><code>SELECT 
  contributors,
  metric_test,
  metric_control,
  difference,
  relative_difference,
  apriori_support,
  contribution
FROM ML.GET_INSIGHTS(MODEL 
bqml_tutorial.ecommerce_summable_category_model);</code></pre></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="3" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/3_IJbLMlS.max-1000x1000.jpg" />
        
        </a>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">The output is automatically ordered by the </span><code style="vertical-align: baseline;">contribution</code><span style="vertical-align: baseline;"> column, which is the absolute value of </span><code style="vertical-align: baseline;">difference</code><span style="vertical-align: baseline;">, in descending order. Drilling down to specific results, in row 1, you can see that the sales per user in the United States that were referred from another website decreased from $101.65 in 2020 to $58.37 in 2021, a decrease of -$43.27 as seen by the </span><code style="vertical-align: baseline;">difference</code><span style="vertical-align: baseline;"> metric, or a -42.5% decrease as seen by the </span><code style="vertical-align: baseline;">relative_difference</code><span style="vertical-align: baseline;"> metric. In row 3, you can see that there was a -34.6% decrease in sales per user in the United States overall, a segment that makes up 47.3% of all data as shown in the </span><code style="vertical-align: baseline;">apriori_support</code><span style="vertical-align: baseline;"> metric. This is invaluable information to help inform your business strategy! </span></p>
<h3><strong style="vertical-align: baseline;">Get started today</strong></h3>
<p><span style="vertical-align: baseline;">To give contribution analysis a try on your own data, please see the </span><a href="https://cloud.google.com/bigquery/docs/get-contribution-analysis-insights"><span style="text-decoration: underline; vertical-align: baseline;">tutorial</span></a><span style="vertical-align: baseline;"> and </span><a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-contribution-analysis"><span style="text-decoration: underline; vertical-align: baseline;">documentation</span></a><span style="vertical-align: baseline;">. </span></p></div>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/products/ai-machine-learning/how-ai-can-redefine-education-for-students/</id>
            <title>Inside Infinity Learn's AI Tutor, powered by Google Cloud</title>
            <link>https://cloud.google.com/blog/products/ai-machine-learning/how-ai-can-redefine-education-for-students/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/products/ai-machine-learning/how-ai-can-redefine-education-for-students/</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 May 2025 16:00:00 +0000</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Late-night study sessions, complex equations, and immense pressure: for millions of students in India, the IIT JEE Main exam and NEET for medical colleges are critical gateways for their futures. Exam success hinges on intricate reasoning within physics, chemistry, mathematics, zoology, and botany. These students require a clear, step-by-step understanding of the topics, not just the correct answers.</span></p>
<p><span style="vertical-align: baseline;">Recognizing this vital need for deep comprehension and problem-solving, Infinity Learn, a pioneer in tech-forward education, collaborated with Google Cloud Consulting to develop an AI tutor to assist students during exam prep. The goal wasn't to merely create a question-answering tool, but a true </span><span style="font-style: italic; vertical-align: baseline;">tutor</span><span style="vertical-align: baseline;">.</span></p>
<p><span style="vertical-align: baseline;">Acting as a custom search engine, the AI tutor allows students to learn through detailed guidance for solving math, physics, and chemistry problems. This searchable system aims to foster in-depth knowledge via Google Cloud’s Vertex AI Retrieval Augmented Generation (RAG) services. Using a cutting-edge Gemini 2.0 Flash model, the AI tutor enables students to independently find the answers they need across faculty resources, helping them truly grasp the </span><span style="font-style: italic; vertical-align: baseline;">how</span><span style="vertical-align: baseline;"> and the </span><span style="font-style: italic; vertical-align: baseline;">why</span><span style="vertical-align: baseline;"> behind solutions.</span></p></div>
<div class="block-aside"><dl>
    <dt>aside_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;title&#x27;, &#x27;$300 in free credit to try Google Cloud AI and ML&#x27;), (&#x27;body&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e753c4a0520&gt;), (&#x27;btn_text&#x27;, &#x27;Start building for free&#x27;), (&#x27;href&#x27;, &#x27;http://console.cloud.google.com/freetrial?redirectPath=/vertex-ai/&#x27;), (&#x27;image&#x27;, None)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">Powered by Agent Builder and Vertex AI RAG Engine</strong></h3>
<p><span style="vertical-align: baseline;">The biggest challenge early on in development was building a system that could do more than just anticipate exam questions. It needed to:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Solve multi-step problems logically, mirroring a human tutor's approach.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Articulate each step in a way that promotes conceptual clarity.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Serve a large user base (both free and paid) while balancing cost and accuracy.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Analyze and self-correct errors to continuously refine performance.</span></p>
</li>
</ul>
<p><span style="vertical-align: baseline;">In a collaborative effort, Google Cloud Consulting and Infinity Learn used Agent Builder for experimentation and RAG Engine on Vertex AI for refinement.</span></p>
<p><span style="vertical-align: baseline;">They began with Vertex AI's Agent Builder, an out-of-the-box RAG system that allowed for rapid prototyping and quick iteration. Using </span><a href="https://cloud.google.com/generative-ai-app-builder/docs/introduction#es-and-personalize"><span style="text-decoration: underline; vertical-align: baseline;">Vertex AI Search </span></a><span style="vertical-align: baseline;">within Agent Builder, they were able to supply the AI language models with Infinity Learn's collective library of resources, ensuring answers were relevant, accurate, and grounded.</span></p>
<p><span style="vertical-align: baseline;">For further customization, they then implemented a do-it-yourself </span><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/rag-overview"><span style="text-decoration: underline; vertical-align: baseline;">RAG engine</span></a><span style="vertical-align: baseline;"> with </span><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/use-vertexai-search"><span style="text-decoration: underline; vertical-align: baseline;">Vertex AI Search as retriever.</span></a><span style="vertical-align: baseline;"> This offered more flexibility when selecting the retrieval mechanisms and generation model, as well as granular control for fine-tuning the AI tutor’s accuracy and reasoning capabilities while using Google's search infrastructure.</span></p></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="image1" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image1_bz9viJi.max-1000x1000.png" />
        
        </a>
      
        <figcaption class="article-image__caption "><p>Technical architecture: A blended architecture which leverages state–of-the-art Vertex AI Search as the backend engine and Gemini 2.0 Flash and thinking-mode as the generative model along with a custom evaluation method.</p></figcaption>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">Beyond accuracy: Measuring real-world effectiveness</strong></h3>
<p><span style="vertical-align: baseline;">Following implementation, the team launched a thorough evaluation for the AI tutor. This model-based custom evaluation leveraged </span><a href="https://cloud.google.com/vertex-ai/docs/workbench/introduction"><span style="text-decoration: underline; vertical-align: baseline;">Vertex AI workbench</span></a><span style="vertical-align: baseline;"> and subject matter experts to confirm AI accuracy within a variation of up to 3-5% (as compared to human reviewer-based accuracy).</span></p>
<p><span style="vertical-align: baseline;">The evaluation process focused on measuring and analyzing errors related to conceptual, computational, and procedural mistakes. Mitigating these errors can help in the self-correction of generated answers, providing smarter future responses for the solution, valuable data for ongoing refinement, and insight into the AI's ability to pinpoint areas where students commonly struggle.</span></p>
<h3><strong style="vertical-align: baseline;">The AI tutor's impact and a shift in learning </strong></h3>
<p><span style="vertical-align: baseline;">Through combining advanced reasoning, custom AI, and flexible data retrieval, the AI tutor yielded a remarkable 90% accuracy when addressing student queries across an average of 500 exam preparation questions. Essentially, this level of precision means students can confidently rely on the AI tutor for the majority of their questions.</span></p>
<p><span style="vertical-align: baseline;">But more than just accuracy, this technology is driving a fundamental shift in education. Students are now able to move beyond rote memorization and learn at their own pace through the AI tutor’s detailed, comprehensive responses. </span></p>
<p><span style="vertical-align: baseline;">As an example, imagine a student struggling with a difficult mathematics problem late at night. Instead of getting stuck for hours, they can ask the AI tutor, giving them not only the correct answer, but a foundational understanding of the basic principles and the steps to get there.</span></p>
<p><span style="vertical-align: baseline;">This personalized, always-on exam companion continues to encourage independent thinking and understanding, thanks to a tech-first approach to education, and Infinity Learn’s collaborative work with Google Cloud Consulting.</span></p>
<p style="padding-left: 40px;"><span style="vertical-align: baseline;">"</span><strong style="vertical-align: baseline;">Since implementing the AI tutor, we've seen a remarkable shift in how students approach complex problems. They're engaging with the underlying concepts on a deeper level. Google Cloud Consulting's expertise was instrumental in helping us build a truly innovative AI tutor that goes beyond surface-level answers and cultivates real understanding.</strong><span style="vertical-align: baseline;">" – Amit Bansal, Chief Product Officer, Infinity Learn</span></p>
<h3><strong style="vertical-align: baseline;">Inspired by Infinity Learn's transformative approach? </strong></h3>
<p><span style="vertical-align: baseline;">The </span><a href="https://education.economictimes.indiatimes.com/news/industry/infinity-learn-by-sri-chaitanya-chooses-google-cloud-india-to-transform-education-with-generative-ai-solutions/116011017" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">partnership</span></a><span style="vertical-align: baseline;"> between Google Cloud and Infinity Learn by Sri Chaitanya is just one of the latest examples of how we’re providing AI-powered solutions to solve complex problems and help organizations drive desired outcomes. </span></p>
<p><span style="vertical-align: baseline;">Discover how Google Cloud Consulting can help you leverage Vertex AI to transform your own challenges or business needs. The Google Cloud Consulting portfolio provides unified services, bringing together offerings throughout multiple specializations like technical account management, professional services, customer success, and more. Check out Google Cloud Consulting’s </span><a href="https://cloud.google.com/consulting?hl=en"><span style="text-decoration: underline; vertical-align: baseline;">full portfolio of offerings</span></a><span style="vertical-align: baseline;">, or learn </span><a href="https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform"><span style="text-decoration: underline; vertical-align: baseline;">how to get started with Vertex AI</span></a><span style="vertical-align: baseline;">.</span></p></div>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/products/databases/aiven-for-alloydb-omni-now-ga/</id>
            <title>Multi-cloud AI made easier: Aiven for AlloyDB Omni now generally available</title>
            <link>https://cloud.google.com/blog/products/databases/aiven-for-alloydb-omni-now-ga/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/products/databases/aiven-for-alloydb-omni-now-ga/</guid>
            <pubDate></pubDate>
            <updated>Mon, 12 May 2025 16:00:00 +0000</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Building modern, data-driven applications requires a database that can handle transactional, analytical, and vector search workloads, especially as AI and machine learning become increasingly vital. You need a solution that scales, maintains compliance, delivers consistent performance, and that doesn’t require constant re-architecting.</span></p>
<p><span style="vertical-align: baseline;">Whether you’re building transactional, translytical, or generative AI applications across one or multiple cloud environments, you know the challenges: operational complexity, scaling, and inconsistent performance. That's why we're excited that </span><a href="https://aiven.io/alloydb-omni" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Aiven for AlloyDB Omni</span></a><span style="vertical-align: baseline;"> is generally available. This fully managed, high-performance PostgreSQL-compatible solution simplifies your workflow, offering seamless access through all major cloud marketplaces. Start your </span><a href="http://console.aiven.io/signup?campaign=alloydb" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">30 day trial at no charge </span></a><span style="vertical-align: baseline;">today, and get your service up and running in no time.</span></p>
<h3><strong style="vertical-align: baseline;">Simplifying data management on any cloud</strong></h3>
<p><span style="vertical-align: baseline;">Aiven for AlloyDB Omni addresses these challenges by providing a fully managed experience, enabling you to build high-performance, intelligent applications more efficiently. With this GA, you'll benefit from:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Strong uptime SLA across all major clouds:</strong><span style="vertical-align: baseline;"> Ensure high availability and resilience for your critical applications, regardless of your cloud infrastructure.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">A consistent multi-cloud experience</strong><span style="vertical-align: baseline;">: Deploy Aiven for AlloyDB Omni on AWS, Azure or Google Cloud.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Marketplace availability across all major clouds:</strong><span style="vertical-align: baseline;"> Streamline procurement and deployment by accessing Aiven for AlloyDB Omni through your preferred cloud marketplace or directly from Aiven.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Disaster recovery:</strong><span style="vertical-align: baseline;"> Deploy read replicas across clouds or regions for proactive disaster recovery.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Superior price/performance:</strong><span style="vertical-align: baseline;"> Experience significant performance gains, with AlloyDB Omni offering faster speeds for transactional, analytical, and vector search workloads.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Flexible licensing:</strong><span style="vertical-align: baseline;"> Have the freedom to move workloads quickly between clouds or regions, optimizing infrastructure costs and maintaining control.</span></p>
</li>
</ul></div>
<div class="block-aside"><dl>
    <dt>aside_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;title&#x27;, &#x27;$300 in free credit to try Google Cloud databases&#x27;), (&#x27;body&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e753fe59400&gt;), (&#x27;btn_text&#x27;, &#x27;Start building for free&#x27;), (&#x27;href&#x27;, &#x27;http://console.cloud.google.com/freetrial?redirectPath=/products?#databases&#x27;), (&#x27;image&#x27;, None)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">Build AI-powered applications</strong></h3>
<p><span style="vertical-align: baseline;">Aiven for AlloyDB Omni integrates seamlessly with Aiven's unified data platform, providing the tools you need to build, stream, manage, and analyze data. This allows you to:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Accelerate AI initiatives:</strong><span style="vertical-align: baseline;"> Utilize AlloyDB Omni's vector search capabilities to build powerful AI applications.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Reduce management overhead:</strong><span style="vertical-align: baseline;"> Focus on application development while Aiven handles infrastructure provisioning and maintenance.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Scale without limits:</strong><span style="vertical-align: baseline;"> Accommodate growth and unpredictable workloads without costly re-architecting.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Streamline data pipelines:</strong><span style="vertical-align: baseline;"> Manage your data pipeline from a single platform, simplifying operations.</span></p>
</li>
</ul>
<p style="padding-left: 40px;"><span style="font-style: italic; vertical-align: baseline;">"As AI and machine learning become central to business success, AlloyDB Omni is more essential than ever. For developers, this is a game-changer as they can easily test AI use cases and build applications — all within the PostgreSQL database they’re already familiar with. With the speed and power of Aiven’s platform, teams can move faster, reduce management overhead and scale AI initiatives with ease.”</span><span style="vertical-align: baseline;"> - Oskari Saarenmaa, CEO, Aiven</span></p>
<h3><strong style="vertical-align: baseline;">Get started with Aiven for AlloyDB Omni</strong></h3>
<p><a href="https://cloud.google.com/alloydb/omni"><span style="text-decoration: underline; vertical-align: baseline;">AlloyDB Omni</span></a><span style="vertical-align: baseline;"> was built to make the innovations of AlloyDB available to workloads outside of Google Cloud. It delivers 100% PostgreSQL compatibility and is designed to run anywhere you need it — from your own data centers to any cloud environment. This means you can expect transactional workloads to run more than two times faster at scale, analytical queries can get accelerated up to 100 times, and vector search operations to perform up to 10 times faster compared to standard PostgreSQL.</span></p>
<p><span style="vertical-align: baseline;">With </span><span style="vertical-align: baseline;">Aiven for AlloyDB Omni, this solution is now available as a managed service across AWS, Azure, and Google Cloud. </span><span style="vertical-align: baseline;">Taking</span><span style="vertical-align: baseline;"> the first step with Aiven for AlloyDB Omni is seamless. Simply choose AlloyDB Omni, your cloud provider, and the region. Select the pricing plan that best suits your needs, and get set up in a few steps. Launching a new service is quick and straightforward, allowing you to get started with your AI and hybrid analytical-transactional applications in no time. </span></p>
<p><span style="vertical-align: baseline;">Ready to experience the benefits of Aiven for AlloyDB Omni firsthand? </span><a href="https://console.aiven.io/signup?campaign=alloydb" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Start your 30-day trial at no charge today</span></a><span style="vertical-align: baseline;">! We're eager to see how you leverage Aiven for AlloyDB Omni to build innovative and powerful applications. Stay tuned for further updates and resources.</span></p></div>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/products/compute/ai-hypercomputer-inference-updates-for-google-cloud-tpu-and-gpu/</id>
            <title>From LLMs to image generation: Accelerate inference workloads with AI Hypercomputer</title>
            <link>https://cloud.google.com/blog/products/compute/ai-hypercomputer-inference-updates-for-google-cloud-tpu-and-gpu/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/products/compute/ai-hypercomputer-inference-updates-for-google-cloud-tpu-and-gpu/</guid>
            <pubDate></pubDate>
            <updated>Fri, 09 May 2025 16:00:00 +0000</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">From retail to gaming, from code generation to customer care, an increasing number of organizations are running LLM-based applications, with </span><a href="https://cloud.google.com/resources/content/state-of-ai-infrastructure"><span style="text-decoration: underline; vertical-align: baseline;">78% of organizations in development or production today</span></a><span style="vertical-align: baseline;">.</span><span style="vertical-align: baseline;"> As the number of generative AI applications and volume of users scale, the need for performant, scalable, and easy to use inference technologies is critical. At Google Cloud, we’re paving the way for this next phase of AI’s rapid evolution with our AI Hypercomputer. </span></p>
<p><span style="vertical-align: baseline;">At Google Cloud Next 25, we shared many updates to </span><a href="https://cloud.google.com/solutions/ai-hypercomputer?hl=en"><span style="text-decoration: underline; vertical-align: baseline;">AI Hypercomputer’s</span></a><span style="vertical-align: baseline;"> inference capabilities, unveiling </span><a href="https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Ironwood</span></a><span style="vertical-align: baseline;">, our newest Tensor Processing Unit (TPU) designed specifically for inference, coupled with software enhancements such as simple and performant inference using </span><a href="https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-vllm-tpu"><span style="text-decoration: underline; vertical-align: baseline;">vLLM on TPU</span></a><span style="vertical-align: baseline;"> and the latest </span><a href="https://cloud.google.com/blog/products/containers-kubernetes/understanding-new-gke-inference-capabilities"><span style="text-decoration: underline; vertical-align: baseline;">GKE inference capabilities</span><span style="vertical-align: baseline;"> — </span></a><span style="vertical-align: baseline;">GKE Inference Gateway and GKE Inference Quickstart.</span></p>
<p><span style="vertical-align: baseline;">With AI Hypercomputer, we also continue to push the envelope for performance with optimized software, backed by strong benchmarks: </span></p>
<ul>
<li><span style="vertical-align: baseline;">Google’s </span><a href="https://github.com/AI-Hypercomputer/JetStream" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">JetStream</span></a><span style="vertical-align: baseline;"> inference engine incorporates new performance optimizations, integrating Pathways for ultra-low latency multi-host, disaggregated serving.</span></li>
<li><a href="https://github.com/AI-Hypercomputer/maxdiffusion" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">MaxDiffusion</span></a><span style="vertical-align: baseline;">, our reference implementation of latent diffusion models, delivers standout performance on TPUs for compute-heavy image generation workloads, and now supports Flux, one of the largest text-to-image generation models to date. </span></li>
<li><span style="vertical-align: baseline;">The latest performance results from </span><a href="https://mlcommons.org/benchmarks/inference-datacenter/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">MLPerf™ Inference v5.0</span></a><span style="vertical-align: baseline;"> demonstrate the power and versatility of Google Cloud’s A3 Ultra (NVIDIA H200) and A4 (NVIDIA HGX B200) VMs for inference.</span></li>
</ul></div>
<div class="block-video">



<div class="article-module article-video ">
  <figure>
    <a class="h-c-video h-c-video--marquee" href="https://youtube.com/watch?v=rpbWWssLkMY">

      
        

        <div class="article-video__aspect-image">
          <span class="h-u-visually-hidden">Inference at scale with Google Cloud’s AI Hypercomputer</span>
        </div>
      
      <svg class="h-c-video__play h-c-icon h-c-icon--color-white" xmlns="http://www.w3.org/2000/svg">
        <use xlink:href="#mi-youtube-icon" xmlns:xlink="http://www.w3.org/1999/xlink"></use>
      </svg>
    </a>

    
  </figure>
</div>

<div class="h-c-modal--video">
   <a class="glue-yt-video" href="https://youtube.com/watch?v=rpbWWssLkMY">
   </a>
</div>

</div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">Optimizing performance for JetStream: Google’s JAX inference engine</strong></h3>
<p><span style="vertical-align: baseline;">To maximize performance and reduce inference costs, we are excited to offer more choice when serving LLMs on TPU, further enhancing JetStream and bringing </span><a href="https://cloud.google.com/tpu/docs/tutorials/LLM/vllm-inference-v6e"><span style="text-decoration: underline; vertical-align: baseline;">vLLM support for TPU</span></a><span style="vertical-align: baseline;">, a widely-adopted fast and efficient library for serving LLMs. With both vLLM on TPU and </span><a href="https://github.com/AI-Hypercomputer/JetStream" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">JetStream</span></a><span style="vertical-align: baseline;">, we deliver standout price-performance with low-latency, high-throughput inference and community support through open-source contributions and from Google AI experts. </span></p>
<p><span style="vertical-align: baseline;">JetStream is Google’s open-source, throughput- and memory-optimized inference engine, purpose-built for TPUs and based on the same inference stack used to serve Gemini models. </span><span style="vertical-align: baseline;">Since we </span><a href="https://cloud.google.com/blog/products/compute/accelerating-ai-inference-with-google-cloud-tpus-and-gpus"><span style="text-decoration: underline; vertical-align: baseline;">announced</span></a><span style="vertical-align: baseline;"> JetStream last April, we have invested significantly in further improving its performance across a wide range of open models. When using JetStream, our sixth-generation Trillium TPU now exceeds throughput performance by 2.9x for Llama 2 70B and 2.8x for Mixtral 8x7B compared to TPU v5e (using our reference implementation </span><a href="https://github.com/AI-Hypercomputer/maxtext" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">MaxText</span></a><span style="vertical-align: baseline;">). </span></p></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="1" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_iM39ujT.max-1000x1000.png" />
        
        </a>
      
        <figcaption class="article-image__caption "><p>Figure 1: JetStream throughput (output tokens / second). Google internal data. Measured using Llama2-70B (MaxText) on Cloud TPU v5e-8 and Trillium 8-chips and Mixtral 8x7B (MaxText) on Cloud TPU v5e-4 and Trillium 4-chips. Maximum input length: 1024, maximum output length: 1024. As of April 2025.</p></figcaption>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Available for the first time for Google Cloud customers, Google’s </span><a href="https://cloud.google.com/ai-hypercomputer/docs/workloads/pathways-on-cloud/pathways-intro"><span style="text-decoration: underline; vertical-align: baseline;">Pathways</span></a><span style="vertical-align: baseline;"> runtime is now integrated into JetStream, enabling multi-host inference and disaggregated serving — two important features as model sizes grow exponentially and generative AI demands evolve.</span></p>
<p><a href="https://cloud.google.com/ai-hypercomputer/docs/workloads/pathways-on-cloud/multihost-inference"><span style="text-decoration: underline; vertical-align: baseline;">Multi-host inference</span></a><span style="vertical-align: baseline;"> using Pathways distributes the model across multiple accelerators hosts when serving. This enables the inference of large models that don't fit on a single host. With multi-host inference, JetStream achieves 1703 token/s on Llama 3.1 405B on Trillium. This translates to three times more inference per dollar compared to TPU v5e.</span></p>
<p><span style="vertical-align: baseline;">In addition, with Pathways, </span><a href="https://cloud.google.com/ai-hypercomputer/docs/workloads/pathways-on-cloud/multihost-inference#disaggregated_inference"><span style="text-decoration: underline; vertical-align: baseline;">disaggregated serving</span></a><span style="vertical-align: baseline;"> capabilities allow workloads to dynamically scale LLM inference’s decode and prefill stages independently. This allows for better utilization of resources and can lead to improvements in performance and efficiency, especially for large models. For Llama2-70B, using multiple hosts with disaggregated serving performs seven times better for prefill (time-to-first-token, TTFT) operations, and nearly three times better for token generation (time-per-output-token, TPOT) compared with interleaving the prefill and decode stages of LLM request processing on the same server on Trillium. </span></p></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="2" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Figure2_TTFT.max-1000x1000.png" />
        
        </a>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="3" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Figure2_TPOT.max-1000x1000.png" />
        
        </a>
      
        <figcaption class="article-image__caption "><p>Figure 2: Measured using Llama2-70B (MaxText) on Cloud TPU Trillium 16-chips (8 chips allocated for prefill server, 8 chips allocated for decode server). Measured using the OpenOrca dataset. Maximum input length: 1024, maximum output length: 1024. As of April 2025.</p></figcaption>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Customers like </span><a href="https://www.osmos.io/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">Osmos</span></a><span style="vertical-align: baseline;"> are using TPUs to maximize cost-efficiency for inference at scale:</span></p>
<p style="padding-left: 40px;"><span style="font-style: italic; vertical-align: baseline;">“Osmos is building the world's first AI Data Engineer. This requires us to deploy AI technologies at the cutting edge of what is possible today. We are excited to continue our journey  building on Google TPUs as our AI infrastructure for training and inference. We have vLLM and JetStream in scaled production deployment on Trillium and are able to achieve industry leading performance at over 3500 tokens/sec per v6e node for long sequence inference for 70B class models. This gives us industry leading tokens/sec/$, comparable to not just other hardware infrastructure, but also fully managed inference services. The availability of TPUs and the ease of deployment on AI Hypercomputer lets us build out an Enterprise software offering with confidence.” </span><span style="vertical-align: baseline;">- Kirat Pandya, CEO, Osmos</span></p>
<h3><strong style="vertical-align: baseline;">MaxDiffusion: High-performance diffusion model inference</strong></h3>
<p><span style="vertical-align: baseline;">Beyond LLMs, Trillium demonstrates standout performance on compute-heavy workloads like image generation. </span><a href="https://github.com/AI-Hypercomputer/maxdiffusion" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">MaxDiffusion</span></a><span style="vertical-align: baseline;"> delivers a collection of reference implementations of various latent diffusion models. In addition to Stable Diffusion inference, we have expanded MaxDiffusion to now support Flux; with 12 billion parameters, Flux is one of the largest open source text-to-image models to date. </span></p>
<p><span style="vertical-align: baseline;">As demonstrated on MLPerf 5.0, Trillium now delivers 3.5x throughput improvement for queries/second on Stable Diffusion XL (SDXL) compared to last performance round for its predecessor, TPU v5e. This further improves throughput by 12% since the MLPerf 4.1 submission. </span></p></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="4" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/4_6ek9LHl.max-1000x1000.png" />
        
        </a>
      
        <figcaption class="article-image__caption "><p>Figure 3: MaxDiffusion throughput (images per second). Google internal data. Measured using the SDXL model on Cloud TPU v5e-4 and Trillium 4-chip. Resolution: 1024x1024, batch size per device: 16, decode steps: 20. As of April 2025.</p></figcaption>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">With this throughput, MaxDiffusion delivers a cost-efficient solution. The cost to generate 1000 images is as low as 22 cents on Trillium, 35% less compared to TPU v5e. </span></p></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="5" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/5_AQdKyIu.max-1000x1000.png" />
        
        </a>
      
        <figcaption class="article-image__caption "><p>Figure 4: Diffusion cost to generate 1000 images. Google internal data. Measured using the SDXL model on Cloud TPU v5e-4 and Cloud TPU Trillium 4-chip. Resolution: 1024x1024, batch size per device: 2, decode steps: 4. Cost is based on the 3Y CUD prices for Cloud TPU v5e-4 and Cloud TPU Trillium 4-chip in the US. As of April 2025.</p></figcaption>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">A3 Ultra and A4 VMs MLPerf 5.0 Inference results</strong></h3>
<p><span style="vertical-align: baseline;">For </span><a href="https://mlcommons.org/benchmarks/inference-datacenter/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">MLPerf™ Inference v5.0</span></a><span style="vertical-align: baseline;">, we submitted 15 results, including our first submission with A3 Ultra (NVIDIA H200) and </span><a href="https://cloud.google.com/blog/products/compute/introducing-a4-vms-powered-by-nvidia-b200-gpu-aka-blackwell"><span style="text-decoration: underline; vertical-align: baseline;">A4</span></a><span style="vertical-align: baseline;"> (NVIDIA HGX B200) VMs. The A3 Ultra VM is powered by eight NVIDIA H200 Tensor Core GPUs and offers 3.2 Tbps of GPU-to-GPU non-blocking network bandwidth and twice the high bandwidth memory (HBM) compared to A3 Mega with NVIDIA H100 GPUs. Google Cloud's A3 Ultra demonstrated highly competitive performance, achieving results comparable to NVIDIA's peak GPU submissions across LLMs, MoE, image, and recommendation models. </span></p>
<p><span style="vertical-align: baseline;">Google Cloud was the only cloud provider to submit results on NVIDIA HGX B200 GPUs, demonstrating excellent performance of A4 VM for serving LLMs including Llama 3.1 405B (a new benchmark introduced in MLPerf 5.0). A3 Ultra and A4 VMs both deliver powerful inference performance, a testament to our deep partnership with NVIDIA to provide infrastructure for the most demanding AI workloads.</span></p>
<p><span style="vertical-align: baseline;">Customers like </span><a href="https://www.jetbrains.com/" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">JetBrains</span></a><span style="vertical-align: baseline;"> are using Google Cloud GPU instances to accelerate their inference workloads:</span></p>
<p style="padding-left: 40px;"><span style="font-style: italic; vertical-align: baseline;">“We’ve been using A3 Mega VMs with NVIDIA H100 Tensor Core GPUs on Google Cloud to run LLM inference across multiple regions. Now, we’re excited to start using A4 VMs powered by NVIDIA HGX B200 GPUs, which we expect will further reduce latency and enhance the responsiveness of AI in JetBrains IDEs.” </span><span style="vertical-align: baseline;">- Vladislav Tankov, Director of AI, JetBrains</span></p>
<h3><strong style="vertical-align: baseline;">AI Hypercomputer is powering the age of AI inference</strong></h3>
<p><span style="vertical-align: baseline;">Google's innovations in AI inference, including hardware advancements in Google Cloud TPUs and NVIDIA GPUs, plus software innovations such as JetStream, MaxText, and MaxDiffusion, are enabling AI breakthroughs with integrated software frameworks and hardware accelerators. Learn more about using </span><a href="https://cloud.google.com/solutions/ai-hypercomputer"><span style="text-decoration: underline; vertical-align: baseline;">AI Hypercomputer</span></a><span style="vertical-align: baseline;"> for inference. Then, check out these </span><a href="https://github.com/AI-Hypercomputer/JetStream/tree/main" rel="noopener" target="_blank"><span style="text-decoration: underline; vertical-align: baseline;">JetStream</span></a><span style="vertical-align: baseline;"> and </span><a href="https://cloud.google.com/tpu/docs/tutorials/LLM/maxdiffusion-inference-v6e"><span style="text-decoration: underline; vertical-align: baseline;">MaxDiffusion</span></a><span style="vertical-align: baseline;"> recipes to get started today.</span></p></div>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://cloud.google.com/blog/products/data-analytics/search-indexes-with-column-granularity-in-bigquery/</id>
            <title>New column-granularity indexing in BigQuery offers a leap in query performance</title>
            <link>https://cloud.google.com/blog/products/data-analytics/search-indexes-with-column-granularity-in-bigquery/</link>
            <guid isPermaLink="false">https://cloud.google.com/blog/products/data-analytics/search-indexes-with-column-granularity-in-bigquery/</guid>
            <pubDate></pubDate>
            <updated>Thu, 08 May 2025 16:00:00 +0000</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">BigQuery delivers </span><a href="https://cloud.google.com/blog/products/data-analytics/improved-text-analytics-in-bigquery-search-indexes-now-ga?e=48754805"><span style="text-decoration: underline; vertical-align: baseline;">optimized search/lookup query performance</span></a><span style="vertical-align: baseline;"> by efficiently pruning irrelevant files. However, in some cases, additional column information is required for search indexes to further optimize query performance. To help, we </span><a href="https://cloud.google.com/bigquery/docs/release-notes#March_26_2025"><span style="text-decoration: underline; vertical-align: baseline;">recently announced</span></a><span style="vertical-align: baseline;"> indexing with column granularity, which lets BigQuery pinpoint relevant data within columns, for faster search queries and lower costs. </span></p>
<p><span style="vertical-align: baseline;">BigQuery arranges table data into one or more physical files, each holding N rows. This data is stored in a columnar format, meaning each column has its own dedicated file block. You can learn more about this in the </span><a href="https://cloud.google.com/blog/topics/developers-practitioners/bigquery-admin-reference-guide-storage"><span style="text-decoration: underline; vertical-align: baseline;">BigQuery Storage Internals blog</span></a><span style="vertical-align: baseline;">. The default search index is at the file level, which means it maintains mappings from a data token to all the files containing it. Thus, at query time, the search index helps reduce the search space by only scanning those relevant files. This file-level indexing approach excels when search tokens are selective, appearing in only a few files. However, scenarios arise where search tokens are selective within specific columns but common across others, causing these tokens to appear in most files, and thus diminishing the effectiveness of file-level indexes.</span></p>
<p><span style="vertical-align: baseline;">For example, imagine a scenario where we have a collection of technical articles stored in a simplified table named </span><code style="vertical-align: baseline;">TechArticles</code><span style="vertical-align: baseline;"> with two columns — </span><code style="vertical-align: baseline;">Title </code><span style="vertical-align: baseline;">and </span><code style="vertical-align: baseline;">Content</code><span style="vertical-align: baseline;">. And let's assume that the data is distributed across four files, as shown below. </span></p></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="1 - Table content stored across files" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_-_Table_content_stored_across_files.max-1000x1000.png" />
        
        </a>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Our goal is to search for articles specifically related to Google Cloud Logging. Note that:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">The tokens "google", "cloud", and "logging" appear in every file.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Those three tokens  also appear in the "Title" column, but only in the first file.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Therefore, the combination of the three tokens is common overall, but highly selective in the "Title" column.</span></p>
</li>
</ul>
<p><span style="vertical-align: baseline;">Now, let’s say, we </span><a href="https://cloud.google.com/bigquery/docs/search-index#create_a_search_index"><span style="text-decoration: underline; vertical-align: baseline;">create a search index</span></a><span style="vertical-align: baseline;"> on both columns of the table with the following DDL statement:</span></p></div>
<div class="block-paragraph_advanced"><pre class="language-plain"><code>CREATE SEARCH INDEX myIndex ON myDataset.TechArticles(Title, Content);</code></pre></div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">The search index stores the mapping of data tokens to the data files containing the tokens, without any column information; the index looks like the following (showing the three tokens of interest: "google", "cloud", and "logging"):</span></p></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="2 - File-level Index" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_-_File-level_Index.max-1000x1000.jpg" />
        
        </a>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">With the usual query </span><code style="vertical-align: baseline;">SELECT * FROM TechArticles WHERE SEARCH(Title, "Google Cloud Logging")</code><span style="vertical-align: baseline;">, using the index without column information, BigQuery ends up scanning all four files, adding unnecessary processing and latency to your query.</span></p></div>
<div class="block-aside"><dl>
    <dt>aside_block</dt>
    <dd>&lt;ListValue: [StructValue([(&#x27;title&#x27;, &#x27;$300 in free credit to try Google Cloud data analytics&#x27;), (&#x27;body&#x27;, &lt;wagtail.rich_text.RichText object at 0x3e753c5027f0&gt;), (&#x27;btn_text&#x27;, &#x27;Start building for free&#x27;), (&#x27;href&#x27;, &#x27;http://console.cloud.google.com/freetrial?redirectPath=/bigquery/&#x27;), (&#x27;image&#x27;, None)])]&gt;</dd>
</dl></div>
<div class="block-paragraph_advanced"><h3><strong style="vertical-align: baseline;">Indexing with column granularity</strong></h3>
<p><span style="vertical-align: baseline;">Indexing with column granularity, a new public preview feature in BigQuery, addresses this challenge by adding column information in the indexes. This lets BigQuery leverage the indexes to pinpoint relevant data within columns, even when the search tokens are prevalent across the table's files.</span></p>
<p><span style="vertical-align: baseline;">Let’s go back to the above example. Now we can create the index with COLUMN granularity as follows:</span></p></div>
<div class="block-paragraph_advanced"><pre class="language-plain"><code>CREATE SEARCH INDEX myIndex ON myDataset.TechArticles(Title, Content)
OPTIONS (default_index_column_granularity = 'COLUMN');</code></pre></div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">The index now stores the column information associated with each data token. The index is as follows:</span></p></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="3 - Column-level Index" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/3_-_Column-level_Index.max-1000x1000.jpg" />
        
        </a>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">Using the same query </span><code style="vertical-align: baseline;">SELECT * FROM TechArticles WHERE SEARCH(Title, "Google Cloud Logging")</code><span style="vertical-align: baseline;"> as above but using the index with column information, BigQuery now only needs to scan </span><strong style="vertical-align: baseline;">file1</strong><span style="vertical-align: baseline;"> since the index lookup is the intersection of the following:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Files where Token='google' AND Column='Title' (file1)</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Files where Token='cloud' AND Column='Title' (file1, file2, file3, and file4)</span></p>
</li>
<li style="vertical-align: baseline;">
<p><span style="vertical-align: baseline;">Files where Token'='logging' AND Column='Title' (file1).</span></p>
</li>
</ul>
<h3><strong style="vertical-align: baseline;">Performance improvement benchmark results</strong></h3>
<p><span style="vertical-align: baseline;">We benchmarked query performance on a 1TB table containing </span><a href="https://cloud.google.com/logging"><span style="text-decoration: underline; vertical-align: baseline;">Google Cloud Logging</span></a><span style="vertical-align: baseline;"> data of an internal Google test project with the following query:</span></p></div>
<div class="block-paragraph_advanced"><pre class="language-plain"><code>SELECT COUNT(*)
FROM `dataset.log_1T`
WHERE SEARCH((logName, trace, labels, metadata), 'appengine');</code></pre></div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">In this benchmark query, the token 'appengine' appears infrequently in the columns used for query filtering, but is more common in other columns. The default search index already helped reduce a large portion of the search space, resulting in half the execution time, reducing processed bytes and slot usage. By employing column granularity indexing, the improvements are even more significant.</span></p></div>
<div class="block-image_full_width">






  
    <div class="article-module h-c-page">
      <div class="h-c-grid">
  

    <figure class="article-image--large
      
      
        h-c-grid__col
        h-c-grid__col--6 h-c-grid__col--offset-3
        
        
      ">

      
      
        
        <img alt="4 - benchmark result" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/4_-_benchmark_result_v1.max-1000x1000.jpg" />
        
        </a>
      
    </figure>

  
      </div>
    </div>
  




</div>
<div class="block-paragraph_advanced"><p><span style="vertical-align: baseline;">In short, column-granularity indexing in BigQuery offers the following benefits: </span></p>
<ul>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Enhanced query performance:</strong><span style="vertical-align: baseline;"> By precisely identifying relevant data within columns, column-granularity indexing significantly accelerates query execution, especially for queries with selective search tokens within specific columns.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Improved cost efficiency:</strong><span style="vertical-align: baseline;"> Index pruning results in reduced bytes processed and/or slot time, translating to improved cost efficiency.</span></p>
</li>
</ul>
<p><span style="vertical-align: baseline;">This is particularly valuable in scenarios where search tokens are selective within specific columns but common across others, or where queries frequently filter or aggregate data based on specific columns.</span></p>
<h3><strong style="vertical-align: baseline;">Best practices and getting started</strong></h3>
<p><span style="vertical-align: baseline;">Indexing with column granularity represents a significant advancement in BigQuery's indexing capabilities, letting you achieve greater query performance and cost efficiency.</span></p>
<p><span style="vertical-align: baseline;">For best results, consider the following best practices:</span></p>
<ul>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Identify high-impact columns:</strong><span style="vertical-align: baseline;"> Analyze your query patterns to identify columns that are frequently used in filters or aggregations and would benefit from column-granularity indexing.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Monitor performance:</strong><span style="vertical-align: baseline;"> Continuously monitor query performance and adjust your indexing strategy as needed.</span></p>
</li>
<li style="vertical-align: baseline;">
<p><strong style="vertical-align: baseline;">Consider indexing and storage costs:</strong><span style="vertical-align: baseline;"> While column-granularity indexing can optimize query performance, be mindful of potential increases in indexing and storage costs.</span></p>
</li>
</ul>
<p><span style="vertical-align: baseline;">To get started, simply enable indexing with column granularity. For more information, refer to the </span><a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_search_index_statement"><span style="text-decoration: underline; vertical-align: baseline;">CREATE SEARCH INDEX DDL documentation</span></a><span style="vertical-align: baseline;">.</span></p></div>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>