<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>Google AI Blog</title>
        <link>http://blog.research.google/</link>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-1569605132526995799</id>
            <title>Generative AI to quantify uncertainty in weather forecasting</title>
            <link>http://blog.research.google/2024/03/generative-ai-to-quantify-uncertainty.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-1569605132526995799</guid>
            <pubDate></pubDate>
            <updated>2024-03-29T11:03:10.261-07:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdlATDKSy6EC1BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s72-c/image3.gif"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Lizao (Larry) Li, Software Engineer, and Rob Carver, Research Scientist, Google Research</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdlATDKSy6EC1BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s1000/image3.gif" style="display: none;" />

<p>
Accurate weather forecasts can have a direct impact on people’s lives, from helping make routine decisions, like what to pack for a day’s activities, to informing urgent actions, for example, protecting people in the face of hazardous weather conditions. The importance of accurate and timely weather forecasts will only increase as the climate changes. Recognizing this, we at Google have been investing in weather and climate research to help ensure that the forecasting technology of tomorrow can meet the demand for reliable weather information. Some of our recent innovations include <a href="https://blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html">MetNet-3</a>, Google's high-resolution forecasts up to 24-hours into the future, and <a href="https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/">GraphCast</a>, a weather model that can predict weather up to 10 days ahead.
</p>
<a name="more"></a> 

<p>
Weather is inherently stochastic. To quantify the uncertainty, traditional methods rely on physics-based simulation to generate an ensemble of forecasts. However, it is computationally costly to generate a large ensemble so that rare and extreme weather events can be discerned and characterized accurately.  
</p>
<p>
With that in mind, we are excited to announce our latest innovation designed to accelerate progress in weather forecasting, <a href="https://www.science.org/doi/10.1126/sciadv.adk4489">Scalable Ensemble Envelope Diffusion Sampler</a> (SEEDS), recently published in <em><a href="https://www.science.org/journal/sciadv">Science Advances</a></em>. SEEDS is a generative AI model that can efficiently generate ensembles of weather forecasts <em>at scale </em>at a small fraction of the cost of traditional physics-based forecasting models. This technology opens up novel opportunities for weather and climate science, and it represents one of the first applications to weather and climate forecasting of probabilistic diffusion models, a generative AI technology behind recent advances in media generation.
</p>
<br /> 

<h2>The need for probabilistic forecasts: the butterfly effect</h2>

<p>
In December 1972, at the <a href="https://www.aaas.org/">American Association for the Advancement of Science</a> meeting in Washington, D.C., MIT meteorology professor <a href="https://en.wikipedia.org/wiki/Edward_Norton_Lorenz">Ed Lorenz</a> gave a talk entitled, “Does the Flap of a Butterfly's Wings in Brazil Set Off a Tornado in Texas?” which contributed to the term “<a href="https://en.wikipedia.org/wiki/Butterfly_effect">butterfly effect</a>”. He was building on his earlier, landmark 1963 paper where he examined the feasibility of “very-long-range weather prediction” and described how errors in initial conditions grow exponentially when integrated in time with numerical weather prediction models. This exponential error growth, known as chaos, results in a deterministic predictability limit that restricts the use of individual forecasts in decision making, because they do not quantify the inherent uncertainty of weather conditions. This is particularly problematic when forecasting extreme weather events, such as hurricanes, heatwaves, or floods.
</p>
<p>
Recognizing the limitations of deterministic forecasts, weather agencies around the world issue <em>probabilistic forecasts</em>. Such forecasts are based on ensembles of deterministic forecasts, each of which is generated by including synthetic noise in the initial conditions and stochasticity in the physical processes. Leveraging the fast error growth rate in weather models, the forecasts in an ensemble are purposefully different: the initial uncertainties are tuned to generate runs that are as different as possible and the stochastic processes in the weather model introduce additional differences during the model run. The error growth is mitigated by averaging all the forecasts in the ensemble and the variability in the ensemble of forecasts quantifies the uncertainty of the weather conditions.
</p>
<p>
While effective, generating these probabilistic forecasts is computationally costly. They require running highly complex numerical weather models on massive supercomputers multiple times. Consequently, many operational weather forecasts can only afford to generate ~10–50 ensemble members for each forecast cycle. This is a problem for users concerned with the likelihood of rare but high-impact weather events, which typically require much larger ensembles to assess beyond a few days. For instance, one would need a 10,000-member ensemble to forecast the likelihood of events with 1% probability of occurrence with a relative error less than 10%. Quantifying the probability of such extreme events could be useful, for example, for emergency management preparation or for energy traders.
</p>
<br /> 

<h2>SEEDS: AI-enabled advances</h2>

<p>
In the aforementioned <a href="https://www.science.org/doi/10.1126/sciadv.adk4489">paper</a>, we present the Scalable Ensemble Envelope Diffusion Sampler (SEEDS), a generative AI technology for weather forecast ensemble generation. SEEDS is based on <a href="https://blog.research.google/2021/07/high-fidelity-image-generation-using.html">denoising diffusion probabilistic</a> models, a state-of-the-art generative AI method pioneered in part by Google Research.
</p>
<p>
SEEDS can generate a large ensemble conditioned on as few as one or two forecasts from an operational numerical weather prediction system. The generated ensembles not only yield plausible real-weather–like forecasts but also match or exceed physics-based ensembles in skill metrics such as the <a href="https://www.jstor.org/stable/26201352">rank histogram</a>, the <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">root-mean-squared error</a> (RMSE), and the <a href="https://www.tandfonline.com/doi/abs/10.1198/016214506000001437">continuous ranked probability score</a> (CRPS). In particular, the generated ensembles assign more accurate likelihoods to the tail of the forecast distribution, such as ±2σ and ±3σ weather events. Most importantly, the computational cost of the model is negligible when compared to the hours of computational time needed by supercomputers to make a forecast. It has a throughput of 256 ensemble members (at 2° resolution) per 3 minutes on Google Cloud TPUv3-32 instances and can easily scale to higher throughput by deploying more accelerators. 
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdlATDKSy6EC1BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s1000/image3.gif" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglI5U51vvhkA4cAuVvMLn0TbbL5pdlFL-LO1sNnqLyUieA6A88I5HrhJlszxR1GKQqSK5wsdlATDKSy6EC1BsNF7tzS6oVlFLtau13mVFLk954nFu85HDMP3PrQboG4eXExEtUjEuDRFpcrMqE_F0ikSwXiWBECAfJiLbjr6h6523DROJkbC284xX35zC7/s16000/image3.gif" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">SEEDS generates an order-of-magnitude more samples to in-fill distributions of weather patterns.</td></tr></tbody></table>
<div style="line-height: 40%;">
    <br />
</div>

<h2>Generating plausible weather forecasts</h2>


<p>
Generative AI is known to generate very detailed images and videos. This property is especially useful for generating ensemble forecasts that are consistent with plausible weather patterns, which ultimately result in the most added value for downstream applications.  As Lorenz points out, “The [weather forecast] maps which they produce should look like real weather maps." The figure below contrasts the forecasts from SEEDS to those from the operational U.S. weather prediction system (<a href="https://www.emc.ncep.noaa.gov/emc/pages/numerical_forecast_systems/gefs.php">Global Ensemble Forecast System</a>, GEFS) for a particular date during the <a href="https://en.wikipedia.org/wiki/2022_European_heatwaves">2022 European heat waves</a>. We also compare the results to the forecasts from a Gaussian model that predicts the univariate mean and standard deviation of each atmospheric field at each location, a common and computationally efficient but less sophisticated data-driven approach. This Gaussian model is meant to characterize the output of pointwise post-processing, which ignores correlations and treats each grid point as an independent random variable. In contrast, a real weather map would have detailed <em>correlational</em> structures. 
</p>
<p>
Because SEEDS directly models the joint distribution of the atmospheric state, it realistically captures both the spatial covariance and the correlation between mid-tropospheric geopotential and mean sea level pressure, both of which are closely related and are commonly used by weather forecasters for evaluation and verification of forecasts. Gradients in the mean sea level pressure are what drive winds at the surface, while gradients in mid-tropospheric geopotential create upper-level winds that move large-scale weather patterns. 
</p>
<p>
The generated samples from SEEDS shown in the figure below (frames Ca–Ch) display a geopotential trough west of Portugal with spatial structure similar to that found in the operational U.S. forecasts or the reanalysis based on observations. Although the Gaussian model predicts the marginal univariate distributions adequately, it fails to capture cross-field or spatial correlations. This hinders the assessment of the effects that these anomalies may have on hot air intrusions from North Africa, which can exacerbate heat waves over Europe.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQE94TGK404COMAKKxaPwUO9bD8gIzQfu6A0u5c-5xbGKhlUtBW_0KAj-Ur8kpgt5_f-IjAuFzeecpRbbWVujZNQVExTsl0UuDRtOb84Y8uFWc4G1UYYZos6gLVtIHQ3AZ7ojRqoMSmt8IHdTOSx365AaoNyUfNMi1ksC0Wh_axeD_THB6sOmnZZHhrvHQ/s1999/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQE94TGK404COMAKKxaPwUO9bD8gIzQfu6A0u5c-5xbGKhlUtBW_0KAj-Ur8kpgt5_f-IjAuFzeecpRbbWVujZNQVExTsl0UuDRtOb84Y8uFWc4G1UYYZos6gLVtIHQ3AZ7ojRqoMSmt8IHdTOSx365AaoNyUfNMi1ksC0Wh_axeD_THB6sOmnZZHhrvHQ/s16000/image2.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Stamp maps over Europe on 2022/07/14 at 0:00 UTC. The contours are for the mean sea level pressure (dashed lines mark isobars below 1010 hPa) while the heatmap depicts the geopotential height at the 500 hPa pressure level. (A) The&nbsp;<a href="https://www.ecmwf.int/en/forecasts/dataset/ecmwf-reanalysis-v5">ERA5</a>&nbsp;reanalysis, a proxy for real observations. (Ba-Bb) 2 members from the 7-day U.S. operational forecasts used as seeds to our model. (Ca-Ch) 8 samples drawn from SEEDS. (Da-Dh) 8 non-seeding members from the 7-day U.S. operational ensemble forecast. (Ea-Ed) 4 samples from a pointwise Gaussian model parameterized by the mean and variance of the entire U.S. operational ensemble.</td></tr></tbody></table>

<div style="line-height: 40%;">
    <br />
</div>
<h2>Covering extreme events more accurately  </h2>

<p>
Below we show the joint distributions of temperature at 2 meters and total column water vapor near Lisbon during the extreme heat event on 2022/07/14, at 1:00 local time. We used the 7-day forecasts issued on 2022/07/07. For each plot, we generate 16,384-member ensembles with SEEDS. The observed weather event from ERA5 is denoted by the star. The operational ensemble is also shown, with squares denoting the forecasts used to seed the generated ensembles, and triangles denoting the rest of ensemble members.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVbbmrrrJ5L1NVb_O7WPUD-d6ULlTJTSns6ZaqjxOqZ4YAi4zOiT72rfMBf8EGTe0kdofIrWAMESq1m2v9IBjnd_k6UAIDM7LvhbxdVr41FOQ0fqkKeERF_QqXbxs94qKLdMxR-A7Hbxkjd4zZn07AlldAsuvn7jsYCu-V3UVAatovY1ELbrcLQz5I1ppX/s1999/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVbbmrrrJ5L1NVb_O7WPUD-d6ULlTJTSns6ZaqjxOqZ4YAi4zOiT72rfMBf8EGTe0kdofIrWAMESq1m2v9IBjnd_k6UAIDM7LvhbxdVr41FOQ0fqkKeERF_QqXbxs94qKLdMxR-A7Hbxkjd4zZn07AlldAsuvn7jsYCu-V3UVAatovY1ELbrcLQz5I1ppX/s16000/image1.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">SEEDS provides better statistical coverage of the 2022/07/14 European extreme heat event, denoted by the brown star . Each plot shows the values of the total column-integrated water vapor (TCVW) vs. temperature over a grid point near Lisbon, Portugal from 16,384 samples generated by our models, shown as green dots, conditioned on 2 seeds (blue squares) taken from the 7-day U.S. operational ensemble forecasts (denoted by the sparser brown triangles). The valid forecast time is 1:00 local time. The solid contour levels correspond to iso-proportions of the kernel density of SEEDS, with the outermost one encircling 95% of the mass and 11.875% between each level.</td></tr></tbody></table>
<br />

<p>
According to the U.S. operational ensemble, the observed event was so unlikely seven days prior that none of its 31 members predicted near-surface temperatures as warm as those observed. Indeed, the event probability computed from a Gaussian kernel density estimate is lower than 1%, which means that ensembles with less than 100 members are unlikely to contain forecasts as extreme as this event. In contrast, the SEEDS ensembles are able to extrapolate from the two seeding forecasts, providing an envelope of possible weather states with much better statistical coverage of the event. This allows both quantifying the probability of the event taking place and sampling weather regimes under which it would occur. Specifically, our highly scalable generative approach enables the creation of very large ensembles that can characterize very rare events by providing samples of weather states exceeding a given threshold for any user-defined diagnostic.
</p>
<br /> 

<h2>Conclusion and future outlook</h2>

<p>
SEEDS leverages the power of generative AI to produce ensemble forecasts comparable to those from the operational U.S. forecast system, but at an accelerated pace. The results reported in this paper need only 2 seeding forecasts from the operational system, which generates 31 forecasts in its current version. This leads to a hybrid forecasting system where a few weather trajectories computed with a physics-based model are used to seed a diffusion model that can generate additional forecasts much more efficiently. This methodology provides an alternative to the current operational weather forecasting paradigm, where the computational resources saved by the statistical emulator could be allocated to increasing the resolution of the physics-based model or issuing forecasts more frequently.
</p>
<p>
We believe that SEEDS represents just one of the many ways that AI will accelerate progress in operational numerical weather prediction in coming years. We hope this demonstration of the  utility of generative AI for weather forecast emulation and post-processing will spur its application in research areas such as climate risk assessment, where generating a large number of ensembles of climate projections is crucial to accurately quantifying the uncertainty about future climate.
</p>
<br /> 

<h2>Acknowledgements</h2>

<p>
<em>All SEEDS authors, Lizao Li, Rob Carver, Ignacio Lopez-Gomez, Fei Sha and John Anderson, co-authored this blog post, with Carla Bromberg as Program Lead. We also thank Tom Small who designed the animation. Our colleagues at Google Research have provided invaluable advice to the SEEDS work. Among them, we thank Leonardo Zepeda-Núñez, Zhong Yi Wan, Stephan Rasp, Stephan Hoyer, and Tapio Schneider for their inputs and useful discussion. We thank Tyler Russell for additional technical program management, as well as Alex Merose for data coordination and support. We also thank Cenk Gazen, Shreya Agrawal, and Jason Hickey for discussions in the early stage of the SEEDS work. </em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-1799535679952845079</id>
            <title>AutoBNN: Probabilistic time series forecasting with compositional bayesian neural networks</title>
            <link>http://blog.research.google/2024/03/autobnn-probabilistic-time-series.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-1799535679952845079</guid>
            <pubDate></pubDate>
            <updated>2024-03-29T12:00:03.604-07:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgd5Wc54p1HvgIokpazxDsMo1u6i9wg3ovpNOiFc4-wYwebETvjs9-hm2wxZ4osNbBAxhet8To3hwGg-whFScksHQB_BP1kS4Z8Cu7FQT2bjVtJl4trPid-OxCyYocwyRTN66tuvAedu9z0FepBg4zZvmLbLxY6uuib8p5jVH2kfb3RxT_HMABsKMXuSFXr/s72-c/AutoBNN.jpg"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Urs Köster, Software Engineer, Google Research</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgd5Wc54p1HvgIokpazxDsMo1u6i9wg3ovpNOiFc4-wYwebETvjs9-hm2wxZ4osNbBAxhet8To3hwGg-whFScksHQB_BP1kS4Z8Cu7FQT2bjVtJl4trPid-OxCyYocwyRTN66tuvAedu9z0FepBg4zZvmLbLxY6uuib8p5jVH2kfb3RxT_HMABsKMXuSFXr/s320/AutoBNN.jpg" style="display: none;" />

<p>
<a href="https://en.wikipedia.org/wiki/Time_series">Time series</a> problems are ubiquitous, from forecasting weather and traffic patterns to understanding economic trends. <a href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian</a> approaches start with an assumption about the data's patterns (prior probability), collecting evidence (e.g., new time series data), and continuously updating that assumption to form a posterior probability distribution. Traditional Bayesian approaches like <a href="https://gaussianprocess.org/gpml/">Gaussian processes</a> (GPs) and <a href="https://blog.tensorflow.org/2019/03/structural-time-series-modeling-in.html">Structural Time Series</a> are extensively used for modeling time series data, e.g., the commonly used <a href="https://gml.noaa.gov/ccgg/trends/">Mauna Loa CO2</a> dataset. However, they often rely on domain experts to painstakingly select appropriate model components and may be computationally expensive. Alternatives such as neural networks lack interpretability, making it difficult to understand how they generate forecasts, and don't produce reliable confidence intervals. 
</p>
<a name="more"></a>
<p>
To that end, we introduce <a href="https://github.com/tensorflow/probability/tree/main/spinoffs/autobnn">AutoBNN</a>, a new open-source package written in <a href="https://github.com/google/jax">JAX</a>. AutoBNN automates the discovery of interpretable time series forecasting models, provides high-quality uncertainty estimates, and scales effectively for use on large datasets. We describe how AutoBNN combines the interpretability of traditional probabilistic approaches with the scalability and flexibility of neural networks.
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h2>AutoBNN</h2>


<p>
AutoBNN is based on a <a href="https://proceedings.mlr.press/v28/duvenaud13.html">line</a> <a href="https://royalsocietypublishing.org/doi/10.1098/rsta.2011.0550">of</a> <a href="https://proceedings.mlr.press/v202/saad23a.html">research</a> that over the past decade has yielded improved predictive accuracy by modeling time series using GPs with learned <a href="https://www.cs.toronto.edu/~duvenaud/cookbook/">kernel</a> structures. The kernel function of a GP encodes assumptions about the function being modeled, such as the presence of trends, periodicity or noise.  With learned GP kernels, the kernel function is defined compositionally: it is either a base kernel (such as <code>Linear</code>, <code>Quadratic</code>, <code>Periodic</code>, <code><a href="https://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function">Matérn</a></code> or <code>ExponentiatedQuadratic</code>) or a composite that combines two or more kernel functions using operators such as <code>Addition</code>, <code>Multiplication</code>, or <code><a href="https://icml.cc/Conferences/2010/papers/170.pdf">ChangePoint</a></code>. This compositional kernel structure serves two related purposes. First, it is simple enough that a user who is an expert about their data, but not necessarily about GPs, can construct a reasonable prior for their time series. Second, techniques like <a href="https://www.stats.ox.ac.uk/~doucet/doucet_defreitas_gordon_smcbookintro.pdf">Sequential Monte Carlo</a> can be used for discrete searches over small structures and can output interpretable results.</p>

<p>
AutoBNN improves upon these ideas, replacing the GP with <a href="https://www.cs.toronto.edu/~duvenaud/distill_bayes_net/public/">Bayesian neural networks</a> (BNNs) while retaining the compositional kernel structure. A BNN is a neural network with a probability distribution over weights rather than a fixed set of weights. This induces a distribution over outputs, capturing uncertainty in the predictions. BNNs bring the following advantages over GPs: First, training large GPs is computationally expensive, and traditional training algorithms scale as the cube of the number of data points in the time series. In contrast, for a fixed width, training a BNN will often be approximately linear in the number of data points. Second, BNNs lend themselves better to GPU and <a href="https://cloud.google.com/tpu?hl=en">TPU</a> hardware acceleration than GP training operations. Third, compositional BNNs can be easily combined with <a href="https://arxiv.org/abs/2007.06823">traditional deep BNNs</a>, which have the ability to do feature discovery. One could imagine "hybrid" architectures, in which users specify a top-level structure of <code>Add</code>(<code>Linear</code>, <code>Periodic</code>, <code>Deep</code>), and the deep BNN is left to learn the contributions from potentially high-dimensional covariate information.
</p>

<p>
How might one translate a GP with compositional kernels into a BNN then? A single layer neural network will typically converge to a GP as the number of neurons (or "width") <a href="https://link.springer.com/chapter/10.1007/978-1-4612-0745-0_2">goes to infinity</a>. More recently, researchers have <a href="https://openreview.net/forum?id=gRwh5HkdaTm">discovered</a> a correspondence in the other direction — many popular GP <a href="https://www.cs.toronto.edu/~duvenaud/cookbook/">kernels</a> (such as <code>Matern</code>, <code>ExponentiatedQuadratic</code>, <code>Polynomial</code> or <code>Periodic</code>) can be obtained as infinite-width BNNs with appropriately chosen activation functions and weight distributions. Furthermore, these BNNs remain close to the corresponding GP even when the width is very much less than infinite. For example, the figures below show the difference in the <a href="https://en.wikipedia.org/wiki/Covariance_matrix#:~:text=In%20probability%20theory%20and%20statistics,of%20a%20given%20random%20vector">covariance</a> between pairs of observations, and <a href="https://en.wikipedia.org/wiki/Kriging">regression</a> results of the true GPs and their corresponding width-10 neural network versions.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHJ7hHI33S76Id3RrWCYezQKky9oELeuWf_CTm7GYadxpV7-B9GSQKCZgTmVQABi9zpWcEK8uvTYITyX2_jcbv_qF-eGv2C1QkU9oDCAS09FfoCne81yEAqC5moTNIqsn05aHfWNr8uy48N3UfV_tRGOyGrrQvB8l7RegzAq5_LNK2W8_Y_gSavdfi5aDI/s1350/image3.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHJ7hHI33S76Id3RrWCYezQKky9oELeuWf_CTm7GYadxpV7-B9GSQKCZgTmVQABi9zpWcEK8uvTYITyX2_jcbv_qF-eGv2C1QkU9oDCAS09FfoCne81yEAqC5moTNIqsn05aHfWNr8uy48N3UfV_tRGOyGrrQvB8l7RegzAq5_LNK2W8_Y_gSavdfi5aDI/s16000/image3.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Comparison of <a href="https://en.wikipedia.org/wiki/Gram_matrix">Gram matrices</a> between true GP kernels (top row) and their width 10 neural network approximations (bottom row).</td></tr></tbody></table>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoidYqlAK2J1n4y71Qn-WuIcmaxGI9ynwSjtHAvyukuY_q5QcX4pVEheX2pwMxIhkAu7_OZR-0s7N7e-cU-caromj1wntP7E1txZfxHqh2yeTedusA90k9hFZ2yvzEZmC2QlPyR7trgVuMro-MoicBxpAbrkQXs2F9h1uux3AXzUENmJ0NA8Ch9dyICT15/s1328/image4.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoidYqlAK2J1n4y71Qn-WuIcmaxGI9ynwSjtHAvyukuY_q5QcX4pVEheX2pwMxIhkAu7_OZR-0s7N7e-cU-caromj1wntP7E1txZfxHqh2yeTedusA90k9hFZ2yvzEZmC2QlPyR7trgVuMro-MoicBxpAbrkQXs2F9h1uux3AXzUENmJ0NA8Ch9dyICT15/s16000/image4.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Comparison of regression results between true GP kernels (top row) and their width 10 neural network approximations (bottom row).</td></tr></tbody></table>



<p>
Finally, the translation is completed with <a href="https://arxiv.org/abs/1905.06076">BNN analogues</a> of the <code>Addition</code> and <code>Multiplication</code> operators over GPs, and input warping to produce periodic kernels. BNN addition is straightforwardly given by adding the outputs of the component BNNs. BNN multiplication is achieved by multiplying the activations of the hidden layers of the BNNs and then applying a shared dense layer. We are therefore limited to only multiplying BNNs with the same hidden width.
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h2>Using AutoBNN</h2>


<p>
The AutoBNN <a href="https://github.com/tensorflow/probability/tree/main/spinoffs/autobnn">package</a> is available within <a href="https://www.tensorflow.org/probability">Tensorflow Probability</a>. It is implemented in <a href="https://github.com/google/jax">JAX</a> and uses the <a href="https://github.com/google/flax">flax.linen</a> neural network library. It implements all of the base kernels and operators discussed so far (<code>Linear</code>, <code>Quadratic</code>, <code>Matern</code>, <code>ExponentiatedQuadratic</code>, <code>Periodic</code>, <code>Addition</code>, <code>Multiplication</code>) plus one new kernel and three new operators:  
</p>

<ul>

<li>a <code>OneLayer</code> kernel, a single hidden layer <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> BNN,

</li><li>a <code><a href="https://icml.cc/Conferences/2010/papers/170.pdf">ChangePoint</a></code> operator that allows smoothly switching between two kernels,

</li><li>a <code>LearnableChangePoint</code> operator which is the same as <code>ChangePoint</code> except position and slope are given prior distributions and can be learnt from the data, and

</li><li>a <code>WeightedSum</code> operator.
</li>
</ul>


<p>
<code>WeightedSum</code> combines two or more BNNs with learnable mixing weights, where the learnable weights follow a <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet prior</a>. By default, a flat Dirichlet distribution with concentration 1.0 is used.
</p>

<p>
<code>WeightedSums</code> allow a "soft" version of structure discovery, i.e., training a linear combination of many possible models at once. In contrast to structure discovery with discrete structures, such as in <a href="https://proceedings.mlr.press/v202/saad23a.html">AutoGP</a>, this allows us to use standard gradient methods to learn structures, rather than using expensive discrete optimization. Instead of evaluating potential combinatorial structures in series, WeightedSum allows us to evaluate them in parallel. 
</p>

<p>
To easily enable exploration, AutoBNN defines a <a href="https://github.com/tensorflow/probability/blob/main/spinoffs/autobnn/autobnn/models.py">number of model structures</a> that contain either top-level or internal <code>WeightedSums</code>. The names of these models can be used as the first parameter in any of the <a href="https://github.com/tensorflow/probability/blob/main/spinoffs/autobnn/autobnn/estimators.py">estimator</a> constructors, and include things like <code><a href="https://github.com/tensorflow/probability/blob/main/spinoffs/autobnn/autobnn/models.py#L133">sum_of_stumps</a></code> (the <code>WeightedSum</code> over all the base kernels) and <code>sum_of_shallow</code> (which adds all possible combinations of base kernels with all operators).</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNmWFuh7tVRkaF9o4nr3Fu7B2CNmXpDkGx8_9fMASh2olAfjlSdBXLj-0cgh7UIVWs6fHlNyyCvRPA_vc4eq-3lixkC2VXzCeSCZBFDHIc1qYfK53EwEdngf1KykzCfpPiIg3YoN46AZkBSSmCLrgPXX84PaZp_cxLrNnmojz2S6pLOCmTTT2niRi8Qfe5/s1389/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgNmWFuh7tVRkaF9o4nr3Fu7B2CNmXpDkGx8_9fMASh2olAfjlSdBXLj-0cgh7UIVWs6fHlNyyCvRPA_vc4eq-3lixkC2VXzCeSCZBFDHIc1qYfK53EwEdngf1KykzCfpPiIg3YoN46AZkBSSmCLrgPXX84PaZp_cxLrNnmojz2S6pLOCmTTT2niRi8Qfe5/s16000/image2.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Illustration of the <code>sum_of_stumps</code> model. The bars in the top row show the amount by which each base kernel contributes, and the bottom row shows the function represented by the base kernel. The resulting weighted sum is shown on the right.</td></tr></tbody></table>

<p>
The figure below demonstrates the technique of structure discovery on the N374 (a time series of yearly financial data starting from 1949) from the <a href="https://forecasters.org/resources/time-series-data/m3-competition/">M3</a> dataset. The six base structures were <code>ExponentiatedQuadratic</code> (which is the same as the Radial Basis Function kernel, or <a href="https://en.wikipedia.org/wiki/Radial_basis_function_kernel">RBF</a> for short), <code>Matern</code>, <code>Linear</code>, <code>Quadratic</code>, <code>OneLayer</code> and <code>Periodic</code> kernels. The figure shows the MAP estimates of their weights over an ensemble of 32 particles. All of the high likelihood particles gave a large weight to the <code>Periodic</code> component, low weights to <code>Linear</code>, <code>Quadratic</code> and <code>OneLayer</code>, and a large weight to either <code>RBF</code> or <code>Matern</code>.
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_5mU3VknB1oyCwNdCQj9kWTVV5J0BuylHB8W2LUK4sT6JpkOWdluZwh8_fKvRN5eSo2xBbQ0pRxDYa86IqML9H2-JZOmxxRJSm9ExG_PUr6U7iFl8nyp4lEaNpG3guYov3hPP3l9zifdu_iv_5aeP05OftccGqwJ7D0WAeMox_aWMGm3hN5nOkrj4BPxU/s868/image5.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_5mU3VknB1oyCwNdCQj9kWTVV5J0BuylHB8W2LUK4sT6JpkOWdluZwh8_fKvRN5eSo2xBbQ0pRxDYa86IqML9H2-JZOmxxRJSm9ExG_PUr6U7iFl8nyp4lEaNpG3guYov3hPP3l9zifdu_iv_5aeP05OftccGqwJ7D0WAeMox_aWMGm3hN5nOkrj4BPxU/s16000/image5.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Parallel coordinates plot of the <a href="https://www.probabilitycourse.com/chapter9/9_1_2_MAP_estimation.php">MAP</a> estimates of the base kernel weights over 32 particles. The <code>sum_of_stumps</code> model was trained on the N374 series from the M3 dataset (insert in blue). Darker lines correspond to particles with higher likelihoods.</td></tr></tbody></table>


<p>
By using <code>WeightedSums</code> as the inputs to other operators, it is possible to express rich combinatorial structures, while keeping models compact and the number of learnable weights small. As an example, we include the <code>sum_of_products</code> model (illustrated in the figure below) which first creates a pairwise product of two <code>WeightedSums</code>, and then a sum of the two products. By setting some of the weights to zero, we can create many different discrete structures. The total number of possible structures in this model is 2<sup>16</sup>, since there are 16 base kernels that can be turned on or off. All these structures are explored implicitly by training just this one model.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9VhSV6af55mkKxUKzpJJrqQiAV6WUWJ8HY9Q-5qcPB_mr8_P0lvrcGGkEUNe_-UB6Ri5VgWFkdHvRwEe7snZucQtvzMR_548jt4h2lbTzfnp7ZUeYFDmas7LwKc_9UAzdLE4gr8g9pVVkMXy9GU8qMUzrKfd9tjDEc2C4Ub6aXDzjHf2FjCryg_pWu39E/s1754/AutoBNN%20illustration.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9VhSV6af55mkKxUKzpJJrqQiAV6WUWJ8HY9Q-5qcPB_mr8_P0lvrcGGkEUNe_-UB6Ri5VgWFkdHvRwEe7snZucQtvzMR_548jt4h2lbTzfnp7ZUeYFDmas7LwKc_9UAzdLE4gr8g9pVVkMXy9GU8qMUzrKfd9tjDEc2C4Ub6aXDzjHf2FjCryg_pWu39E/s16000/AutoBNN%20illustration.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Illustration of the "sum_of_products" model. Each of the four WeightedSums have the same structure as the "sum_of_stumps" model.</td></tr></tbody></table>


<p>
We have found, however, that certain combinations of kernels (e.g., the product of <code>Periodic</code> and either the <code>Matern</code> or <code>ExponentiatedQuadratic</code>) lead to overfitting on many datasets. To prevent this, we have defined model classes like <code>sum_of_safe_shallow</code> that exclude such products when performing structure discovery with <code>WeightedSums</code>.
</p>

<p>
For training, AutoBNN provides <code>AutoBnnMapEstimator</code> and <code>AutoBnnMCMCEstimator</code> to perform MAP and MCMC inference, respectively. Either estimator can be combined with any of the six <a href="https://github.com/tensorflow/probability/blob/main/spinoffs/autobnn/autobnn/likelihoods.py">likelihood functions</a>, including four based on normal distributions with different noise characteristics for continuous data and two based on the negative binomial distribution for count data.  
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVzVWT-e-lcT53h75r2QJpR7iH9FAgCkpQY_oBNq7o1YoO4TkJ2GVpXLYcyY3RjOfgaXRM2LRII_jK31PbxTQF29yH1cTJRdI-XkXmnZMR_imlFv0uOuIPni3nW_vb1ercfuJuKHbrbuIA4bVR5EuGTs5iUHRXs-4WaA9wFEX54RwOJQt0BGMGfkNW4kxn/s1076/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVzVWT-e-lcT53h75r2QJpR7iH9FAgCkpQY_oBNq7o1YoO4TkJ2GVpXLYcyY3RjOfgaXRM2LRII_jK31PbxTQF29yH1cTJRdI-XkXmnZMR_imlFv0uOuIPni3nW_vb1ercfuJuKHbrbuIA4bVR5EuGTs5iUHRXs-4WaA9wFEX54RwOJQt0BGMGfkNW4kxn/s16000/image1.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Result from running AutoBNN on the <a href="https://gml.noaa.gov/ccgg/trends/">Mauna Loa CO2</a> dataset in our example <a href="https://github.com/tensorflow/probability/blob/main/discussion/examples/Forecasting_With_AutoBNN.ipynb">colab</a>. The model captures the trend and seasonal component in the data. Extrapolating into the future, the mean prediction slightly underestimates the actual trend, while the 95% confidence interval gradually increases.</td></tr></tbody></table>


<p>
To fit a model like in the figure above, all it takes is the following 10 lines of code, using the <a href="https://scikit-learn.org/stable/">scikit-learn</a>–inspired estimator interface:</p>


<pre class="prettyprint">import autobnn as ab

model = ab.operators.Add(
    bnns=(ab.kernels.PeriodicBNN(width=50),
          ab.kernels.LinearBNN(width=50),
          ab.kernels.MaternBNN(width=50)))

estimator = ab.estimators.AutoBnnMapEstimator(
    model, 'normal_likelihood_logistic_noise', jax.random.PRNGKey(42),
    periods=[12])

estimator.fit(my_training_data_xs, my_training_data_ys)
low, mid, high = estimator.predict_quantiles(my_training_data_xs)
</pre>

<br />

<div style="line-height: 40%;">
    <br />
</div>
<h2>Conclusion</h2>


<p>
<a href="https://github.com/tensorflow/probability/tree/main/spinoffs/autobnn">AutoBNN</a> provides a powerful and flexible framework for building sophisticated time series prediction models. By combining the strengths of BNNs and GPs with compositional kernels, AutoBNN opens a world of possibilities for understanding and forecasting complex data. We invite the community to try the&nbsp;<a href="https://github.com/tensorflow/probability/blob/main/discussion/examples/Forecasting_With_AutoBNN.ipynb" target="_blank">colab</a>, and leverage this library to innovate and solve real-world challenges. 
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h2>Acknowledgements</h2>


<p>
<em>AutoBNN was written by Colin Carroll, Thomas Colthurst, Urs Köster and Srinivas Vasudevan. We would like to thank Kevin Murphy, Brian Patton and Feras Saad for their advice and feedback.</em>
</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-7061041222399769838</id>
            <title>Computer-aided diagnosis for lung cancer screening</title>
            <link>http://blog.research.google/2024/03/computer-aided-diagnosis-for-lung.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-7061041222399769838</guid>
            <pubDate></pubDate>
            <updated>2024-03-20T13:54:06.249-07:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFpuCd82OUmuS2oG2cVir_ZgeOyUpFndr-kCq8V4pDv6fzxeyViBJymfVt5FFUqgkM_X57msxNv84XBtaXs2FsD7R8_tNqtH6D8X_KiMtZRaJ37JphQsvM35_gIk-4Tn2eEYvrInjMLV5ouwhRJv3Oqb30Z71P546NszeURINBoJnlWnzgASn-6D9YFwZo/s72-c/PULMA%20hero.jpg"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Atilla Kiraly, Software Engineer, and Rory Pilgrim, Product Manager, Google Research </span>


<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFpuCd82OUmuS2oG2cVir_ZgeOyUpFndr-kCq8V4pDv6fzxeyViBJymfVt5FFUqgkM_X57msxNv84XBtaXs2FsD7R8_tNqtH6D8X_KiMtZRaJ37JphQsvM35_gIk-4Tn2eEYvrInjMLV5ouwhRJv3Oqb30Z71P546NszeURINBoJnlWnzgASn-6D9YFwZo/s320/PULMA%20hero.jpg" style="display: none;" />

<p>
Lung cancer is the leading cause of cancer-related deaths globally with <a href="https://www.who.int/news-room/fact-sheets/detail/cancer#:~:text=The%20most%20common%20causes%20of,rectum%20(916%20000%20deaths)%3B">1.8 million deaths</a> reported in 2020. Late diagnosis dramatically reduces the chances of survival. <a href="https://www.cdc.gov/cancer/lung/basic_info/screening.htm">Lung cancer screening</a> via <a href="https://www.cancer.gov/about-cancer/diagnosis-staging/ct-scans-fact-sheet#:~:text=indicate%20real%20problems.-,Lung%20cancer,-Low%2Ddose%20CT">computed tomography</a> (CT), which provides a detailed 3D image of the lungs, has been shown to reduce mortality in high-risk populations by at least 20% by detecting potential signs of cancers earlier. In the US, screening involves annual scans, with some countries or cases recommending more or less frequent scans. 
</p>
<a name="more"></a>
<p>
The <a href="https://www.uspreventiveservicestaskforce.org/uspstf/recommendation/lung-cancer-screening">United States Preventive Services Task Force</a> recently expanded lung cancer screening recommendations by <a href="https://pubmed.ncbi.nlm.nih.gov/34636916/">roughly 80%</a>, which is expected to increase screening access for women and racial and ethnic minority groups. However, false positives (i.e., incorrectly reporting a potential cancer in a cancer-free patient) can cause anxiety and lead to unnecessary procedures for patients while increasing costs for the healthcare system. Moreover, efficiency in screening a large number of individuals can be challenging depending on healthcare infrastructure and radiologist availability.
</p>


<p>
At Google we have previously developed <a href="https://blog.google/technology/health/lung-cancer-prediction/">machine learning (ML) models for lung cancer detection</a>, and have evaluated their ability to automatically detect and classify regions that show signs of potential cancer. Performance has been shown to be comparable to that of specialists in detecting possible cancer. While they have achieved high performance, effectively communicating findings in realistic environments is necessary to realize their full potential.
</p>

<p>
To that end, in “<a href="https://pubs.rsna.org/doi/10.1148/ryai.230079">Assistive AI in Lung Cancer Screening: A Retrospective Multinational Study in the US and Japan</a>”, published in <em><a href="https://pubs.rsna.org/journal/ai">Radiology AI</a></em>, we investigate how ML models can effectively communicate findings to radiologists. We also introduce a generalizable user-centric interface to help radiologists leverage such models for lung cancer screening. The system takes CT imaging as input and outputs a cancer suspicion rating using four categories (no suspicion, probably benign, suspicious, highly suspicious) along with the corresponding regions of interest. We evaluate the system’s utility in improving clinician performance through randomized reader studies in both the US and Japan, using the local cancer scoring systems (<a href="https://www.acr.org/-/media/ACR/Files/RADS/Lung-RADS/LungRADSAssessmentCategoriesv1-1.pdf">Lung-RADSs V1.1</a> and <a href="https://www.jscts.org/pdf/guideline/gls3rdfig_english130621.pdf">Sendai Score</a>) and image viewers that mimic realistic settings. We found that reader specificity increases with model assistance in both reader studies. To accelerate progress in conducting similar studies with ML models, we have <a href="https://github.com/Google-Health/google-health/tree/master/ct_dicom">open-sourced code</a> to process CT images and generate images compatible with the <a href="https://en.wikipedia.org/wiki/Picture_archiving_and_communication_system">picture archiving and communication system</a> (PACS) used by radiologists. 
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h2>Developing an interface to communicate model results</h2>


<p>
Integrating ML models into radiologist workflows involves understanding the nuances and goals of their tasks to meaningfully support them. In the case of lung cancer screening, hospitals follow various country-specific guidelines that are regularly updated. For example, in the US, Lung-RADs V1.1 assigns an <a href="https://www.acr.org/-/media/ACR/Files/RADS/Lung-RADS/LungRADSAssessmentCategoriesv1-1.pdf">alpha-numeric score</a> to indicate the lung cancer risk and follow-up recommendations<em>. </em>When assessing patients, radiologists load the CT in their workstation to read the case, find lung nodules or lesions, and apply set guidelines to determine follow-up decisions. 
</p>


<p>
Our first step was to improve the <a href="https://blog.google/technology/health/lung-cancer-prediction/">previously developed ML models</a> through additional training data and architectural improvements, including <a href="https://research.google/pubs/attention-is-all-you-need/">self-attention</a>. Then, instead of targeting specific guidelines, we experimented with a complementary way of communicating AI results independent of guidelines or their particular versions. Specifically, the system output offers a suspicion rating and localization (regions of interest) for the user to consider in conjunction with their own specific guidelines. The interface produces output images directly associated with the CT study, requiring no changes to the user’s workstation. The radiologist only needs to review a small set of additional images. There is no other change to their system or interaction with the system.
</p>


<p>


</p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiChGqKLOWQAzrIzk294q6i6XuUoR1ul0qoTAR8RHQw-bZT-ulyruug-HNY8f2em7ZgzHE1UP6yQbe4plM0gkmXu6KwcTmsNogbr6FjTGzSDrBEDFhVLQ4TdbxVp_bbB21gA_jR84-1r9ly-O5HXqOzuZERgJyjFSYtZty7h6J3UErWsP0-DoQ1pFZtyjiw/s857/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiChGqKLOWQAzrIzk294q6i6XuUoR1ul0qoTAR8RHQw-bZT-ulyruug-HNY8f2em7ZgzHE1UP6yQbe4plM0gkmXu6KwcTmsNogbr6FjTGzSDrBEDFhVLQ4TdbxVp_bbB21gA_jR84-1r9ly-O5HXqOzuZERgJyjFSYtZty7h6J3UErWsP0-DoQ1pFZtyjiw/s16000/image1.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Example of the assistive lung cancer screening system outputs. Results for the radiologist’s evaluation are visualized on the location of the CT volume where the suspicious lesion is found. The overall suspicion is displayed at the top of the CT images. Circles highlight the suspicious lesions while squares show a rendering of the same lesion from a different perspective, called a sagittal view.</td></tr></tbody></table>


<p>
The assistive lung cancer screening system comprises 13 models and has a high-level architecture similar to the end-to-end system used in <a href="https://blog.google/technology/health/lung-cancer-prediction/">prior work</a>. The models coordinate with each other to first segment the lungs, obtain an overall assessment, locate three suspicious regions, then use the information to assign a suspicion rating to each region. The system was deployed on Google Cloud using a <a href="https://cloud.google.com/kubernetes-engine">Google Kubernetes Engine</a> (GKE) that pulled the images, ran the ML models, and provided results. This allows scalability and directly connects to servers where the images are stored in <a href="https://cloud.google.com/healthcare-api/docs/concepts/dicom">DICOM stores</a>.
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlQLk7XcQtSX367ubw0D0TtTqZQg-H69p63qtVrGir3UfJcYUyys0n_Nks-YqURRklRWllhSKdH-FFjRvfkb9mGxEmL191sfpAclKD085x-u20FJS9BWJGULyLk0foVGKfq5T5F7_hx7Z4xHu1ZeHPLM63HUCaiCrkt8BThhiImts9epWqqCE2s0BLeoWU/s646/image4.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlQLk7XcQtSX367ubw0D0TtTqZQg-H69p63qtVrGir3UfJcYUyys0n_Nks-YqURRklRWllhSKdH-FFjRvfkb9mGxEmL191sfpAclKD085x-u20FJS9BWJGULyLk0foVGKfq5T5F7_hx7Z4xHu1ZeHPLM63HUCaiCrkt8BThhiImts9epWqqCE2s0BLeoWU/s16000/image4.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Outline of the Google Cloud deployment of the assistive lung cancer screening system and the directional calling flow for the individual components that serve the images and compute results. Images are served to the viewer and to the system using Google Cloud services. The system is run on a Google Kubernetes Engine that pulls the images, processes them, and writes them back into the DICOM store.</td></tr></tbody></table>
  
<br />

<div style="line-height: 40%;">
    <br />
</div>
<h2>Reader studies </h2>


<p>
To evaluate the system’s utility in improving clinical performance, we conducted two reader studies (i.e., experiments designed to assess clinical performance comparing expert performance with and without the aid of a technology) with 12 radiologists using pre-existing, de-identified CT scans. We presented 627 challenging cases to 6 US-based and 6 Japan-based radiologists. In the experimental setup, readers were divided into two groups that read each case twice, with and without assistance from the model. Readers were asked to apply scoring guidelines they typically use in their clinical practice and report their overall suspicion of cancer for each case. We then compared the results of the reader’s responses to measure the impact of the model on their workflow and decisions. The score and suspicion level were judged against the actual cancer outcomes of the individuals to measure sensitivity, specificity, and <a href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc#:~:text=AUC%20stands%20for%20%22Area%20under,across%20all%20possible%20classification%20thresholds.">area under the ROC curve</a> (AUC) values. These were compared with and without assistance.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmiP7GWIMf_TKezxSK0sM8EOtfm2M3QoZtgvYfcjacMm2atdilirD93ftlu_QlyusIu_ocC6R0iHX1eXtHrU6g1yLUWnZ1Bq0FJ0nXEjTezptuSxGbpwDFIkQGeZrFPmwXV3IYvyzJYPCEhp4etRNzhGmHbbfQAwntOm4ZhQNpuXbei5sfN6MqsQXJctVH/s794/image3.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgmiP7GWIMf_TKezxSK0sM8EOtfm2M3QoZtgvYfcjacMm2atdilirD93ftlu_QlyusIu_ocC6R0iHX1eXtHrU6g1yLUWnZ1Bq0FJ0nXEjTezptuSxGbpwDFIkQGeZrFPmwXV3IYvyzJYPCEhp4etRNzhGmHbbfQAwntOm4ZhQNpuXbei5sfN6MqsQXJctVH/s16000/image3.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">A multi-case multi-reader study involves each case being reviewed by each reader twice, once with ML system assistance and once without. In this visualization one reader first reviews Set A without assistance (<strong>blue</strong>) and then with assistance (<strong>orange</strong>) after a wash-out period. A second reader group follows the opposite path by reading the same set of cases Set A with assistance first. Readers are randomized to these groups to remove the effect of ordering.</td></tr></tbody></table>


<p>
The ability to conduct these studies using the same interface highlights its generalizability to completely different cancer scoring systems, and the generalization of the model and assistive capability to different patient populations. Our study results demonstrated that when radiologists used the system in their clinical evaluation, they had an increased ability to correctly identify lung images without actionable lung cancer findings (i.e., <em>specificity</em>) by an absolute 5–7% compared to when they didn’t use the assistive system. This potentially means that for every 15–20 patients screened, one may be able to avoid unnecessary follow-up procedures, thus reducing their anxiety and the burden on the health care system. This can, in turn, help improve the sustainability of lung cancer screening programs, particularly as <a href="https://pubmed.ncbi.nlm.nih.gov/34636916/">more people become eligible for screening</a>. 
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDMKrqRR9njVuYSLV0Nzb7-MXdpyJTSofvvxFhyendGwnM9pddFyy48MVBWKsadYMUp1RGQBNL77vC0gCvjZ_fIsIQ8ZhGHZmy52srebu49xIL4wYkuvyftssXzvohoSoBKt9C2uwua6gz4ReO4LQvfMbhdrgtXvcYb3JruZAchta2n5MhU41pTpJLyMJI/s1999/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiDMKrqRR9njVuYSLV0Nzb7-MXdpyJTSofvvxFhyendGwnM9pddFyy48MVBWKsadYMUp1RGQBNL77vC0gCvjZ_fIsIQ8ZhGHZmy52srebu49xIL4wYkuvyftssXzvohoSoBKt9C2uwua6gz4ReO4LQvfMbhdrgtXvcYb3JruZAchta2n5MhU41pTpJLyMJI/s16000/image2.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Reader specificity increases with ML model assistance in both the US-based and Japan-based reader studies. Specificity values were derived from reader scores from actionable findings (something suspicious was found) versus no actionable findings, compared against the true cancer outcome of the individual.  Under model assistance, readers flagged fewer cancer-negative individuals for follow-up visits. Sensitivity for cancer positive individuals remained the same.</td></tr></tbody></table>


<div style="line-height: 40%;">
    <br />
</div>
<h2>Translating this into real-world impact through partnership </h2>


<p>
The system results demonstrate the potential for fewer follow-up visits, reduced anxiety, as well lower overall costs for lung cancer screening. In an effort to translate this research into real-world clinical impact, we are working with:  <a href="https://deephealth.com/">DeepHealth</a>, a leading AI-powered health informatics provider; and <a href="https://apolloradiologyintl.com/">Apollo Radiology International</a> a leading provider of Radiology services in India to explore paths for incorporating this system into future products. In addition, we are looking to help other researchers studying how best to integrate ML model results into clinical workflows by <a href="https://github.com/Google-Health/google-health/tree/master/ct_dicom">open sourcing code</a> used for the reader study and incorporating the insights described in this blog. We hope that this will help accelerate medical imaging researchers looking to conduct reader studies for their AI models, and catalyze translational research in the field.  
</p>


<div style="line-height: 40%;">
    <br />
</div>
<h2>Acknowledgements</h2>


<p>
<em>Key contributors to this project include Corbin Cunningham, Zaid Nabulsi, Ryan Najafi, Jie Yang, Charles Lau, Joseph R. Ledsam, Wenxing Ye, Diego Ardila, Scott M. McKinney, Rory Pilgrim, Hiroaki Saito, Yasuteru Shimamura, Mozziyar Etemadi, Yun Liu, David Melnick, Sunny Jansen, Nadia Harhen, David P. Nadich, Mikhail Fomitchev, Ziyad Helali, Shabir Adeel, Greg S. Corrado, Lily Peng, Daniel Tse, Shravya Shetty, Shruthi Prabhakara, Neeral Beladia, and Krish Eswaran. Thanks to Arnav Agharwal and Andrew Sellergren for their open sourcing support and Vivek Natarajan and Michael D. Howell for their feedback. Sincere appreciation also goes to the radiologists who enabled this work with their image interpretation and annotation efforts throughout the study, and Jonny Wong and Carli Sampson for coordinating the reader studies.</em>
</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-4615278636568583418</id>
            <title>Using AI to expand global access to reliable flood forecasts</title>
            <link>http://blog.research.google/2024/03/using-ai-to-expand-global-access-to.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-4615278636568583418</guid>
            <pubDate></pubDate>
            <updated>2024-03-20T09:06:06.753-07:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgABDUlqCHMxNY-QfEftM_9yPy1z4jr1odB-_kSP79yjk6igtpPJNFIocQOKDRnZ3VLmqrI9tqX-dCHpcYtnSx96y9X9V9knp1CiAREvfgZX71D0XpWZNgPdZOI7aMW3POigHJ2rLeA1G1asaAPO3KIB3j0WzUr5C707I7p0L_itspYYEhYDhDTzd39tNUD/s72-c/Flood%20forecasting%20hero%20image.jpg"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Yossi Matias, VP Engineering &amp; Research, and Grey Nearing, Research Scientist, Google Research</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgABDUlqCHMxNY-QfEftM_9yPy1z4jr1odB-_kSP79yjk6igtpPJNFIocQOKDRnZ3VLmqrI9tqX-dCHpcYtnSx96y9X9V9knp1CiAREvfgZX71D0XpWZNgPdZOI7aMW3POigHJ2rLeA1G1asaAPO3KIB3j0WzUr5C707I7p0L_itspYYEhYDhDTzd39tNUD/s320/Flood%20forecasting%20hero%20image.jpg" style="display: none;" />

<p>
Floods are the <a href="https://openknowledge.worldbank.org/server/api/core/bitstreams/e218989e-8b3b-5f8c-944c-06e9812215aa/content">most common natural disaster</a>, and are responsible for roughly <a href="https://www.swissre.com/risk-knowledge/mitigating-climate-risk/floods.html">$50 billion</a> in annual financial damages worldwide. The <a href="https://library.wmo.int/records/item/57630-2021-state-of-climate-services-water?offset=1#:~:text=WMO%2DNo.,1278&amp;text=More%20than%202%20billion%20people,for%20the%20past%2020%20years.">rate of flood-related disasters has more than doubled</a> since the year 2000 partly <a href="https://www.nature.com/articles/s41598-020-70816-2">due to climate change</a>. Nearly <a href="https://openknowledge.worldbank.org/server/api/core/bitstreams/e218989e-8b3b-5f8c-944c-06e9812215aa/content">1.5 billion people</a>, making up 19% of the world’s population, are exposed to substantial risks from severe flood events. Upgrading early warning systems to make accurate and timely information accessible to these populations <a href="https://elibrary.worldbank.org/doi/abs/10.1596/1813-9450-6058">can save thousands of lives per year</a>. 
</p>
<a name="more"></a>
<p>
Driven by the potential impact of reliable flood forecasting on people’s lives globally, we started our flood forecasting effort in 2017. Through this <a href="https://blog.google/technology/ai/google-ai-global-flood-forecasting/">multi-year journey</a>, we advanced research over the years hand-in-hand with building a real-time operational flood forecasting system that <a href="https://blog.google/technology/ai/expanding-our-ml-based-flood-forecasting/">provides alerts</a> on Google Search, Maps, Android notifications and through the <a href="http://g.co/floodhub">Flood Hub</a>. However, in order to <a href="https://blog.google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/">scale globally</a>, especially in places where accurate local data is not available, more research advances were required.
</p>

<p>
In “<a href="https://www.nature.com/articles/s41586-024-07145-1">Global prediction of extreme floods in ungauged watersheds</a>”, published in <em><a href="https://www.nature.com/">Nature</a></em>, we demonstrate how machine learning (ML) technologies can significantly improve global-scale <a href="https://sites.research.google/floodforecasting/">flood forecasting</a> relative to the current state-of-the-art for countries where flood-related data is scarce. With these AI-based technologies we extended the reliability of currently-available global nowcasts, on average, from zero to five days, and improved forecasts across regions in Africa and Asia to be similar to what are currently available in Europe. The evaluation of the models was conducted in collaboration with the European Center for Medium Range Weather Forecasting (<a href="https://www.ecmwf.int/">ECMWF</a>).
</p>

<p>
These technologies also enable <a href="http://g.co/floodhub">Flood Hub</a> to provide real-time river forecasts up to seven days in advance, <a href="https://blog.google/outreach-initiatives/sustainability/flood-hub-ai-flood-forecasting-more-countries/">covering</a> river reaches across over 80 countries. This information can be used by people, communities, governments and international organizations to take anticipatory action to help protect vulnerable populations.
</p>

<br />
<div class="separator" style="clear: both; text-align: center;"></div>
<br />

<div style="line-height: 40%;">
    <br />
</div>
<h2>Flood forecasting at Google </h2>


<p>
The ML models that power the FloodHub tool are the product of many years of research, conducted in collaboration with several partners, including academics, governments, international organizations, and NGOs. 
</p>

<p>
In 2018, we <a href="https://blog.google/products/search/helping-keep-people-safe-ai-enabled-flood-forecasting/">launched a pilot</a> early warning system in the Ganges-Brahmaputra river basin in India, with the <a href="https://arxiv.org/abs/1901.09583">hypothesis</a> that ML could help address the challenging problem of reliable flood forecasting at scale. The pilot was further <a href="https://blog.google/technology/ai/tracking-our-progress-on-flood-forecasting/">expanded</a> the following year <a href="https://ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html">via the combination</a> of an inundation model, real-time water level measurements, the creation of an elevation map and hydrologic modeling.
</p>

<p>
In <a href="https://ai.googleblog.com/2019/03/a-summary-of-google-flood-forecasting.html">collaboration</a> with academics, and, in particular, with the <a href="https://www.jku.at/en/institute-for-machine-learning/">JKU Institute for Machine Learning</a> we explored ML-based hydrologic models, showing that <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">LSTM</a>-based models could <a href="https://hess.copernicus.org/articles/23/5089/2019/">produce more accurate simulations</a> than traditional conceptual and physics-based <a href="https://en.wikipedia.org/wiki/Hydrological_model">hydrology models</a>. This research led to <a href="https://blog.research.google/2020/09/the-technology-behind-our-recent.html">flood forecasting improvements</a> that enabled the <a href="https://blog.google/technology/ai/flood-forecasts-india-bangladesh/">expansion</a> of our forecasting coverage to include all of India and Bangladesh. We also worked with researchers at Yale University to test technological interventions that increase the <a href="https://egc.yale.edu/about/perspectives/pande-and-coauthors-using-technology-save-lives-during-indias-monsoon-season">reach and impact</a> of flood warnings.
</p>

<p>
Our hydrological models predict river floods by processing publicly available weather data like precipitation and physical watershed information. Such models must be calibrated to long data records from <a href="https://en.wikipedia.org/wiki/Stream_gauge">streamflow gauging stations</a> in individual rivers. A low percentage of global river watersheds (basins) have streamflow gauges, which are expensive but necessary to supply relevant data, and it’s challenging for hydrological simulation and forecasting to provide <a href="https://www.tandfonline.com/doi/full/10.1080/02626667.2013.803183">predictions in basins</a> that lack this infrastructure. Lower <a href="https://www.pnas.org/doi/full/10.1073/pnas.1414439112">gross domestic product</a> (GDP) is correlated with increased <a href="https://www.pnas.org/doi/full/10.1073/pnas.1414439112">vulnerability to flood risks</a>, and there is an inverse correlation between national GDP and the amount of publicly available data in a country. ML helps to address this problem by allowing a <a href="https://www.pnas.org/doi/full/10.1073/pnas.1414439112">single model to be trained on all available river data</a> and to be applied to ungauged basins where <a href="https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020wr028091">no data are available</a>. In this way, models can be trained globally, and can make predictions for any river location.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxQUgMZAg0tVPN5LrxYbhpn3dukUCVogsWPgynrYNjFfbXpwK0RF79rYvK9kyehrha0F-vMLZR2eqBWdKCuGter6VoZrbCKnROTNn_hmOXBDxWmOFhFRvyg36ghO0B08fsQv7cqXdyngtfgCAgF5LhONs5VDzyvYjxzEYejVN3FxvzRs8w9Q5EeGJJTr3O/s1051/Streamflow%20data%20from%20the%20Global%20Runoff%20Data%20Center.jpg" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxQUgMZAg0tVPN5LrxYbhpn3dukUCVogsWPgynrYNjFfbXpwK0RF79rYvK9kyehrha0F-vMLZR2eqBWdKCuGter6VoZrbCKnROTNn_hmOXBDxWmOFhFRvyg36ghO0B08fsQv7cqXdyngtfgCAgF5LhONs5VDzyvYjxzEYejVN3FxvzRs8w9Q5EeGJJTr3O/s16000/Streamflow%20data%20from%20the%20Global%20Runoff%20Data%20Center.jpg" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">There is an inverse (log-log) correlation between the amount of publicly available streamflow data in a country and national GDP. Streamflow data from the <a href="https://www.bafg.de/GRDC/EN/Home/homepage_node.html">Global Runoff Data Center</a>.</td></tr></tbody></table>



<p>
Our academic collaborations led to ML research that developed methods to <a href="https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020wr028091">estimate uncertainty in river forecasts</a> and showed how ML river forecast models <a href="https://hess.copernicus.org/articles/25/2685/2021/hess-25-2685-2021-relations.html">synthesize information from multiple data sources</a>. They demonstrated that these models can <a href="https://hess.copernicus.org/articles/26/3377/2022/hess-26-3377-2022.html">simulate extreme events reliably</a>, even when those events are not part of the training data. In an effort to <a href="https://blog.research.google/2023/04/directing-ml-toward-natural-hazard.html">contribute</a> to open science, in 2023 we open-sourced a community-driven dataset for large-sample hydrology in <em><a href="https://www.nature.com/articles/s41597-023-01975-w">Nature Scientific Data</a></em>. 
</p>


<div style="line-height: 40%;">
    <br />
</div>
<h2>The river forecast model</h2>


<p>
Most hydrology models used by national and international agencies for flood forecasting and river modeling are state-space models, which depend only on daily inputs (e.g., precipitation, temperature, etc.) and the current state of the system (e.g., soil moisture, snowpack, etc.). LSTMs are a variant of state-space models and work by defining a neural network that represents a single time step, where input data (such as current weather conditions) are processed to produce updated state information and output values (streamflow) for that time step. LSTMs are applied sequentially to make time-series predictions, and in this sense, behave similarly to how scientists typically conceptualize hydrologic systems. Empirically, we have found that <a href="https://hess.copernicus.org/articles/23/5089/2019/">LSTMs perform well</a> on the task of river forecasting.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMfiw33NkHO8CQsYGWSZ91xhPx0iDONFLe8WZuRWDsoi8RRv7pHlF6M8eDLEWpO8lZECUfGi59_NsMXO8ASDZQ9xxrB87mupNTPpioKT0wRgSSc1FwYDmfCUWyooGGZmvMhZv0RDcWJVslQOPvRNOK_B6dXUGsnijSl-W-lICOIbALAwNC2PNEmqqXhv6g/s960/image1.gif" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMfiw33NkHO8CQsYGWSZ91xhPx0iDONFLe8WZuRWDsoi8RRv7pHlF6M8eDLEWpO8lZECUfGi59_NsMXO8ASDZQ9xxrB87mupNTPpioKT0wRgSSc1FwYDmfCUWyooGGZmvMhZv0RDcWJVslQOPvRNOK_B6dXUGsnijSl-W-lICOIbALAwNC2PNEmqqXhv6g/s16000/image1.gif" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">A diagram of the LSTM, which is a neural network that operates sequentially in time. An accessible primer can be found <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">here</a>.</td></tr></tbody></table>



<p>
Our river forecast model uses two LSTMs applied sequentially: (1) a “hindcast” LSTM ingests historical weather data (dynamic hindcast features) up to the present time (or rather, the issue time of a forecast), and (2) a “forecast” LSTM ingests states from the hindcast LSTM along with forecasted weather data (dynamic forecast features) to make future predictions. One year of historical weather data are input into the hindcast LSTM, and seven days of forecasted weather data are input into the forecast LSTM. Static features include geographical and geophysical characteristics of watersheds that are input into both the hindcast and forecast LSTMs and allow the model to learn different hydrological behaviors and responses in various types of watersheds. 
</p>

<p>
Output from the forecast LSTM is fed into a “head” layer that uses <a href="https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf">mixture density networks</a> to produce a probabilistic forecast (i.e., predicted parameters of a probability distribution over streamflow). Specifically, the model predicts the parameters of a mixture of heavy-tailed probability density functions, called <a href="https://en.wikipedia.org/wiki/Asymmetric_Laplace_distribution">asymmetric Laplacian distributions</a>, at each forecast time step. The result is a mixture density function, called a <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/d80126524c1e9641333502c664fc6ca1-Paper.pdf">Countable Mixture of Asymmetric Laplacians</a> (CMAL) distribution, which represents a probabilistic prediction of the volumetric flow rate in a particular river at a particular time. 
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVPR4LA0EbJyAesDg4HvrMdxgG_0wiyLqJveir2Ryy06qDNVshkM2-zHvMj_y1LEBXOSm7ajMx2qzYCLNQrQ3dm8TRicy_wkTVtM4Xio_mhQPsgaSiN3sm3J8BBNYNpxWQbSm_aTSMyRW9UyIEWAAT9secPekdYNzyKRrXwgm10-ksyeUzTFRydXnt_Wai/s960/LSTM-based%20river%20forecast%20model.jpeg" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVPR4LA0EbJyAesDg4HvrMdxgG_0wiyLqJveir2Ryy06qDNVshkM2-zHvMj_y1LEBXOSm7ajMx2qzYCLNQrQ3dm8TRicy_wkTVtM4Xio_mhQPsgaSiN3sm3J8BBNYNpxWQbSm_aTSMyRW9UyIEWAAT9secPekdYNzyKRrXwgm10-ksyeUzTFRydXnt_Wai/s16000/LSTM-based%20river%20forecast%20model.jpeg" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">LSTM-based river forecast model architecture. Two LSTMs are applied in sequence, one ingesting historical weather data and one ingesting forecasted weather data. The model outputs are the parameters of a probability distribution over streamflow at each forecasted timestep.</td></tr></tbody></table>

<br />

<div style="line-height: 40%;">
    <br />
</div>
<h2>Input and training data</h2>


<p>
The model uses three types of publicly available data inputs, mostly from governmental sources:
</p>
<ol>

<li><em>Static watershed attributes representing geographical and geophysical variables:</em> From the <a href="https://www.hydrosheds.org/hydroatlas">HydroATLAS project</a>, including data like long-term climate indexes (precipitation, temperature, snow fractions), land cover, and anthropogenic attributes (e.g., a nighttime lights index as a proxy for human development). 

</li><li><em>Historical meteorological time-series data</em>: Used to spin up the model for one year prior to the issue time of a forecast. The data comes from <a href="https://gpm.nasa.gov/data/imerg">NASA IMERG</a>, <a href="https://psl.noaa.gov/data/gridded/data.cpc.globalprecip.html">NOAA  CPC  Global Unified Gauge-Based Analysis of Daily Precipitation</a>, and the <a href="https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-land?tab=overview">ECMWF ERA5-land reanalysis</a>. Variables include daily total precipitation, air temperature, solar and thermal radiation, snowfall, and surface pressure. 

</li><li><em>Forecasted meteorological time series over a seven-day forecast horizon</em>: Used as input for the forecast LSTM. These data are the same meteorological variables listed above, and come from the <a href="https://www.ecmwf.int/en/forecasts/datasets/set-i">ECMWF HRES atmospheric model</a>.
</li>
</ol>

<p>
Training data are daily streamflow values from the <a href="https://www.bafg.de/GRDC/EN/Home/homepage_node.html">Global Runoff Data Center</a> over the time period 1980 - 2023. A single streamflow forecast model is trained using data from 5,680 diverse watershed streamflow gauges (shown below) to improve <a href="https://eartharxiv.org/repository/view/6363/">accuracy</a>.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJZa8BMczHa_WiWNB1FJvPgEcw5O6U_IumoXBvI3gB_cIqrbte2SZKu_Msr1MudCVPv3YF6L3BweAC0hhMkET634isx6xzUswrYfDwp8oueoWJ7c3hf0os-RIsaNrdgAboc7HUly0rGtuBt6OVQ-MnY5P44DKOXSHKYl_T-gMz5z0ek8CHk0lIx45fnZYU/s1417/gauge_locations_map(1).jpg" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJZa8BMczHa_WiWNB1FJvPgEcw5O6U_IumoXBvI3gB_cIqrbte2SZKu_Msr1MudCVPv3YF6L3BweAC0hhMkET634isx6xzUswrYfDwp8oueoWJ7c3hf0os-RIsaNrdgAboc7HUly0rGtuBt6OVQ-MnY5P44DKOXSHKYl_T-gMz5z0ek8CHk0lIx45fnZYU/s16000/gauge_locations_map(1).jpg" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Location of 5,680 streamflow gauges that supply training data for the river forecast model from the <a href="https://www.bafg.de/GRDC/EN/Home/homepage_node.html">Global Runoff Data Center</a>.</td></tr></tbody></table>

<br />
  
  
<div style="line-height: 40%;">
    <br />
</div>  
<h2>Improving on the current state-of-the-art</h2>


<p>
We compared our river forecast model with <a href="https://www.globalfloods.eu/">GloFAS version 4</a>, the current state-of-the-art global flood forecasting system. These experiments showed that ML can provide accurate warnings earlier and over larger and more impactful events. 
</p>

<p>
The figure below shows the distribution of <a href="https://en.wikipedia.org/wiki/F-score">F1 scores</a> when predicting different severity events at river locations around the world, with plus or minus 1 day accuracy. F1 scores are an average of precision and recall and event severity is measured by <a href="https://en.wikipedia.org/wiki/Return_period#:~:text=A%20return%20period%2C%20also%20known,river%20discharge%20flows%20to%20occur.">return period</a>. For example, a 2-year return period event is a volume of streamflow that is expected to be exceeded on average once every two years. Our model achieves reliability scores at up to 4-day or 5-day lead times that are similar to or better, on average, than the reliability of GloFAS nowcasts (0-day lead time). 
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjwzwV6QYl4yIlWs1xdHz2HRiNi2I8WUaTGBVlVvA4guppIGpJ3RMj8ypE7chWz8sV5KJuS4dPe9PUd6TqWe46W8Yelga1Nq28Mts72zqJhLJXDgMjSa6VCHlb9ZH3eo8XETWSqj8lNraejCAezFpkGpfJrPIl4xMhRPHSdO1WX7bZmVSLDFMZOwMfarb5/s3908/Distributions%20of%20F1%20scores%20over%202-year%20.jpeg" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjwzwV6QYl4yIlWs1xdHz2HRiNi2I8WUaTGBVlVvA4guppIGpJ3RMj8ypE7chWz8sV5KJuS4dPe9PUd6TqWe46W8Yelga1Nq28Mts72zqJhLJXDgMjSa6VCHlb9ZH3eo8XETWSqj8lNraejCAezFpkGpfJrPIl4xMhRPHSdO1WX7bZmVSLDFMZOwMfarb5/s16000/Distributions%20of%20F1%20scores%20over%202-year%20.jpeg" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Distributions of <a href="https://en.wikipedia.org/wiki/F-score">F1 scores</a> over 2-year return period events in 2,092 watersheds globally during the time period 2014-2023 from GloFAS (<strong>blue</strong>) and our model (<strong>orange</strong>) at different lead times. On average, our model is statistically as accurate as GloFAS nowcasts (0–day lead time) up to 5 days in advance over 2-year (shown) and 1-year, 5-year, and 10-year events (not shown).</td></tr></tbody></table>


<p>
Additionally (not shown), our model achieves accuracies over larger and rarer extreme events, with precision and recall scores over 5-year return period events that are similar to or better than GloFAS accuracies over 1-year return period events. See the <a href="https://www.nature.com/articles/s41586-024-07145-1">paper</a> for more information.
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h2>Looking into the future</h2>


<p>
The flood forecasting initiative is part of our <a href="https://blog.google/outreach-initiatives/sustainability/google-ai-climate-change-solutions/">Adaptation and Resilience efforts</a> and reflects Google's commitment&nbsp;<a href="https://research.google/teams/climate-and-sustainability/">to address climate change</a> while helping global communities become more resilient. We believe that AI and ML will continue to play a critical role in helping advance science and research towards climate action.
</p>

<p>
We actively <a href="https://blog.google/outreach-initiatives/sustainability/4-flood-forecasting-collaboration-case-studies-show-how-ai-can-help-communities-in-need/">collaborate</a> with several international aid organizations (e.g., the Centre for Humanitarian Data and the Red Cross) to provide actionable flood forecasts. Additionally, in an ongoing collaboration with the <a href="https://wmo.int/">World Meteorological Organization</a> (WMO) to <a href="https://blog.google/outreach-initiatives/sustainability/early-warning-system-wmo-google/">support early warning systems</a> for climate hazards, we are conducting a study to help understand how AI can help address real-world challenges faced by national flood forecasting agencies. 
</p>

<p>
While the work presented here demonstrates a significant step forward in flood forecasting, future work  is needed to further expand flood forecasting coverage to more locations globally and other types of flood-related events and disasters, including flash floods and urban floods. We are looking forward to continuing collaborations with our partners in the academic and expert communities, local governments and the industry to reach these goals. 
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-520087429457973735</id>
            <title>ScreenAI: A visual language model for UI and visually-situated language understanding</title>
            <link>http://blog.research.google/2024/03/screenai-visual-language-model-for-ui.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-520087429457973735</guid>
            <pubDate></pubDate>
            <updated>2024-03-19T13:15:33.664-07:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoXlMR7pAKRRnyKZT8C40i6mPX0KKNGT6AFNvFOFIhZ7BD0rXaU3NS_aqISTGq9S_d0zozgcO0HR_v3R6Msm4uUDkaBFsFVx-miaDL6L0UhSz1Is8_L_iFjtvNE5OX9HX98t92b3r-rLQfJG1RrzVW354NdVUlIJVRLdQ_l4dFYa1773J-tJligdvh7QsX/s72-c/ScreenAI%20-%20hero.jpeg"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Srinivas Sunkara and Gilles Baechler, Software Engineers, Google Research</span>


<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoXlMR7pAKRRnyKZT8C40i6mPX0KKNGT6AFNvFOFIhZ7BD0rXaU3NS_aqISTGq9S_d0zozgcO0HR_v3R6Msm4uUDkaBFsFVx-miaDL6L0UhSz1Is8_L_iFjtvNE5OX9HX98t92b3r-rLQfJG1RrzVW354NdVUlIJVRLdQ_l4dFYa1773J-tJligdvh7QsX/s320/ScreenAI%20-%20hero.jpeg" style="display: none;" />

<p>
Screen user interfaces (UIs) and infographics, such as charts, diagrams and tables, play important roles in human communication and human-machine interaction as they facilitate rich and interactive user experiences. UIs and infographics share similar design principles and visual language (e.g., icons and layouts), that offer an opportunity to build a single model that can understand, reason, and interact with these interfaces. However, because of their complexity and varied presentation formats, infographics and UIs present a unique modeling challenge.
</p>
<a name="more"></a>
<p>
To that end, we introduce “<a href="https://arxiv.org/abs/2402.04615">ScreenAI: A Vision-Language Model for UI and Infographics Understanding</a>”. ScreenAI improves upon the <a href="https://arxiv.org/abs/2305.18565">PaLI architecture</a> with the flexible patching strategy from <a href="https://arxiv.org/abs/2210.03347">pix2struct</a>. We train ScreenAI on a unique mixture of datasets and tasks, including a novel Screen Annotation task that requires the model to identify UI element information (i.e., type, location and description) on a screen. These text annotations provide large language models (LLMs) with screen descriptions, enabling them to automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. At only 5B parameters, ScreenAI achieves state-of-the-art results on UI- and infographic-based tasks (<a href="https://x-lance.github.io/WebSRC/">WebSRC</a> and <a href="https://github.com/aburns4/MoTIF">MoTIF</a>), and best-in-class performance on <a href="https://github.com/vis-nlp/ChartQA">Chart QA</a>, <a href="https://rrc.cvc.uab.es/?ch=17&amp;com=evaluation&amp;task=1">DocVQA</a>, and <a href="https://arxiv.org/abs/2104.12756">InfographicVQA</a> compared to models of similar size. We are also releasing three new datasets: <a href="https://github.com/google-research-datasets/screen_qa?tab=readme-ov-file#screen-annotation-dataset-details">Screen Annotation</a> to evaluate the layout understanding capability of the model, as well as <a href="https://github.com/google-research-datasets/screen_qa/tree/main?tab=readme-ov-file#short_answers-directory">ScreenQA Short</a> and <a href="https://github.com/google-research-datasets/screen_qa?tab=readme-ov-file#complexqa" target="_blank">Complex ScreenQA</a> for a more comprehensive evaluation of its QA capability. 
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h2>ScreenAI</h2>


<p>
ScreenAI’s architecture is based on <a href="https://arxiv.org/abs/2209.06794">PaLI</a>, composed of a multimodal encoder block and an autoregressive decoder. The PaLI encoder uses a <a href="https://arxiv.org/abs/2010.11929">vision transformer</a> (ViT) that creates image embeddings and a multimodal encoder that takes the concatenation of the image and text embeddings as input. This flexible architecture allows ScreenAI to solve vision tasks that can be recast as text+image-to-text problems. 
</p>

<p>
On top of the PaLI architecture, we employ a flexible patching strategy introduced in pix2struct. Instead of using a fixed-grid pattern, the grid dimensions are selected such that they preserve the native aspect ratio of the input image. This enables ScreenAI to work well across images of various aspect ratios. 
</p>

<p>
The ScreenAI model is trained in two stages: a pre-training stage followed by a fine-tuning stage. First, self-supervised learning is applied to automatically generate data labels, which are then used to train ViT and the language model. ViT is frozen during the fine-tuning stage, where most data used is manually labeled by human raters. 
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS1qatfLUw6BZZgkPxrv0Hx1pAPAehiF8q3kfA0BUyyPx4XXpwZRr75nYl99fTIQwLNmOHXhSBbpzHDnw6yQXZls1ZV-IE-d75jP5M02cRSZTYuU8FJBS4mubPzUPIuvcj_oqkEJcWtNWtnLmPZ3P1jJlDmc8GA1WNq00jUwl2o8gfLIIXlknrjy4z6y7Y/s1600/image6.gif" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjS1qatfLUw6BZZgkPxrv0Hx1pAPAehiF8q3kfA0BUyyPx4XXpwZRr75nYl99fTIQwLNmOHXhSBbpzHDnw6yQXZls1ZV-IE-d75jP5M02cRSZTYuU8FJBS4mubPzUPIuvcj_oqkEJcWtNWtnLmPZ3P1jJlDmc8GA1WNq00jUwl2o8gfLIIXlknrjy4z6y7Y/s16000/image6.gif" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">ScreenAI model architecture.</td></tr></tbody></table>

<br />


<div style="line-height: 40%;">
    <br />
</div>
<h2>Data generation</h2>


<p>
To create a pre-training dataset for ScreenAI, we first compile an extensive collection of screenshots from various devices, including desktops, mobile, and tablets. This is achieved by using <a href="https://arxiv.org/abs/1910.10683" target="_blank">publicly accessible web pages</a> and following the programmatic exploration approach used for the <a href="https://dl.acm.org/doi/10.1145/3126594.3126651" target="_blank">RICO dataset</a> for mobile apps. We then apply a layout annotator, based on the <a href="https://arxiv.org/abs/2005.12872" target="_blank">DETR</a> model, that identifies and labels a wide range of UI elements (e.g., image, pictogram, button, text) and their spatial relationships. Pictograms undergo further analysis using an <a href="https://arxiv.org/abs/2210.02663" target="_blank">icon classifier</a> capable of distinguishing 77 different icon types. This detailed classification is essential for interpreting the subtle information conveyed through icons. For icons that are not covered by the classifier, and for infographics and images, we use the PaLI image captioning model to generate descriptive captions that provide contextual information. We also apply an <a href="https://cloud.google.com/use-cases/ocr" target="_blank">optical character recognition</a> (OCR) engine to extract and annotate textual content on screen. We combine the OCR text with the previous annotations to create a detailed description of each screen.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_wzxsb1U_PH17m3dG92ny7PpJjIYK39k1NQme1i5GM63tAd_OGdxMAV2_OQQVQSdkdyY1Tb3s8ibI2M3Kp1VpdNMsBr0ugBcBdL_r6dUwOwdfJfBMn3ae9Zl3zM2IpfZV654DFybMhMLimy0cuUNsnU5L8O2byu9eHmhdWcIvsb1t8AWi-tKNkXFq7Neo/s1747/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_wzxsb1U_PH17m3dG92ny7PpJjIYK39k1NQme1i5GM63tAd_OGdxMAV2_OQQVQSdkdyY1Tb3s8ibI2M3Kp1VpdNMsBr0ugBcBdL_r6dUwOwdfJfBMn3ae9Zl3zM2IpfZV654DFybMhMLimy0cuUNsnU5L8O2byu9eHmhdWcIvsb1t8AWi-tKNkXFq7Neo/s16000/image2.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">A mobile app screenshot with generated annotations that include UI elements and their descriptions, e.g., <code>TEXT</code> elements also contain the text content from OCR, <code>IMAGE</code> elements contain image captions, <code>LIST_ITEMs</code> contain all their child elements.</td></tr></tbody></table>
<br />



<div style="line-height: 40%;">
    <br />
</div>
<h3>LLM-based data generation</h3>


<p>
We enhance the pre-training data's diversity using <a href="https://blog.google/technology/ai/google-palm-2-ai-large-language-model/">PaLM 2</a> to generate input-output pairs in a two-step process. First, screen annotations are generated using the technique outlined above, then we craft a prompt around this schema for the LLM to create synthetic data. This process requires prompt engineering and iterative refinement to find an effective prompt. We assess the generated data's quality through human validation against a quality threshold. 
</p>


<br />
<pre class="prettyprint" style="margin-left: 40px; margin-right: 40px; white-space: pre-wrap;"><font color="#008000">You only speak JSON. Do not write text that isn’t JSON.
You are given the following mobile screenshot, described in words. Can you generate 5 questions regarding the content of the screenshot as well as the corresponding short answers to them? 

The answer should be as short as possible, containing only the necessary information. Your answer should be structured as follows:
questions: [
{{question: the question,
    answer: the answer
}},
 ...
]

{THE SCREEN SCHEMA}
</font></pre>
<br />
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td class="tr-caption" style="text-align: center;">A sample prompt for QA data generation.</td></tr></tbody></table>


<p>
By combining the natural language capabilities of LLMs with a structured schema, we simulate a wide range of user interactions and scenarios to generate synthetic, realistic tasks. In particular, we generate three categories of tasks:
</p>

<ul>

<li><strong>Question answering</strong>: The model is asked to answer questions regarding the content of the screenshots, e.g., “When does the restaurant open?”

</li><li><strong>Screen navigation</strong>: The model is asked to convert a natural language utterance into an executable action on a screen, e.g., “Click the search button.”

</li><li><strong>Screen summarization</strong>: The model is asked to summarize the screen content in one or two sentences. 
</li>
</ul>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiinxXWrVJQr3tZJ4-o3ipkdJriUqTRbi2CFWor4I2SpyMiswx6uZOM2ZJW0gZC75MXYshkjXPABvDuSnhR44ceNwDpkvaSLa4R3v4C-hEsnHdEc-JUUx31zZmDHDDwhWaMDqnD0wo6ibt7qBZfaYN_yx1myH77k-ruO9fjd33SiLnP0jLnjOfmhdEHbsR7/s1398/image3.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiinxXWrVJQr3tZJ4-o3ipkdJriUqTRbi2CFWor4I2SpyMiswx6uZOM2ZJW0gZC75MXYshkjXPABvDuSnhR44ceNwDpkvaSLa4R3v4C-hEsnHdEc-JUUx31zZmDHDDwhWaMDqnD0wo6ibt7qBZfaYN_yx1myH77k-ruO9fjd33SiLnP0jLnjOfmhdEHbsR7/s16000/image3.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Block diagram of our workflow for generating data for QA, summarization and navigation tasks using existing ScreenAI models and LLMs. Each task uses a custom prompt to emphasize desired aspects, like questions related to counting, involving reasoning, etc.</td></tr></tbody></table>

<br />



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><img height="540" src="https://lh7-us.googleusercontent.com/LmUtXBMXK-zy_rMShHQ_Hk4vQeXu2Kpx8zfzjhE3uAREczbkbGTEjZ7OMTbqtB37lD4rF31xJsoWdVXNAXLbbM1Uc_01WZWmOfBg9RwyAUEToPpa1W38Pt117Zj5LrNfnxXqjXoAJDZd-zcAIgU4QSoBaAKsIrSi8_POI14F5hguN1NJL9a2RsrKg6WHz7w" style="margin-left: auto; margin-right: auto; margin-top: 0px;" width="705" /></td></tr><tr><td class="tr-caption" style="text-align: center;">LLM-generated data. Examples for screen QA, navigation and summarization. For navigation, the action bounding box is displayed in red on the screenshot.</td></tr></tbody></table>

<br />
<div style="line-height: 40%;">
    <br />
</div>
<h2>Experiments and results</h2>


<p>
As previously mentioned, ScreenAI is trained in two stages: pre-training and fine-tuning. Pre-training data labels are obtained using self-supervised learning and fine-tuning data labels comes from human raters. 
</p>

<p>
We fine-tune ScreenAI using public QA, summarization, and navigation datasets and a variety of tasks related to UIs. For QA, we use well established benchmarks in the multimodal and document understanding field, such as <a href="https://github.com/vis-nlp/ChartQA">ChartQA</a>, <a href="https://rrc.cvc.uab.es/?ch=17&amp;com=evaluation&amp;task=1">DocVQA</a>, <a href="https://rrc.cvc.uab.es/?ch=17&amp;com=tasks">Multi page DocVQA</a>, <a href="https://arxiv.org/abs/2104.12756">InfographicVQA</a>, <a href="https://ocr-vqa.github.io/">OCR VQA</a>, <a href="https://x-lance.github.io/WebSRC/">Web SRC</a> and <a href="https://github.com/google-research-datasets/screen_qa">ScreenQA</a>. For navigation, datasets used include <a href="https://github.com/google-research-datasets/uibert/tree/main">Referring Expressions</a>, <a href="https://github.com/aburns4/MoTIF">MoTIF</a>, <a href="https://arxiv.org/abs/2209.15099">Mug</a>, and <a href="https://github.com/google-research/google-research/tree/master/android_in_the_wild">Android in the Wild</a>. Finally, we use <a href="https://github.com/google-research-datasets/screen2words">Screen2Words</a> for screen summarization and <a href="https://paperswithcode.com/paper/widget-captioning-generating-natural-language/review/">Widget Captioning</a> for describing specific UI elements. Along with the fine-tuning datasets, we  evaluate the fine-tuned ScreenAI model using three novel benchmarks:
</p>

<ol>

<li>Screen Annotation: Enables the evaluation model layout annotations and spatial understanding capabilities.

</li><li>ScreenQA Short: A variation of ScreenQA, where its ground truth answers have been shortened to contain only the relevant information that better aligns with other QA tasks.

</li><li>Complex ScreenQA: Complements ScreenQA Short with more difficult questions (counting, arithmetic, comparison, and non-answerable questions) and contains screens with various aspect ratios.
</li>
</ol>

<p>
The fine-tuned ScreenAI model achieves state-of-the-art results on various UI and infographic-based tasks (<a href="https://x-lance.github.io/WebSRC/">WebSRC</a> and <a href="https://github.com/aburns4/MoTIF">MoTIF</a>) and best-in-class performance on <a href="https://github.com/vis-nlp/ChartQA">Chart QA</a>, <a href="https://rrc.cvc.uab.es/?ch=17&amp;com=evaluation&amp;task=1">DocVQA</a>, and <a href="https://arxiv.org/abs/2104.12756">InfographicVQA</a> compared to models of similar size. ScreenAI achieves competitive performance on Screen2Words and OCR-VQA. Additionally, we report results on the new benchmark datasets introduced to serve as a baseline for further research.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijJAw824LdVbrFU3c7oerx9Ik86dWnuQ2NqliLpUZLp6U-9pDxZKsw10VSMfYOSwns-GWJRdSCj3UmyxytOZxfoM64psBSKCjLYa-3zkXDt8mGvFbNpydwS1Ya2dhDeYfihWL1mVCyTWIzdgfblxawoxukWW1vLLwfNWMNKQ64B8wUM5SlNKgegdGxXlr7/s1183/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijJAw824LdVbrFU3c7oerx9Ik86dWnuQ2NqliLpUZLp6U-9pDxZKsw10VSMfYOSwns-GWJRdSCj3UmyxytOZxfoM64psBSKCjLYa-3zkXDt8mGvFbNpydwS1Ya2dhDeYfihWL1mVCyTWIzdgfblxawoxukWW1vLLwfNWMNKQ64B8wUM5SlNKgegdGxXlr7/s16000/image2.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Comparing model performance of ScreenAI with state-of-the-art (SOTA) models of similar size.</td></tr></tbody></table>




<p>
Next, we examine ScreenAI’s scaling capabilities and observe that across all tasks, increasing the model size improves performances and the improvements have not saturated at the largest size.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKNMvTyz1RhM0wqgn7eAGB9Lev3YUhKhHrcAmJt3SB1Gi6ozIaxHoPzAj-bm6II-_91viG2FXrfNZiiwSSI_YNQGwKGyO6YkAW05Cfl9oys869f7DMyJcthlj6c0CLwzMAGP8HM9AmxdCK92d4PL2Ujz-tI4CZsQOlzlecMLgElWBjl9FZtj-zWIWata2k/s1999/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKNMvTyz1RhM0wqgn7eAGB9Lev3YUhKhHrcAmJt3SB1Gi6ozIaxHoPzAj-bm6II-_91viG2FXrfNZiiwSSI_YNQGwKGyO6YkAW05Cfl9oys869f7DMyJcthlj6c0CLwzMAGP8HM9AmxdCK92d4PL2Ujz-tI4CZsQOlzlecMLgElWBjl9FZtj-zWIWata2k/s16000/image1.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Model performance increases with size, and the performance has not saturated even at the largest size of 5B params.</td></tr></tbody></table>


<br />


<div style="line-height: 40%;">
    <br />
</div>
<h2>Conclusion</h2>


<p>
We introduce the ScreenAI model along with a unified representation that enables us to develop self-supervised learning tasks leveraging data from all these domains. We also illustrate the impact of data generation using LLMs and investigate improving model performance on specific aspects with modifying the training mixture. We apply all of these techniques to build multi-task trained models that perform competitively with state-of-the-art approaches on a number of public benchmarks. However, we also note that our approach still lags behind large models and further research is needed to bridge this gap.
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h2>Acknowledgements</h2>


<p>
<em>This project is the result of joint work with Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Carbune, Jason Lin, Jindong Chen and Abhanshu Sharma. We thank Fangyu Liu, Xi Chen, Efi Kokiopoulou, Jesse Berent, Gabriel Barcik, Lukas Zilka, Oriana Riva, Gang Li,Yang Li, Radu Soricut, and Tania Bedrax-Weiss for their insightful feedback and discussions, along with Rahul Aralikatte, Hao Cheng and Daniel Kim for their support in data preparation. We also thank Jay Yagnik, Blaise Aguera y Arcas, Ewa Dominowska, David Petrou, and Matt Sharifi for their leadership, vision and support. We are very grateful toTom Small for helping us create the animation in this post.</em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-4328167517765145678</id>
            <title>SCIN: A new resource for representative dermatology images</title>
            <link>http://blog.research.google/2024/03/scin-new-resource-for-representative.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-4328167517765145678</guid>
            <pubDate></pubDate>
            <updated>2024-03-19T08:00:00.150-07:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_fSTMFxLAMHLJ0rw7OAddGSPMW2tRl8kmTr2mWiiJunKxB8ZflMJeWkBmB5IqCD2LvRoikpN7OYnZO3CdKpArGn32b4o-T8ZD6XCPxmUBtE1-sPBi6J05y5_UrfbWSMTjNpldKYzM3xjXoC0iWU7q_a7Ktfi2S1hVHLY8uq1986yp_pgEjQn3elNuSUbJ/s72-c/SCINHero.png"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Pooja Rao, Research Scientist, Google Research</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_fSTMFxLAMHLJ0rw7OAddGSPMW2tRl8kmTr2mWiiJunKxB8ZflMJeWkBmB5IqCD2LvRoikpN7OYnZO3CdKpArGn32b4o-T8ZD6XCPxmUBtE1-sPBi6J05y5_UrfbWSMTjNpldKYzM3xjXoC0iWU7q_a7Ktfi2S1hVHLY8uq1986yp_pgEjQn3elNuSUbJ/s1600/SCINHero.png" style="display: none;" />

<p>
Health datasets play a crucial role in research and medical education, but it can be challenging to create a dataset that represents the real world. For example, dermatology conditions are diverse in their appearance and severity and manifest differently across skin tones. Yet, existing dermatology image datasets often lack representation of everyday conditions (like rashes, allergies and infections) and skew towards lighter skin tones. Furthermore, race and ethnicity information is frequently missing, hindering our ability to assess disparities or create solutions.
</p>
<a name="more"></a>


<p>
To address these limitations, we are releasing the <a href="https://github.com/google-research-datasets/scin">Skin Condition Image Network (SCIN) dataset</a> in collaboration with physicians at <a href="https://med.stanford.edu/">Stanford Medicine</a>. We designed SCIN to reflect the broad range of concerns that people search for online, supplementing the types of conditions typically found in clinical datasets. It contains images across various skin tones and body parts, helping to ensure that future AI tools work effectively for all. We've made <a href="https://github.com/google-research-datasets/scin">the SCIN dataset</a> freely available as an open-access resource for researchers, educators, and developers, and have taken careful steps to protect contributor privacy.   
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-lvUDxsY1bC8xXeRFKGtdyRiCk25knKK3tKzW2dCVtfvzFMUYvM7laqOBS0yP6Dnur5Fd945gbC96OMoiJ2nvguO6uguDArYkvnLUz5glvPlNpI1THL_bctcQCGlR670V4szxkHlcdvAJbP7T8HS7U3ASnHh_sWhSxoKJSsLN-1IPUpysj5ErdHaduz5r/s1327/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-lvUDxsY1bC8xXeRFKGtdyRiCk25knKK3tKzW2dCVtfvzFMUYvM7laqOBS0yP6Dnur5Fd945gbC96OMoiJ2nvguO6uguDArYkvnLUz5glvPlNpI1THL_bctcQCGlR670V4szxkHlcdvAJbP7T8HS7U3ASnHh_sWhSxoKJSsLN-1IPUpysj5ErdHaduz5r/s16000/image1.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Example set of images and metadata from the SCIN dataset.</td></tr></tbody></table>



<div style="line-height: 40%;">
    <br />
</div>
<h2>Dataset composition</h2>


<p>
The SCIN dataset currently contains over 10,000 images of skin, nail, or hair conditions, directly contributed by individuals experiencing them. All contributions were made voluntarily with informed consent by individuals in the US, under an institutional-review board approved study. To provide context for retrospective dermatologist labeling, contributors were asked to take images both close-up and from slightly further away. They were given the option to self-report demographic information and <a href="https://en.wikipedia.org/wiki/Fitzpatrick_scale">tanning propensity</a> (self-reported Fitzpatrick Skin Type, i.e., sFST), and to describe the texture, duration and symptoms related to their concern.
</p>
<p>
One to three dermatologists labeled each contribution with up to five dermatology conditions, along with a confidence score for each label. The SCIN dataset contains these individual labels, as well as an aggregated and weighted differential diagnosis derived from them that could be useful for model testing or training. These labels were assigned retrospectively and are not equivalent to a clinical diagnosis, but they allow us to compare the distribution of dermatology conditions in the SCIN dataset with existing datasets.
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7oYE7nKEvgBaW6SEHfGFzCrhnKqX5w86_7ujHMbpMENOByxcUTgAzXJrZCgv6kbDVmTN8NmKSBBSvF4XkWKcKf5DT_b3A5D50ZpAr-93i3a69KUFOZy54diZxH_wcf1PeKdFlRbEe_OZODxS0N4ZrHSaiki8ZslUfFUatw4w-0p0zzD4GRwlqgmPLR6gw/s1851/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7oYE7nKEvgBaW6SEHfGFzCrhnKqX5w86_7ujHMbpMENOByxcUTgAzXJrZCgv6kbDVmTN8NmKSBBSvF4XkWKcKf5DT_b3A5D50ZpAr-93i3a69KUFOZy54diZxH_wcf1PeKdFlRbEe_OZODxS0N4ZrHSaiki8ZslUfFUatw4w-0p0zzD4GRwlqgmPLR6gw/s16000/image2.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The SCIN dataset contains largely allergic, inflammatory and infectious conditions while datasets from clinical sources focus on benign and malignant <a href="https://en.wikipedia.org/wiki/Neoplasm">neoplasms</a>.</td></tr></tbody></table>




<p>
While many existing dermatology datasets focus on malignant and benign tumors and are intended to assist with skin cancer diagnosis, the SCIN dataset consists largely of common allergic, inflammatory, and infectious conditions. The majority of images in the SCIN dataset show early-stage concerns — more than half arose less than a week before the photo, and 30% arose less than a day before the image was taken. Conditions within this time window are seldom seen within the health system and therefore are underrepresented in existing dermatology datasets. 
</p>
<p>
We also obtained dermatologist estimates of Fitzpatrick Skin Type (estimated FST or eFST) and layperson labeler estimates of <a href="https://en.wikipedia.org/wiki/Monk_Skin_Tone_Scale">Monk Skin Tone</a> (eMST) for the images. This allowed comparison of the skin condition and skin type distributions to those in existing dermatology datasets. Although we did not selectively target any skin types or skin tones, the SCIN dataset has a balanced Fitzpatrick skin type distribution (with more of Types 3, 4, 5, and 6) compared to similar datasets from clinical sources. 
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNnhVt5yEHKdMsi-tMYH9Q9oITruBrlrrfaQk8oopWHBr1qq6lfPrZnLrav-y2w7i9vgptlNDw_xKX3J8W0fZ1NfU-cOeINXc6bgf2vHJL3bc-UCWA7T846QQHkTvob6QbB3sR0HbwI9Vms3oXtAZ_zbrd4w_eAKLTo5-obYoG3A2urPmiF7RS5GcgVRhH/s1851/image3.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNnhVt5yEHKdMsi-tMYH9Q9oITruBrlrrfaQk8oopWHBr1qq6lfPrZnLrav-y2w7i9vgptlNDw_xKX3J8W0fZ1NfU-cOeINXc6bgf2vHJL3bc-UCWA7T846QQHkTvob6QbB3sR0HbwI9Vms3oXtAZ_zbrd4w_eAKLTo5-obYoG3A2urPmiF7RS5GcgVRhH/s16000/image3.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Self-reported and dermatologist-estimated Fitzpatrick Skin Type distribution in the SCIN dataset compared with existing un-enriched dermatology datasets <a href="https://github.com/mattgroh/fitzpatrick17k">(Fitzpatrick17k</a>, <a href="https://www.fc.up.pt/addi/ph2%20database.html">PH²</a>, <a href="https://www.it.pt/AutomaticPage?id=3459">SKINL2</a>, and<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7479321/"> PAD-UFES-20</a>).</td></tr></tbody></table>



<p>
The <a href="https://en.wikipedia.org/wiki/Fitzpatrick_scale">Fitzpatrick Skin Type</a> scale was originally developed as a photo-typing scale to measure the response of skin types to UV radiation, and it is widely used in dermatology research. The Monk Skin Tone scale is a newer 10-shade scale that measures skin tone rather than skin phototype, capturing more nuanced differences between the darker skin tones. While neither scale was intended for retrospective estimation using images, the inclusion of these labels is intended to enable future research into skin type and tone representation in dermatology. For example, the SCIN dataset provides an initial benchmark for the distribution of these skin types and tones in the US population.
</p>
<p>
The SCIN dataset has a high representation of women and younger individuals, likely reflecting a combination of factors. These could include differences in skin condition incidence, propensity to seek health information online, and variations in willingness to contribute to research across demographics.
</p>



<div style="line-height: 40%;">
    <br />
</div>
<h2>Crowdsourcing method</h2>


<p>
To create the SCIN dataset, we used a novel crowdsourcing method, which we describe in the accompanying <a href="https://arxiv.org/abs/2402.18545">research paper</a> co-authored with investigators at <a href="https://med.stanford.edu/">Stanford Medicine</a>. This approach empowers individuals to play an active role in healthcare research. It allows us to reach people at earlier stages of their health concerns, potentially before they seek formal care. Crucially, this method uses advertisements on web search result pages — the starting point for many people’s health journey — to connect with participants. 
</p>
<p>
Our results demonstrate that crowdsourcing can yield a high-quality dataset with a low spam rate. Over 97.5% of contributions were genuine images of skin conditions. After performing further filtering steps to exclude images that were out of scope for the SCIN dataset and to remove duplicates, we were able to release nearly 90% of the contributions received over the 8-month study period. Most images were sharp and well-exposed. Approximately half of the contributions include self-reported demographics, and 80% contain self-reported information relating to the skin condition, such as texture, duration, or other symptoms. We found that dermatologists’ ability to retrospectively assign a differential diagnosis depended more on the availability of self-reported information than on image quality.
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1QMRINpok_qmh5jjtktgqytapBRfHWDFxLKffzY9L_jG8uE8oJXA7QwtGY76gPksw5EH0yLuO7Ihk3IitXQDCjQ54DXlxFtpClbIIZzZAb6fDufHR-aW1m81cAMBqxmPIZsN8p3VYlys8b9cczZOzI-VB9d1Nwzk8nCnPTSCDwwh1fmEf4Q8DRdJHo6dR/s1999/image4.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1QMRINpok_qmh5jjtktgqytapBRfHWDFxLKffzY9L_jG8uE8oJXA7QwtGY76gPksw5EH0yLuO7Ihk3IitXQDCjQ54DXlxFtpClbIIZzZAb6fDufHR-aW1m81cAMBqxmPIZsN8p3VYlys8b9cczZOzI-VB9d1Nwzk8nCnPTSCDwwh1fmEf4Q8DRdJHo6dR/s16000/image4.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Dermatologist confidence in their labels (scale from 1-5) depended on the availability of self-reported demographic and symptom information.</td></tr></tbody></table>




<p>
While perfect image de-identification can never be guaranteed, protecting the privacy of individuals who contributed their images was a top priority when creating the SCIN dataset. Through informed consent, contributors were made aware of potential re-identification risks and advised to avoid uploading images with identifying features. Post-submission privacy protection measures included manual redaction or cropping to exclude potentially identifying areas, reverse image searches to exclude publicly available copies and metadata removal or aggregation. The SCIN <a href="https://github.com/google-research-datasets/scin?tab=License-1-ov-file#readme">Data Use License</a> prohibits attempts to re-identify contributors.
</p>
<p>
We hope the SCIN dataset will be a helpful resource for those working to advance inclusive dermatology research, education, and AI tool development. By demonstrating an alternative to traditional dataset creation methods, SCIN paves the way for more representative datasets in areas where self-reported data or retrospective labeling is feasible. 
</p>



<div style="line-height: 40%;">
    <br />
</div>
<h2>Acknowledgements</h2>


<p>
<em>We are grateful to all our co-authors Abbi Ward, Jimmy Li, Julie Wang, Sriram Lakshminarasimhan, Ashley Carrick, Bilson Campana, Jay Hartford, Pradeep Kumar S, Tiya Tiyasirisokchai, Sunny Virmani, Renee Wong, Yossi Matias, Greg S. Corrado, Dale R. Webster, Dawn Siegel (Stanford Medicine), Steven Lin (Stanford Medicine), Justin Ko (Stanford Medicine), Alan Karthikesalingam and Christopher Semturs. We also thank Yetunde Ibitoye, Sami Lachgar, Lisa Lehmann, Javier Perez, Margaret Ann Smith (Stanford Medicine), Rachelle Sico, Amit Talreja, Annisah Um’rani and Wayne Westerlind for their essential contributions to this work. Finally, we are grateful to Heather Cole-Lewis, Naama Hammel, Ivor Horn, Michael Howell, Yun Liu, and Eric Teasley for their insightful comments on the study design and manuscript. </em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-757976001746578714</id>
            <title>MELON: Reconstructing 3D objects from images with unknown poses</title>
            <link>http://blog.research.google/2024/03/melon-reconstructing-3d-objects-from.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-757976001746578714</guid>
            <pubDate></pubDate>
            <updated>2024-03-18T12:01:42.865-07:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh8LjCbKjfNXVUyCpGiZysx_pNF5BK8p5VBCJXXPaz_Bb75CW-33weoMh0YaNcn4AdmGN-Pufd_XlsRzo2MWZLQxqgtri7Nip9tXoGX0CritvRKF-63StOWxp_gVaY-MTnOk9IvJdVt_CczVR6Ip_R8Yv32MHTw2-FckCTF4UOFrgMyq3PCPCkZaZ-nyMcE/s72-c/MELON%20HERO.jpg"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Mark Matthews, Senior Software Engineer, and Dmitry Lagun, Research Scientist, Google Research</span>


<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh8LjCbKjfNXVUyCpGiZysx_pNF5BK8p5VBCJXXPaz_Bb75CW-33weoMh0YaNcn4AdmGN-Pufd_XlsRzo2MWZLQxqgtri7Nip9tXoGX0CritvRKF-63StOWxp_gVaY-MTnOk9IvJdVt_CczVR6Ip_R8Yv32MHTw2-FckCTF4UOFrgMyq3PCPCkZaZ-nyMcE/s320/MELON%20HERO.jpg" style="display: none;" />

<p>
A person's prior experience and understanding of the world generally enables them to easily infer what an object looks like in whole, even if only looking at a few 2D pictures of it. Yet the capacity for a computer to reconstruct the shape of an object in 3D given only a few images has remained a difficult algorithmic problem for years. This fundamental computer vision task has applications ranging from the creation of e-commerce 3D models to autonomous vehicle navigation. 
</p>
<a name="more"></a>
<p>
A key part of the problem is how to determine the exact positions from which images were taken, known as <em>pose inference</em>. If camera poses are known, a range of successful techniques — such as <a href="https://www.matthewtancik.com/nerf">neural radiance fields</a> (NeRF) or <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">3D Gaussian Splatting</a> — can reconstruct an object in 3D. But if these poses are not available, then we face a difficult “chicken and egg” problem where we could determine the poses if we knew the 3D object, but we can’t reconstruct the 3D object until we know the camera poses. The problem is made harder by pseudo-symmetries — i.e., many objects look similar when viewed from different angles. For example, square objects like a chair tend to look similar every 90° rotation. Pseudo-symmetries of an object can be revealed by rendering it on a turntable from various angles and plotting its photometric <a href="https://en.wikipedia.org/wiki/Self-similarity">self-similarity</a> map. 
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjt0nP5M8f5UodttSIPoY5t0JRXEuLosGgock3B0lyOzIn4icGF5jwVuxgX0PiRqc0kBbJ36CLiGA3KPrmaQbjKElGeHrsSRmkpDppU9abE84nuYu9MquqE3gULDzz_INDutmL2i1Wv3_tUpTh5U9UwSck9YRUeVyg-md2GByg3EQYYy7Vs_aeTEk5akpSo/s1764/image5.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjt0nP5M8f5UodttSIPoY5t0JRXEuLosGgock3B0lyOzIn4icGF5jwVuxgX0PiRqc0kBbJ36CLiGA3KPrmaQbjKElGeHrsSRmkpDppU9abE84nuYu9MquqE3gULDzz_INDutmL2i1Wv3_tUpTh5U9UwSck9YRUeVyg-md2GByg3EQYYy7Vs_aeTEk5akpSo/s16000/image5.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Self-Similarity map of a toy truck model. <strong>Left:</strong> The model is rendered on a turntable from various <a href="https://en.wikipedia.org/wiki/Azimuth">azimuthal angles</a>, θ. <strong>Right:</strong> The average <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm">L2</a> RGB similarity of a rendering from θ with that of θ*. The pseudo-similarities are indicated by the dashed red lines.</td></tr></tbody></table>


<p>
The diagram above only visualizes one dimension of rotation. It becomes even more complex (and difficult to visualize) when introducing more degrees of freedom. Pseudo-symmetries make the problem <em>ill-posed</em>, with naïve approaches often converging to local minima. In practice, such an approach might mistake the back view as the front view of an object, because they share a similar silhouette. Previous techniques (such as <a href="https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF/">BARF</a> or <a href="https://arxiv.org/abs/2205.15768">SAMURAI</a>) side-step this problem by relying on an initial pose estimate that starts close to the global minima. But how can we approach this if those aren’t available?
</p>

<p>
Methods, such as <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Meng_GNeRF_GAN-Based_Neural_Radiance_Field_Without_Posed_Camera_ICCV_2021_paper.pdf">GNeRF</a> and <a href="https://dl.acm.org/doi/10.1145/3503161.3548078">VMRF</a> leverage <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">generative adversarial networks</a> (GANs) to overcome the problem. These techniques have the ability to artificially “amplify” a limited number of training views, aiding reconstruction. GAN techniques, however, often have complex, sometimes unstable, training processes, making robust and reliable convergence difficult to achieve in practice. A range of other successful methods, such as <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sinha_SparsePose_Sparse-View_Camera_Pose_Regression_and_Refinement_CVPR_2023_paper.html">SparsePose</a> or <a href="https://rust-paper.github.io/">RUST</a>, can infer poses from a limited number views, but require pre-training on a large dataset of posed images, which aren’t always available, and can suffer from “domain-gap” issues when inferring poses for different types of images.
</p>

<p>
In “<a href="https://arxiv.org/abs/2303.08096">MELON: NeRF with Unposed Images in SO(3)</a>”, spotlighted at <a href="https://3dvconf.github.io/2024/">3DV 2024</a>, we present a technique that can determine object-centric camera poses entirely from scratch while reconstructing the object in 3D. <a href="https://melon-nerf.github.io/">MELON</a> (Modulo Equivalent Latent Optimization of NeRF) is one of the first techniques that can do this without initial pose camera estimates, complex training schemes or pre-training on labeled data. MELON is a relatively simple technique that can easily be integrated into existing NeRF methods. We demonstrate that MELON can reconstruct a NeRF from unposed images with state-of-the-art accuracy while requiring as few as 4–6 images of an object. 
</p>


<div style="line-height: 40%;">
    <br />
</div>
<h2>MELON</h2>


<p>
We leverage two key techniques to aid convergence of this ill-posed problem. The first is a very lightweight, dynamically trained <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural network</a> (CNN) encoder that regresses camera poses from training images. We pass a downscaled training image to a four layer CNN that infers the camera pose. This CNN is initialized from noise and requires no pre-training. Its capacity is so small that it forces similar looking images to similar poses, providing an implicit regularization greatly aiding convergence.
</p>

<p>
The second technique is a <em>modulo loss</em> that simultaneously considers pseudo symmetries of an object. We render the object from a fixed set of viewpoints for each training image, backpropagating the loss only through the view that best fits the training image. This effectively considers the plausibility of multiple views for each image. In practice, we find <em>N</em>=2 views (viewing an object from the other side) is all that’s required in most cases, but sometimes get better results with <em>N</em>=4 for square objects.
</p>

<p>
These two techniques are integrated into standard NeRF training, except that instead of fixed camera poses, poses are inferred by the CNN and duplicated by the modulo loss. Photometric gradients back-propagate through the best-fitting cameras into the CNN. We observe that cameras generally converge quickly to globally optimal poses (see animation below). After training of the neural field, MELON can synthesize novel views using standard NeRF rendering methods.
</p>

<p>
We simplify the problem by using the <a href="https://github.com/bmild/nerf">NeRF-Synthetic</a> dataset, a popular benchmark for NeRF research and common in the pose-inference literature. This synthetic dataset has cameras at precisely fixed distances and a consistent “up” orientation, requiring us to infer only the <a href="https://en.wikipedia.org/wiki/Spherical_coordinate_system">polar coordinates</a> of the camera. This is the same as an object at the center of a globe with a camera always pointing at it, moving along the surface. We then only need the latitude and longitude (2 degrees of freedom) to specify the camera pose.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEjisRopoeGPgbCRa3sQ7hmBUtnfI6TRapBD7Yn96xeDA_LxzTayiw3DMijPHS0ovkLVTcQGpp2_gAyA_P5BCPwXuEcz7lApC8WQbGfMvj_aAxShjgsmcklf_-4ekgbFH6VZ92Ey3Ta4XAhZvEdc00D2o7SzPIOSnFAj8CgrdmdJunijsGaw1Zx46b94wk/s1315/image4.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEjisRopoeGPgbCRa3sQ7hmBUtnfI6TRapBD7Yn96xeDA_LxzTayiw3DMijPHS0ovkLVTcQGpp2_gAyA_P5BCPwXuEcz7lApC8WQbGfMvj_aAxShjgsmcklf_-4ekgbFH6VZ92Ey3Ta4XAhZvEdc00D2o7SzPIOSnFAj8CgrdmdJunijsGaw1Zx46b94wk/s16000/image4.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">MELON uses a dynamically trained lightweight CNN encoder that predicts a pose for each image. Predicted poses are replicated by the <em>modulo loss, </em>which only penalizes the smallest L2 distance from the ground truth color. At evaluation time, the neural field can be used to generate novel views.</td></tr></tbody></table>


<br />


<div style="line-height: 40%;">
    <br />
</div>
<h2>Results</h2>


<p>
We compute two key metrics to evaluate MELON’s performance on the NeRF Synthetic dataset. The error in orientation between the ground truth and inferred poses can be quantified as a single angular error that we average across all training images, the pose error. We then test the accuracy of MELON’s rendered objects from novel views by measuring the <a href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio">peak signal-to-noise ratio</a> (PSNR) against held out test views. We see that MELON quickly converges to the approximate poses of most cameras within the first 1,000 steps of training, and achieves a competitive PSNR of 27.5 dB after 50k steps. 
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU5wdw89PfwRbvZeaIWLM3rNEAo69__A-ovDwB5x8emIkAGZq05FgF-wDMNlkXPS6tOcC_0NJVD4Glq8eX02yb3CDIiqXbadI4lnvcZ_MI9sHUkz8risxP1orPA8ZnTZUq-PcRLPoEc_AmFuARCokXHQlTOv_q35TH1tivuK2PpA54hO7q7kh_M8ZynO-J/s960/image1.gif" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU5wdw89PfwRbvZeaIWLM3rNEAo69__A-ovDwB5x8emIkAGZq05FgF-wDMNlkXPS6tOcC_0NJVD4Glq8eX02yb3CDIiqXbadI4lnvcZ_MI9sHUkz8risxP1orPA8ZnTZUq-PcRLPoEc_AmFuARCokXHQlTOv_q35TH1tivuK2PpA54hO7q7kh_M8ZynO-J/s16000/image1.gif" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Convergence of MELON on a toy truck model during optimization. <strong>Left</strong>: Rendering of the NeRF. <strong>Right</strong>: Polar plot of predicted (blue <em>x</em>), and ground truth (red dot) cameras.</td></tr></tbody></table>


<p>
MELON achieves similar results for other scenes in the NeRF Synthetic dataset.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWEC7CE_iWu1QZ_jgEUHHCEdqaUBMO7cK-1DZuHaZRDq4Y59_CriUlb_aOSJP5psB6Cbs1E41mm81EsfwVM0zAUojRKToWwiDmPfaWFPr2UGqf6F4n3P8ZpgYxiqyWIgst6op3Fhsbu0nlR727zLVV38KqJvNFY_KDeoJbdOjJFpHjLZkEd95Z9TqSg4R_/s1999/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWEC7CE_iWu1QZ_jgEUHHCEdqaUBMO7cK-1DZuHaZRDq4Y59_CriUlb_aOSJP5psB6Cbs1E41mm81EsfwVM0zAUojRKToWwiDmPfaWFPr2UGqf6F4n3P8ZpgYxiqyWIgst6op3Fhsbu0nlR727zLVV38KqJvNFY_KDeoJbdOjJFpHjLZkEd95Z9TqSg4R_/s16000/image2.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Reconstruction quality comparison between ground-truth (GT) and MELON on NeRF-Synthetic scenes after 100k training steps.</td></tr></tbody></table>

<br />


<div style="line-height: 40%;">
    <br />
</div>
<h3>Noisy images</h3>


<p>
MELON also works well when performing <a href="https://en.wikipedia.org/wiki/View_synthesis">novel view synthesis</a> from extremely noisy, unposed images. We add varying amounts, <em>σ</em>, of <a href="https://en.wikipedia.org/wiki/Additive_white_Gaussian_noise">white Gaussian noise</a> to the training images. For example, the object in <em>σ</em>=1.0 below is impossible to make out, yet MELON can determine the pose and generate novel views of the object. 
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHKYFcj-CKKc5kUvfsoOD5rBTp2QMnd3CdYiVzXjMClNwJrcgSrvIZngAdLgxUthE-aiXx5NapxcMx66i-Bi9RhC0zTRVkA0R8fj2A7lOnIdFDIE3YkTh_hWO2PhPa0FjYWYHuNUuae_tPhsrmVHJAkCeeI1f0ooJGe44KgpcO7jVNyLcnUvwtMX-KpJdD/s1182/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHKYFcj-CKKc5kUvfsoOD5rBTp2QMnd3CdYiVzXjMClNwJrcgSrvIZngAdLgxUthE-aiXx5NapxcMx66i-Bi9RhC0zTRVkA0R8fj2A7lOnIdFDIE3YkTh_hWO2PhPa0FjYWYHuNUuae_tPhsrmVHJAkCeeI1f0ooJGe44KgpcO7jVNyLcnUvwtMX-KpJdD/s16000/image1.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Novel view synthesis from noisy unposed 128×128 images. Top: Example of noise level present in training views. Bottom: Reconstructed model from noisy training views and mean angular pose error.</td></tr></tbody></table>

<p>
This perhaps shouldn’t be too surprising, given that techniques like <a href="https://bmild.github.io/rawnerf/">RawNeRF</a> have demonstrated NeRF’s excellent de-noising capabilities with known camera poses. The fact that MELON works for noisy images of unknown camera poses so robustly was unexpected. 
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h2>Conclusion</h2>


<p>
We present MELON, a technique that can determine object-centric camera poses to reconstruct objects in 3D without the need for approximate pose initializations, complex GAN training schemes or pre-training on labeled data. MELON is a relatively simple technique that can easily be integrated into existing NeRF methods. Though we only demonstrated MELON on synthetic images we are adapting our technique to work in real world conditions. See the <a href="https://arxiv.org/abs/2303.08096">paper</a> and <a href="https://melon-nerf.github.io/">MELON site</a> to learn more.
</p>


<div style="line-height: 40%;">
    <br />
</div>
<h2>Acknowledgements</h2>
<p>
<em>We would like to thank our paper co-authors Axel Levy, Matan Sela, and Gordon Wetzstein, as well as Florian Schroff and Hartwig Adam for continuous help in building this technology. We also thank Matthew Brown, Ricardo Martin-Brualla and Frederic Poitevin for their helpful feedback on the paper draft. We also acknowledge the use of the computational resources at the SLAC Shared Scientific Data Facility (SDF).</em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-977556648557231190</id>
            <title>HEAL: A framework for health equity assessment of machine learning performance</title>
            <link>http://blog.research.google/2024/03/heal-framework-for-health-equity.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-977556648557231190</guid>
            <pubDate></pubDate>
            <updated>2024-03-15T11:22:13.760-07:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYi3V0CsXup8WA6SSjPagoMWfkIpbr9oRWEaUM1vIWOX8_TsZs6ikqOn6qIGbqUzAPhOxwhEPNfWSkECIxRz5fJ629cRGScLraFn2CSw53Sr5_li8Fe7A9I1nMShys_15IiUZNhNiPh_ueFVcu_7f34A-A0pMXXVdDaSoSAf2h0jETJ1PemIR5I6o9pIIW/s72-c/HEAL-Hero.png"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Mike Schaekermann, Research Scientist, Google Research, and Ivor Horn, Chief Health Equity Officer &amp; Director, Google Core</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYi3V0CsXup8WA6SSjPagoMWfkIpbr9oRWEaUM1vIWOX8_TsZs6ikqOn6qIGbqUzAPhOxwhEPNfWSkECIxRz5fJ629cRGScLraFn2CSw53Sr5_li8Fe7A9I1nMShys_15IiUZNhNiPh_ueFVcu_7f34A-A0pMXXVdDaSoSAf2h0jETJ1PemIR5I6o9pIIW/s1600/HEAL-Hero.png" style="display: none;" />

<p>
Health equity is a major societal concern worldwide with disparities having many causes. These sources include limitations in access to healthcare, differences in clinical treatment, and even fundamental differences in the diagnostic technology. In dermatology for example, skin cancer outcomes are worse for populations such as minorities, those with lower socioeconomic status, or individuals with limited healthcare access. While there is great promise in recent advances in machine learning (ML) and artificial intelligence (AI) to help improve healthcare, this transition from research to bedside must be accompanied by a careful understanding of whether and how they impact health equity.
</p>
<a name="more"></a> 

<p>
<em>Health equity</em> is defined by public health organizations as fairness of opportunity for everyone to be as healthy as possible. Importantly, equity may be different from <em>equality</em>. For example, people with greater barriers to improving their health may require more or different effort to experience this fair opportunity. Similarly, equity is not <em>fairness</em> as defined in the AI for healthcare literature. Whereas AI fairness often strives for equal performance of the AI technology across different patient populations, this does not center the goal of prioritizing performance with respect to pre-existing health disparities.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi21VRS33NG-Imj1XlKXWtrwUrl4loEEywV0tO8M0JWtUFFksbTLOhilTZtMdJTgOBdXACUPQX-f5TMAFkABFhdv_cEDmFn4d-JirU78covJI32sHus6XQVJ1C1elwM_MExsQfeVCpFYlq9QZeynLNpLqmW8GqM-DKWiGSyi_18n8Xb3-8IeepHSyBZ6_2l/s1999/image2.jpg" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi21VRS33NG-Imj1XlKXWtrwUrl4loEEywV0tO8M0JWtUFFksbTLOhilTZtMdJTgOBdXACUPQX-f5TMAFkABFhdv_cEDmFn4d-JirU78covJI32sHus6XQVJ1C1elwM_MExsQfeVCpFYlq9QZeynLNpLqmW8GqM-DKWiGSyi_18n8Xb3-8IeepHSyBZ6_2l/s16000/image2.jpg" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Health equity considerations. An intervention (e.g., an ML-based tool, indicated in dark blue) promotes health equity if it helps reduce existing disparities in health outcomes (indicated in lighter blue).</td></tr></tbody></table>

<p>
In “<a href="https://www.thelancet.com/journals/eclinm/article/PIIS2589-5370(24)00058-0/fulltext">Health Equity Assessment of machine Learning performance (HEAL): a framework and dermatology AI model case study</a>”, published in <a href="https://www.thelancet.com/journals/eclinm/home"><i>The Lancet eClinicalMedicine</i></a>, we propose a methodology to quantitatively assess whether ML-based health technologies perform equitably. In other words, does the ML model perform well for those with the worst health outcomes for the condition(s) the model is meant to address? This goal anchors on the principle that health equity should prioritize and measure model performance with respect to disparate health outcomes, which may be due to a number of factors that include structural inequities (e.g., demographic, social, cultural, political, economic, environmental and geographic).
</p>
<br /> 

<h2>The health equity framework (HEAL)</h2>

<p>
The HEAL framework proposes a 4-step process to estimate the likelihood that an ML-based health technology performs equitably:
</p>
<ol>
<li>
Identify factors associated with health inequities and define tool performance metrics,
</li>
<li>
Identify and quantify pre-existing health disparities,
</li>
<li>
Measure the performance of the tool for each subpopulation,
</li>
<li>
Measure the likelihood that the tool prioritizes performance with respect to health disparities.
</li>
</ol>

<p>
The final step’s output is termed the HEAL metric, which quantifies how anticorrelated the ML model’s performance is with health disparities. In other words, does the model perform better with populations that have the worse health outcomes?
</p>
<p>
This 4-step process is designed to inform improvements for making ML model performance more equitable, and is meant to be iterative and re-evaluated on a regular basis. For example, the availability of health outcomes data in step (2) can inform the choice of demographic factors and brackets in step (1), and the framework can be applied again with new datasets, models and populations.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoGLCxn9QWS5QQpW39mJH1A_pw9wniWKIGGapN_gBC5WdxAWo4jHRS29GhNq7XBgNdZ867tMdP7TcszMz2WxUR4sYBFz0-dJ4cQZCODN2YFRjCP14QhNh_kMVGUdklbToOCYwHXV-UofhZdwZzDZudaVedOqvcC-QbW3LtMGb04FwFclbfzKHVUcqHodW_/s1999/image1.jpg" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoGLCxn9QWS5QQpW39mJH1A_pw9wniWKIGGapN_gBC5WdxAWo4jHRS29GhNq7XBgNdZ867tMdP7TcszMz2WxUR4sYBFz0-dJ4cQZCODN2YFRjCP14QhNh_kMVGUdklbToOCYwHXV-UofhZdwZzDZudaVedOqvcC-QbW3LtMGb04FwFclbfzKHVUcqHodW_/s16000/image1.jpg" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Framework for Health Equity Assessment of machine Learning performance (HEAL).&nbsp;Our guiding principle is to avoid exacerbating health inequities, and these steps help us identify disparities and assess for inequitable model performance to move towards better outcomes for all.</td></tr></tbody></table>

<p>
With this work, we take a step towards encouraging explicit assessment of the health equity considerations of AI technologies, and encourage prioritization of efforts during model development to reduce health inequities for subpopulations exposed to structural inequities that can precipitate disparate outcomes. We should note that the present framework does not model causal relationships and, therefore, cannot quantify the actual impact a new technology will have on reducing health outcome disparities. However, the HEAL metric may help identify opportunities for improvement, where the current performance is not prioritized with respect to pre-existing health disparities.
</p>
<br /> 

<h2>Case study on a dermatology model</h2>


<p>
As an illustrative case study, we applied the framework to a dermatology model, which utilizes a convolutional neural network similar to that described in <a href="https://blog.research.google/2019/09/using-deep-learning-to-inform.html">prior work</a>. This example dermatology model was trained to classify 288 skin conditions using a development dataset of 29k cases. The input to the model consists of three photos of a skin concern along with demographic information and a brief structured medical history. The output consists of a ranked list of possible matching skin conditions. 
</p>
<p>
Using the HEAL framework, we evaluated this model by assessing whether it prioritized performance with respect to pre-existing health outcomes. The model was designed to predict possible dermatologic conditions (from a list of hundreds) based on photos of a skin concern and patient metadata. Evaluation of the model is done using a top-3 agreement metric, which quantifies how often the top 3 output conditions match the most likely condition as suggested by a dermatologist panel. The HEAL metric is computed via the anticorrelation of this top-3 agreement with health outcome rankings. 
</p>
<p>
We used a dataset of 5,420 teledermatology cases, enriched for diversity in age, sex and race/ethnicity, to retrospectively evaluate the model’s HEAL metric. The dataset consisted of “store-and-forward” cases from patients of 20 years or older from primary care providers in the USA and skin cancer clinics in Australia. Based on a review of the literature, we decided to explore race/ethnicity, sex and age as potential factors of inequity, and used sampling techniques to ensure that our evaluation dataset had sufficient representation of all race/ethnicity, sex and age groups. To quantify pre-existing health outcomes for each subgroup we relied on measurements from <a href="https://www.who.int/data/gho/data/themes/mortality-and-global-health-estimates/global-health-estimates-leading-causes-of-dalys">public</a> <a href="https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30925-9/fulltext">databases</a> endorsed by the World Health Organization, such as <a href="https://www.who.int/data/gho/indicator-metadata-registry/imr-details/4427">Years of Life Lost</a> (YLLs) and <a href="https://www.who.int/data/gho/indicator-metadata-registry/imr-details/158">Disability-Adjusted Life Years</a> (DALYs; years of life lost plus years lived with disability).
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSS4J8AzS5iaHYvB7RyUVEDkx1ykrC7zOEAbUvjb8ZybZRZ0C71fRlJjPYBzGYVu9D3Ok0zRdz4MUdHMX6rOqnYKoHv91QNPw0TiqHJ6MKjtgn_UIqW-xoZeihO-A-ZrPgWT8bs-t9bSZWmMQ9AJaQh85BZWHH-T0KPWMx2unNO9HpTzYXiD_24gwNYWot/s1511/Table1.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiSS4J8AzS5iaHYvB7RyUVEDkx1ykrC7zOEAbUvjb8ZybZRZ0C71fRlJjPYBzGYVu9D3Ok0zRdz4MUdHMX6rOqnYKoHv91QNPw0TiqHJ6MKjtgn_UIqW-xoZeihO-A-ZrPgWT8bs-t9bSZWmMQ9AJaQh85BZWHH-T0KPWMx2unNO9HpTzYXiD_24gwNYWot/s16000/Table1.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">HEAL metric for all dermatologic conditions across race/ethnicity subpopulations, including health outcomes (YLLs per 100,000), model performance (top-3 agreement), and rankings for health outcomes and tool performance.<br />(* Higher is better; measures the likelihood the model performs equitably with respect to the axes in this table.)</td></tr></tbody></table>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMAQjyuGMXvzq4FxZg5Vhlgozwwnzza-QS-mjr3i0oOnDFIeqUGTrPxX2c7ssbpCZtLUoT2lpr8bXg_nJ3ToaaVe6Grge-HcWQl8SFy1gaBCoT-6ZHtFmQV4_S2sA6eOsdMFryegLjZFwOcPiqZDfFFItxqS96ysTZZn1OXVcbQSOG5WazZGjxSkNt9JQK/s1518/Table2.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMAQjyuGMXvzq4FxZg5Vhlgozwwnzza-QS-mjr3i0oOnDFIeqUGTrPxX2c7ssbpCZtLUoT2lpr8bXg_nJ3ToaaVe6Grge-HcWQl8SFy1gaBCoT-6ZHtFmQV4_S2sA6eOsdMFryegLjZFwOcPiqZDfFFItxqS96ysTZZn1OXVcbQSOG5WazZGjxSkNt9JQK/s16000/Table2.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">HEAL metric for all dermatologic conditions across sexes, including health outcomes (DALYs per 100,000), model performance (top-3 agreement), and rankings for health outcomes and tool performance. (* As above.)</td></tr></tbody></table><p>
Our analysis estimated that the model was 80.5% likely to perform equitably across race/ethnicity subgroups and 92.1% likely to perform equitably across sexes.
</p>
<p>
However, while the model was likely to perform equitably across age groups for cancer conditions specifically, we discovered that it had room for improvement across age groups for non-cancer conditions. For example, those 70+ have the poorest health outcomes related to non-cancer skin conditions, yet the model didn't prioritize performance for this subgroup.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4s5yfNQCksLIqP3kYuDXahUlOcJSCEtt-JkSTsecDft21uJ8JR0imnsPVGYHVQnc7OPo1WOkcwx2Yevu6su-rbqc1Fl6_NfzCKl0_vOvZA3PPnLkVWKFk7jHPJCm-x69MupVih_zct1YOXJVvSNUIsvn4rICk-_RWbOeuKj4HdRphBOakRXsiJ4lETJ_M/s1508/Table3.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4s5yfNQCksLIqP3kYuDXahUlOcJSCEtt-JkSTsecDft21uJ8JR0imnsPVGYHVQnc7OPo1WOkcwx2Yevu6su-rbqc1Fl6_NfzCKl0_vOvZA3PPnLkVWKFk7jHPJCm-x69MupVih_zct1YOXJVvSNUIsvn4rICk-_RWbOeuKj4HdRphBOakRXsiJ4lETJ_M/s16000/Table3.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">HEAL metrics for all cancer and non-cancer dermatologic conditions across age groups, including health outcomes (DALYs per 100,000), model performance (top-3 agreement), and rankings for health outcomes and tool performance. (* As above.)</td></tr></tbody></table>

<br /> 

<h2>Putting things in context</h2>


<p>
For holistic evaluation, the HEAL metric cannot be employed in isolation. Instead this metric should be contextualized alongside many other factors ranging from computational efficiency and data privacy to ethical values, and aspects that may influence the results (e.g., selection bias or differences in representativeness of the evaluation data across demographic groups). 
</p>
<p>
As an adversarial example, the HEAL metric can be artificially improved by deliberately reducing model performance for the most advantaged subpopulation until performance for that subpopulation is worse than all others. For illustrative purposes, given subpopulations A and B where A has worse health outcomes than B, consider the choice between two models: Model 1 (M1) performs 5% better for subpopulation A than for subpopulation B. Model 2 (M2) performs 5% worse on subpopulation A than B. The HEAL metric would be higher for M1 because it prioritizes performance on a subpopulation with worse outcomes. However, M1 may have absolute performances of just 75% and 70% for subpopulations A and B respectively, while M2 has absolute performances of 75% and 80% for subpopulations A and B respectively. Choosing M1 over M2 would lead to worse overall performance for all subpopulations because some subpopulations are worse-off while no subpopulation is better-off. 
</p>
<p>
Accordingly, the HEAL metric should be used alongside a <a href="https://en.wikipedia.org/wiki/Pareto_efficiency">Pareto condition</a> (discussed further in the paper), which restricts model changes so that outcomes for each subpopulation are either unchanged or improved compared to the status quo, and performance does not worsen for any subpopulation.
</p>
<p>
The HEAL framework, in its current form, assesses the likelihood that an ML-based model prioritizes performance for subpopulations with respect to pre-existing health disparities for specific subpopulations. This differs from the goal of understanding whether ML will reduce disparities in outcomes across subpopulations in reality. Specifically, modeling improvements in outcomes requires a causal understanding of steps in the care journey that happen both before and after use of any given model. Future research is needed to address this gap.
</p>
<br /> 

<h2>Conclusion</h2>


<p>
The HEAL framework enables a quantitative assessment of the likelihood that health AI technologies prioritize performance with respect to health disparities. The case study demonstrates how to apply the framework in the dermatological domain, indicating a high likelihood that model performance is prioritized with respect to health disparities across sex and race/ethnicity, but also revealing the potential for improvements for non-cancer conditions across age. The case study also illustrates limitations in the ability to apply all recommended aspects of the framework (e.g., mapping societal context, availability of data), thus highlighting the complexity of health equity considerations of ML-based tools. 
</p>
<p>
This work is a proposed approach to address a grand challenge for AI and health equity, and may provide a useful evaluation framework not only during model development, but during pre-implementation and real-world monitoring stages, e.g., in the form of health equity dashboards. We hold that the strength of the HEAL framework is in its future application to various AI tools and use cases and its refinement in the process. Finally, we acknowledge that a successful approach towards understanding the impact of AI technologies on health equity needs to be more than a set of metrics. It will require a set of goals agreed upon by a community that represents those who will be most impacted by a model.
</p>
<br /> 

<h2>Acknowledgements</h2>


<p>
<em>The research described here is joint work across many teams at Google. We are grateful to all our co-authors: Terry Spitz, Malcolm Pyles, Heather Cole-Lewis, Ellery Wulczyn, Stephen R. Pfohl, Donald Martin, Jr., Ronnachai Jaroensri, Geoff Keeling, Yuan Liu, Stephanie Farquhar, Qinghan Xue, Jenna Lester, Cían Hughes, Patricia Strachan, Fraser Tan, Peggy Bui, Craig H. Mermel, Lily H. Peng, Yossi Matias, Greg S. Corrado, Dale R. Webster, Sunny Virmani, Christopher Semturs, Yun Liu, and Po-Hsuan Cameron Chen. We also thank Lauren Winer, Sami Lachgar, Ting-An Lin, Aaron Loh, Morgan Du, Jenny Rizk, Renee Wong, Ashley Carrick, Preeti Singh, Annisah Um'rani, Jessica Schrouff, Alexander Brown, and Anna Iurchenko for their support of this project.</em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-7868032799856333119</id>
            <title>Cappy: Outperforming and boosting large multi-task language models with a small scorer</title>
            <link>http://blog.research.google/2024/03/cappy-outperforming-and-boosting-large.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-7868032799856333119</guid>
            <pubDate></pubDate>
            <updated>2024-03-14T12:38:11.597-07:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFNlqVAnwoYdZ97LvC4-ipR6FeOc4o9udsTUtNBBWl5Y4XHclcrz3kTCibizteSBc_xsVLh-pyRiCCNfIzTDHEs7VsJcUMCk0EjUxzvKITKCncdx1y7u9JXGkXM6TyoZY5RhUt2l_up-Us0yIV-0-EUvHsjOlFNSSNgNHlpwK1PAliqcj4gSoLsYXhIi18/s72-c/Cappy%20hero.jpg"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Yun Zhu and Lijuan Liu, Software Engineers, Google Research</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFNlqVAnwoYdZ97LvC4-ipR6FeOc4o9udsTUtNBBWl5Y4XHclcrz3kTCibizteSBc_xsVLh-pyRiCCNfIzTDHEs7VsJcUMCk0EjUxzvKITKCncdx1y7u9JXGkXM6TyoZY5RhUt2l_up-Us0yIV-0-EUvHsjOlFNSSNgNHlpwK1PAliqcj4gSoLsYXhIi18/s320/Cappy%20hero.jpg" style="display: none;" />


<p>
Large language model (LLM) advancements have led to a new paradigm that unifies various natural language processing (NLP) tasks within an instruction-following framework. This paradigm is exemplified by recent multi-task LLMs, such as <a href="https://arxiv.org/abs/2110.08207">T0</a>, <a href="https://arxiv.org/abs/2210.11416">FLAN</a>, and <a href="https://arxiv.org/abs/2212.12017">OPT-IML</a>. First, multi-task data is gathered with each task following a task-specific template, where each labeled example is converted into an instruction (e.g., <em>"</em>Put the concepts together to form a sentence: ski, mountain, skier<em>”</em>) paired with a corresponding response (e.g., <em>"</em>Skier skis down the mountain<em>"</em>). These instruction-response pairs are used to train the LLM, resulting in a conditional generation model that takes an instruction as input and generates a response. Moreover, multi-task LLMs have exhibited remarkable task-wise generalization capabilities as they can address unseen tasks by understanding and solving brand-new instructions.
</p>
<a name="more"></a>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMcacnhPA68XiEskvhExF4SGFh4997UZzwvhYfXt-ReGXtzfGTamLB3LZoYSh8WWuf1dmlBnNAUecAMhrBTOMVF6vxsw3BqY8Ld5xPgSdZY_cywScxxxQ5e6uwhawA5VYDEj6VtSyOTNGZtjdLXieeFV5OLiDk3bnB-xaz4MIbvUO-7RPadk8iQDv3206V/s640/Cappy%20instruction-following.gif" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMcacnhPA68XiEskvhExF4SGFh4997UZzwvhYfXt-ReGXtzfGTamLB3LZoYSh8WWuf1dmlBnNAUecAMhrBTOMVF6vxsw3BqY8Ld5xPgSdZY_cywScxxxQ5e6uwhawA5VYDEj6VtSyOTNGZtjdLXieeFV5OLiDk3bnB-xaz4MIbvUO-7RPadk8iQDv3206V/s16000/Cappy%20instruction-following.gif" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The demonstration of the instruction-following pre-training of multi-task LLMs, e.g., FLAN. Pre-training tasks under this paradigm improves the performance for unseen tasks.</td></tr></tbody></table>


<p>
Due to the complexity of understanding and solving various tasks solely using instructions, the size of multi-task LLMs typically spans from several billion parameters to hundreds of billions (e.g., <a href="https://arxiv.org/abs/2210.11416">FLAN-11B</a>, <a href="https://arxiv.org/abs/2110.08207">T0-11B</a> and <a href="https://arxiv.org/abs/2212.12017">OPT-IML-175B</a>). As a result, operating such sizable models poses significant challenges because they demand considerable computational power and impose substantial requirements on the memory capacities of GPUs and TPUs, making their training and inference expensive and inefficient. Extensive storage is required to maintain a unique LLM copy for each downstream task. Moreover, the most powerful multi-task LLMs (e.g., FLAN-PaLM-540B) are closed-sourced, making them impossible to be adapted. However, in practical applications, harnessing a single multi-task LLM to manage all conceivable tasks in a zero-shot manner remains difficult, particularly when dealing with complex tasks, personalized tasks and those that cannot be succinctly defined using instructions. On the other hand, the size of downstream training data is usually insufficient to train a model well without incorporating rich prior knowledge. Hence, it is long desired to adapt LLMs with downstream supervision while bypassing storage, memory, and access issues. 
</p>

<p>
Certain <em>parameter-efficient tuning</em> strategies, including <a href="https://aclanthology.org/2021.acl-long.353.pdf">prompt tuning</a> and <a href="https://openreview.net/pdf?id=nZeVKeeFYf9">adapters</a>, substantially diminish storage requirements, but they still perform back-propagation through LLM parameters during the tuning process, thereby keeping their memory demands high. Additionally, some <em><a href="https://arxiv.org/pdf/2301.00234.pdf">in-context learning</a></em> techniques circumvent parameter tuning by integrating a limited number of supervised examples into the instruction. However, these techniques are constrained by the model's maximum input length, which permits only a few samples to guide task resolution.
</p>

<p>
In “<a href="https://arxiv.org/abs/2311.06720">Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer</a>”, presented at <a href="https://nips.cc/virtual/2023/index.html">NeurIPS 2023</a>, we propose a novel approach that enhances the performance and efficiency of multi-task LLMs. We introduce a lightweight pre-trained scorer, Cappy, based on continual pre-training on top of <a href="https://arxiv.org/abs/1907.11692">RoBERTa</a> with merely 360 million parameters. Cappy takes in an instruction and a candidate response as input, and produces a score between 0 and 1, indicating an estimated correctness of the response with respect to the instruction. Cappy functions either independently on classification tasks or serves as an auxiliary component for LLMs, boosting their performance. Moreover, Cappy efficiently enables downstream supervision without requiring any finetuning, which avoids the need for back-propagation through LLM parameters and reduces memory requirements. Finally, adaptation with Cappy doesn’t require access to LLM parameters as it is compatible with closed-source multi-task LLMs, such as those only accessible via WebAPIs.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKxfEf2em6vxULs9wHGOB2jU7AiGMhUiEsJENdGWjB-8AMW6T2uRUrp3k3776491wzNsQCEk2T26AmiPNaKi-mfiIRNHe7JKZuR4ETQbHrM5h1knDNDBZ-qPw6sPGhtA4v0dz9YtKbHyoXPWEgYkY6r-tv8brepN8_Qq7MjCIwGUaYw5LmJMY4KLxu28ku/s1999/Cappy%20overview.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKxfEf2em6vxULs9wHGOB2jU7AiGMhUiEsJENdGWjB-8AMW6T2uRUrp3k3776491wzNsQCEk2T26AmiPNaKi-mfiIRNHe7JKZuR4ETQbHrM5h1knDNDBZ-qPw6sPGhtA4v0dz9YtKbHyoXPWEgYkY6r-tv8brepN8_Qq7MjCIwGUaYw5LmJMY4KLxu28ku/s16000/Cappy%20overview.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Cappy takes an instruction and response pair as input and outputs a score ranging from 0 to 1, indicating an estimation of the correctness of the response with respect to the instruction.</td></tr></tbody></table>

<br />

<div style="line-height: 40%;">
    <br />
</div>
<h2>Pre-training</h2>


<p>
We begin with the same dataset collection, which includes 39 diverse datasets from <a href="https://arxiv.org/abs/2202.01279">PromptSource</a> that were used to train <a href="https://arxiv.org/abs/2110.08207">T0</a>. This collection encompasses a wide range of task types, such as question answering, sentiment analysis, and summarization. Each dataset is associated with one or more templates that convert each instance from the original datasets into an instruction paired with its ground truth response.
</p>

<p>
Cappy's regression modeling requires each pre-training data instance to include an instruction-response pair along with a correctness annotation for the response, so we produce a dataset with correctness annotations that range from 0 to 1. For every instance within a generation task, we leverage an existing multi-task LLM to generate multiple responses by sampling, conditioned on the given instruction. Subsequently, we assign an annotation to the pair formed by the instruction and every response, using the similarity between the response and the ground truth response of the instance. Specifically, we employ <a href="https://aclanthology.org/W04-1013/">Rouge-L</a>, a commonly-used metric for measuring overall multi-task performance that has demonstrated a strong alignment with human evaluation, to calculate this similarity as a form of weak supervision.
</p>

<p>
As a result, we obtain an effective regression dataset of 160 million instances paired with correctness score annotations. The final Cappy model is the result of continuous pre-training using the regression dataset on top of the <a href="https://arxiv.org/abs/1907.11692">RoBERTa</a> model. The pre-training of Cappy is conducted on Google's <a href="https://arxiv.org/abs/2304.01433">TPU-v4</a>, with <a href="https://arxiv.org/pdf/2310.16355.pdf">RedCoast</a>, a lightweight toolkit for automating distributed training.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguKQnabejBzwCo7XEYZBJaaHi9_Z0Z03aofMxhmno2dKMbh2d6qVhmu7kKLN7FVExLXwYZYu1UEa1brRSC7bX3ASLyZymVyougwQqhCoE7Iio6DvIzdIK_dYT-1IGk41jZ6qdYcDynxezST6FY8u73opddwlGcGTf-3fXY4KfPo5hhfIinUl7iXRN7V6Sr/s1999/Cappy%20data%20augmentation.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguKQnabejBzwCo7XEYZBJaaHi9_Z0Z03aofMxhmno2dKMbh2d6qVhmu7kKLN7FVExLXwYZYu1UEa1brRSC7bX3ASLyZymVyougwQqhCoE7Iio6DvIzdIK_dYT-1IGk41jZ6qdYcDynxezST6FY8u73opddwlGcGTf-3fXY4KfPo5hhfIinUl7iXRN7V6Sr/s16000/Cappy%20data%20augmentation.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Data augmentation with a multi-task LLM to construct a weakly supervised regression dataset for Cappy’s pre-training and fine-tuning.</td></tr></tbody></table>

<br />

<div style="line-height: 40%;">
    <br />
</div>
<h2>Applying Cappy</h2>


<p>
Cappy solves practical tasks within a candidate-selection mechanism. More specifically, given an instruction and a set of candidate responses, Cappy produces a score for each candidate response. This is achieved by inputting the instruction alongside each individual response, and then assigning the response with the highest score as its prediction. In classification tasks, all candidate responses are inherently predefined. For example, for an instruction of a sentiment classification task (e.g., “Based on this review, would the user recommend this product?: ‘Stunning even for the non-gamer.’”), the candidate responses are “Yes” or “No”. In such scenarios, Cappy functions independently. On the other hand, in generation tasks, candidate responses are not pre-defined, requiring an existing multi-task LLM to yield the candidate responses. In this case, Cappy serves as an auxiliary component of the multi-task LLM, enhancing its decoding.
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h3>Adapting multi-task LLMs with Cappy </h3>


<p>
When there is available downstream training data, Cappy enables effective and efficient adaptation of multi-task LLMs on downstream tasks. Specifically, we fine-tune Cappy to integrate downstream task information into LLM predictions. This process involves creating a separate regression dataset specific to the downstream training data with the same data annotation process used to construct the pre-training data. As a result, the fine-tuned Cappy collaborates with a multi-task LLM, boosting the LLM's performance on the downstream task.
</p>

<p>
In contrast to other LLM tuning strategies, adapting LLMs with Cappy significantly reduces the high demand for device memory as it avoids the need for back-propagation through LLM parameters for downstream tasks.  Moreover, Cappy adaptation does not rely on the access to LLM parameters, making it compatible with closed-source multi-task LLMs, such as the ones only accessible via WebAPIs. Compared with in-context learning approaches, which circumvent model tuning by attaching training examples to the instruction prefix, Cappy is not restricted by the LLM's maximum input length. Thus, Cappy can incorporate an unlimited number of downstream training examples. Cappy can also be applied with other adaptation methods, such as fine-tuning and in-context learning, further boosting their overall performance.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhf1zSOPmuCuPWHPOXYRczk86xESIKANYJN7jUqjkoSQabuQrDyLEfyLCXG0eAEHG1xiYL6jrZ8iMC14a2FhQs7XNwyncRdCyfIRa3KlLx3786yfSXfP9pEwtUEJ6ax7l5J8MchxjH9cV_hKqQFanTh3kNCs_JHYw0vsMOFi09-69-anFrqJShRgYFcKvfe/s1999/Cappy%20downstream%20adaptation.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhf1zSOPmuCuPWHPOXYRczk86xESIKANYJN7jUqjkoSQabuQrDyLEfyLCXG0eAEHG1xiYL6jrZ8iMC14a2FhQs7XNwyncRdCyfIRa3KlLx3786yfSXfP9pEwtUEJ6ax7l5J8MchxjH9cV_hKqQFanTh3kNCs_JHYw0vsMOFi09-69-anFrqJShRgYFcKvfe/s16000/Cappy%20downstream%20adaptation.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Downstream adaptation comparison between Cappy and approaches that rely on an LLM’s parameters, such as fine-tuning and prompt tuning. Cappy’s application enhances multi-task LLMs.</td></tr></tbody></table>
<br />

<div style="line-height: 40%;">
    <br />
</div>
<h2>Results</h2>
<p>
We assess Cappy’s performance across eleven held-out language understanding classification tasks from <a href="https://arxiv.org/abs/2202.01279">PromptSource</a>. We demonstrate that Cappy, with 360M parameters, outperforms OPT-175B and OPT-IML-30B, and matches the accuracy of  the best existing multi-task LLMs (T0-11B and OPT-IML-175B). These findings highlight Cappy’s capabilities and parameter efficiency, which can be credited to its scoring-based pre-training strategy that integrates contrastive information by differentiating between high-quality and low-quality responses. On the contrary, previous multi-task LLMs depend exclusively on <a href="https://en.wikipedia.org/wiki/Teacher_forcing">teacher-forcing training</a> that utilizes only the ground truth responses.
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyehdahD05Plit772klEfeGTN1GteCZcwwsyWbGgOTtgH4VD3hzPkF8PSDdYZe2EOE0nwL9xdNLZYLzvBJrm9ECTSGIWWUJ-Xo-1uVQUmN8uu0_5dLAERYPvOFfahf1ZZ2bId0tna1ch8BBXV9xKWpPKNIoAlihdNxZvlegShjI6Fjd5Twd8kv6w-axtUW/s1999/Cappy%20accuracy.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjyehdahD05Plit772klEfeGTN1GteCZcwwsyWbGgOTtgH4VD3hzPkF8PSDdYZe2EOE0nwL9xdNLZYLzvBJrm9ECTSGIWWUJ-Xo-1uVQUmN8uu0_5dLAERYPvOFfahf1ZZ2bId0tna1ch8BBXV9xKWpPKNIoAlihdNxZvlegShjI6Fjd5Twd8kv6w-axtUW/s16000/Cappy%20accuracy.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The overall accuracy averaged over eleven test tasks from PromptSource. “RM” refers to a <a href="https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2">pre-trained RLHF reward model</a>. Cappy matches the best ones among existing multi-task LLMs.</td></tr></tbody></table>


<p>
We also examine the adaptation of multi-task LLMs with Cappy on complex tasks from <a href="https://arxiv.org/abs/2206.04615">BIG-Bench</a>, a set of manually curated tasks that are considered beyond the capability of many LLMs. We focus on all the 45 generation BIG-Bench tasks, specifically those that do not offer pre-established answer choices. We evaluate the performance using the Rouge-L score (representing the overall similarity between model generations and corresponding ground truths) on every test set, reporting the average score across 45 tests. In this experiment, all variants of FLAN-T5 serve as the backbone LLMs, and the foundational FLAN-T5 models are frozen. These results, shown below, suggest that Cappy enhances the performance of FLAN-T5 models by a large margin, consistently outperforming the most effective baseline achieved through sample selection using self-scoring of the LLM itself.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUmqWX5mq_zgs3nK6TSR3sNEunAburBnwxpIaFNxTuXbhLKeuI-c71IBxZw3tEnnnOHeE7heImqnZyluCAV92_2fhhXEfus_4R0MC78e_WOOXcSNvfyiVLNqNGhYK88YfiT__Ijss-OPpCo4XDz4vLFjtJKM-Mko_n2IgMabNI5J1a3LAVlIvBvRpiZ8GZ/s1999/Cappy%20averaged%20Rouge-L%20score.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUmqWX5mq_zgs3nK6TSR3sNEunAburBnwxpIaFNxTuXbhLKeuI-c71IBxZw3tEnnnOHeE7heImqnZyluCAV92_2fhhXEfus_4R0MC78e_WOOXcSNvfyiVLNqNGhYK88YfiT__Ijss-OPpCo4XDz4vLFjtJKM-Mko_n2IgMabNI5J1a3LAVlIvBvRpiZ8GZ/s16000/Cappy%20averaged%20Rouge-L%20score.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The averaged Rouge-L score over 45 complex tasks within BIG-Bench. The x-axis refers to FLAN-T5 models of different sizes. Every dashed line represents an approach working on FLAN-T5s. Self-scoring refers to using the cross-entropy of LLM to select responses. Cappy enhances the performance of FLAN-T5 models by a large margin.</td></tr></tbody></table>

<br />

<div style="line-height: 40%;">
    <br />
</div>
<h2>Conclusion</h2>
<p>
We introduce Cappy, a novel approach that enhances the performance and efficiency of multi-task LLMs. In our experiments, we adapt a single LLM to several domains with Cappy. In the future, Cappy as a pre-trained model can potentially be used in other creative ways beyond on single LLMs.
</p>



<div style="line-height: 40%;">
    <br />
</div>
<h2>Acknowledgments</h2>
<p>
<em>Thanks to Bowen Tan, Jindong Chen, Lei Meng, Abhanshu Sharma and Ewa Dominowska for their valuable feedback. We would also like to thank Eric Xing and Zhiting Hu for their suggestions. </em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-6090481872694489715</id>
            <title>Talk like a graph: Encoding graphs for large language models</title>
            <link>http://blog.research.google/2024/03/talk-like-graph-encoding-graphs-for.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-6090481872694489715</guid>
            <pubDate></pubDate>
            <updated>2024-03-19T09:12:05.568-07:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8L7r_SCzFsKsWegtrn8_EOoO2imefs-V_GVHzbM0Xw7GmAxoXIIX0RtpJ2JvloeenxcKCNmhCH_VXRMpu8b5dJP39UkhMJS0wP86TUftZtUi-hfj6tZdVEn30MZAeQEx762q1vN-q4DWP2EdOBIHy_CgNFMcliaJYnzxZHjnuifbVWy52zlls20m4BkyJ/s72-c/Screenshot%202024-03-12%20at%202.18.27%E2%80%AFPM.png"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Bahare Fatemi and Bryan Perozzi, Research Scientists, Google Research</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg8L7r_SCzFsKsWegtrn8_EOoO2imefs-V_GVHzbM0Xw7GmAxoXIIX0RtpJ2JvloeenxcKCNmhCH_VXRMpu8b5dJP39UkhMJS0wP86TUftZtUi-hfj6tZdVEn30MZAeQEx762q1vN-q4DWP2EdOBIHy_CgNFMcliaJYnzxZHjnuifbVWy52zlls20m4BkyJ/s1600/Screenshot%202024-03-12%20at%202.18.27%E2%80%AFPM.png" style="display: none;" />

<p>
Imagine all the things around you — your friends, tools in your kitchen, or even the parts of your bike. They are all connected in different ways. In computer science, the term <em><a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a> </em>is used to describe connections between objects. Graphs consist of nodes (the objects themselves) and edges (connections between two nodes, indicating a  relationship between them). Graphs are everywhere now. The internet itself is a giant graph of websites linked together. Even the knowledge search engines use is organized in a graph-like way.
</p><a name="more"></a>



<p>
Furthermore, consider the remarkable advancements in artificial intelligence — such as chatbots that can write stories in seconds, and even software that can interpret medical reports. This exciting progress is largely thanks to large language models (LLMs). New LLM technology is constantly being developed for different uses. 
</p>
<p>
Since graphs are everywhere and LLM technology is on the rise, in “<a href="https://openreview.net/forum?id=IuXR1CCrSi">Talk like a Graph: Encoding Graphs for Large Language Models</a>”, presented at <a href="https://iclr.cc/">ICLR 2024</a>, we present a way to teach powerful LLMs how to better reason with graph information. Graphs are a useful way to organize information, but LLMs are mostly trained on regular text. The objective is to test different techniques to see what works best and gain practical insights. Translating graphs into text that LLMs can understand is a remarkably complex task. The difficulty stems from the inherent complexity of graph structures with multiple nodes and the intricate web of edges that connect them. Our work studies how to take a graph and translate it into a format that an LLM can understand. We also design a benchmark called <em><a href="https://github.com/google-research/google-research/tree/master/graphqa">GraphQA</a></em> to study different approaches on different graph reasoning problems and show how to <em>phrase</em> a graph-related problem in a way that enables the LLM to solve the graph problem. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: 1) the graph encoding method, 2) the nature of the graph task itself, and 3) interestingly, the very structure of the graph considered. These findings give us clues on how to best represent graphs for LLMs. Picking the right method can make the LLM up to 60% better at graph tasks!
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAnieWluvqGQtyh_L3a_Y7XfYUR2dBRGpQf58DpzJfIrkyM2JnwxiCOvTzDidvP-GtbtRe4NsJUEFlzpW8nQbf8WGQD6P_C2jjsRZeLiyDSO8QF8IiGCRYnSa4MxruywJt60gU8KrH6w87ZoBXsGbPmyWDx01j1nqSCaEtfFeNTmAWSLcVVcND8XuzoaHb/s1600/image7.gif" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjAnieWluvqGQtyh_L3a_Y7XfYUR2dBRGpQf58DpzJfIrkyM2JnwxiCOvTzDidvP-GtbtRe4NsJUEFlzpW8nQbf8WGQD6P_C2jjsRZeLiyDSO8QF8IiGCRYnSa4MxruywJt60gU8KrH6w87ZoBXsGbPmyWDx01j1nqSCaEtfFeNTmAWSLcVVcND8XuzoaHb/s16000/image7.gif" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Pictured, the process of encoding a graph as text using two different approaches and feeding the text and a question about the graph to the LLM.</td></tr></tbody></table>


<div style="line-height: 40%;">
    <br />
</div>
<h2>Graphs as text</h2>


<p>
To be able to systematically find out what is the best way to translate a graph to text, we first design a benchmark called <em><a href="https://github.com/google-research/google-research/tree/master/graphqa">GraphQA</a></em>. Think of GraphQA as an exam designed to evaluate powerful LLMs on graph-specific problems. We want to see how well LLMs can understand and solve problems that involve graphs in different setups. To create a comprehensive and realistic exam for LLMs, we don’t just use one type of graph, we use a mix of graphs ensuring breadth in the number of connections. This is mainly because different graph types make solving such problems easier or harder. This way, GraphQA can help expose biases in how an LLM thinks about the graphs, and the whole exam gets closer to a realistic setup that LLMs might encounter in the real world.
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlHJKqbwgQJFMK4siQJH_Ggag9B8lStCQ4CcXk8iPnNPgxGPLYl_LTrIfjxuP7vKKtzJITlltZ5pcq7RElYNVQJ8PKi9Sr3ctigYfLs6SBlMAEhDHP2nV2PJ-uLhJxUkZ3MdAGV7R8rjw0u6Y8QTCwrMTyqz7tuxzb3TnIFabf4ZZbsSQ95MSboOA42i4w/s1368/image6.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlHJKqbwgQJFMK4siQJH_Ggag9B8lStCQ4CcXk8iPnNPgxGPLYl_LTrIfjxuP7vKKtzJITlltZ5pcq7RElYNVQJ8PKi9Sr3ctigYfLs6SBlMAEhDHP2nV2PJ-uLhJxUkZ3MdAGV7R8rjw0u6Y8QTCwrMTyqz7tuxzb3TnIFabf4ZZbsSQ95MSboOA42i4w/s16000/image6.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Overview of our framework for reasoning with graphs using LLMs.</td></tr></tbody></table>



<p>
GraphQA focuses on simple tasks related to graphs, like checking if an edge exists, calculating the number of nodes or edges, finding nodes that are connected to a specific node, and checking for cycles in a graph. These tasks might seem basic, but they require understanding the relationships between nodes and edges. By covering different types of challenges, from identifying patterns to creating new connections, GraphQA helps models learn how to analyze graphs effectively. These basic tasks are crucial for more complex reasoning on graphs, like finding the shortest path between nodes, detecting communities, or identifying influential nodes. Additionally, GraphQA includes generating random graphs using various algorithms like <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model">Erdős-Rényi</a>, <a href="https://en.wikipedia.org/wiki/Scale-free_network">scale-free networks</a>, <a href="https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model">Barabasi-Albert model</a>, and <a href="https://en.wikipedia.org/wiki/Stochastic_block_model">stochastic block model</a>, as well as simpler graph structures like paths, complete graphs, and star graphs, providing a diverse set of data for training.
</p>
<p>
When working with graphs, we also need to find ways to ask graph-related questions that LLMs can understand.  <em>Prompting heuristics</em> are different strategies for doing this. Let's break down the common ones:
</p>
<ul>

<li><em>Zero-shot</em>: simply describe the task ("Is there a cycle in this graph?") and tell the LLM to go for it. No examples provided.

</li><li><em>Few-shot</em>: This is like giving the LLM a mini practice test before the real deal. We provide a few example graph questions and their correct answers.

</li><li><em>Chain-of-Thought</em>: Here, we show the LLM how to break down a problem step-by-step with examples. The goal is to teach it to generate its own "thought process" when faced with new graphs.

</li><li><em>Zero-CoT</em>: Similar to CoT, but instead of training examples, we give the LLM a simple prompt, like "Let's think step-by-step," to trigger its own problem-solving breakdown.

</li><li><em>BAG (build a graph)</em>: This is specifically for graph tasks. We add the phrase "Let's build a graph..." to the description, helping the LLM focus on the graph structure.
</li>
</ul>
<p>
We explored different ways to translate graphs into text that LLMs can work with. Our key questions were:
</p>
<ul>

<li><em>Node encoding</em>: How do we represent individual nodes? Options tested include simple <a href="https://en.wikipedia.org/wiki/Integer">integers</a>, common names (people, characters), and letters.

</li><li><em>Edge encoding</em>: How do we describe the relationships between nodes? Methods involved parenthesis notation, phrases like "are friends", and symbolic representations like arrows.
</li>
</ul>
<p>
Various node and edge encodings were combined systematically. This led to functions like the ones in the following figure:
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHqSBznT1daPxVFMf0ZOR0uiZpjYrTG46t71FWy4tq5IMh-Ijhbzp_toJVmvp72FGrtoQXFkhCaaDVkhCzQXzcfRUPvW7151j22mmVxejpNJdO6VcvdHOkmEye_1zEBtfvAVgSw6RPFOiCpdo9LnetLvgrS-OL7IZPRLpBaCWGny_mzk6wpZcHDY-oS1ts/s855/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="302" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHqSBznT1daPxVFMf0ZOR0uiZpjYrTG46t71FWy4tq5IMh-Ijhbzp_toJVmvp72FGrtoQXFkhCaaDVkhCzQXzcfRUPvW7151j22mmVxejpNJdO6VcvdHOkmEye_1zEBtfvAVgSw6RPFOiCpdo9LnetLvgrS-OL7IZPRLpBaCWGny_mzk6wpZcHDY-oS1ts/w640-h302/image1.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Examples of graph encoding functions used to encode graphs via text.</td></tr></tbody></table>



<div style="line-height: 40%;">
    <br />
</div>
<h2>Analysis and results</h2>


<p>
We carried out three key experiments: one to test how LLMs handle graph tasks, and two to understand how the size of the LLM and different graph shapes affected performance. We run all our experiments on GraphQA. 
</p>


<div style="line-height: 40%;">
    <br />
</div>
<h3>How LLMs handle graph tasks </h3>


<p>
In this experiment, we tested how well pre-trained LLMs tackle graph problems like identifying connections, cycles, and node degrees. Here is what we learned:
</p>
<ul>

<li><em>LLMs struggle:</em> On most of these basic tasks, LLMs did not do much better than a random guess. 

</li><li><em>Encoding matters significantly</em>: How we represent the graph as text has a great effect on LLM performance. The "incident" encoding excelled for most of the tasks in general.
</li>
</ul>
<p>
Our results are summarized in the following chart. 
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYJLoJxI1twg6uV55JaKbVVnhO-dhgcaSN_B-FK9MTT8kKI1k_xnbGCvaEpmr82U4OGQxJ-oGNYOa0izo3jD1Ssvz8BVaKgw5ObjwN6_zS54BOALM_aO6TbLf-7SfcokAqRRC9fUbdErDeuadKBuRq7ihEootiLodZoYLKtZVDAgTI1ZrxviY7SI1PcFm5/s1864/image8.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhYJLoJxI1twg6uV55JaKbVVnhO-dhgcaSN_B-FK9MTT8kKI1k_xnbGCvaEpmr82U4OGQxJ-oGNYOa0izo3jD1Ssvz8BVaKgw5ObjwN6_zS54BOALM_aO6TbLf-7SfcokAqRRC9fUbdErDeuadKBuRq7ihEootiLodZoYLKtZVDAgTI1ZrxviY7SI1PcFm5/s16000/image8.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Comparison of various graph encoder functions based on their accuracy on different graph tasks. The main conclusion from this figure is that the graph encoding functions matter significantly.</td></tr></tbody></table>



<div style="line-height: 40%;">
    <br />
</div>
<h3>Bigger is (usually) better </h3>


<p>
In this experiment, we wanted to see if the size of the LLM (in terms of the number of parameters) affects how well they can handle graph problems. For that, we tested the same graph tasks on the XXS, XS, S, and L sizes of <a href="https://ai.google/static/documents/palm2techreport.pdf">PaLM 2</a>. Here is a summary of our findings:
</p>
<ul>

<li>In general, bigger models did better on graph reasoning tasks. It seems like the extra parameters gave them space to learn more complex patterns.

</li><li>Oddly, size didn't matter as much for the “edge existence” task (finding out if two nodes in a graph are connected).

</li><li>Even the biggest LLM couldn't consistently beat a simple baseline solution on the cycle check problem (finding out if a graph contains a cycle or not). This shows LLMs still have room to improve with certain graph tasks.
</li>
</ul>





<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiG-zu3s3K3iCIV5k2gakpMwQ_38a08_NrYeO3yITJc64EYiK36sksPulORuZR_BrGdmxZmCWEgIX2sWc42M4f3jpo8v17AddfoORPliE-SefptA4h4gye_g_PBKnufZ9kzTkI0f9MCKwSvuEqfcdgxNiycB2bGUQyUtXx8F7XU4qpXKZGEINZudJxlu-6L/s1227/image3.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="500" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiG-zu3s3K3iCIV5k2gakpMwQ_38a08_NrYeO3yITJc64EYiK36sksPulORuZR_BrGdmxZmCWEgIX2sWc42M4f3jpo8v17AddfoORPliE-SefptA4h4gye_g_PBKnufZ9kzTkI0f9MCKwSvuEqfcdgxNiycB2bGUQyUtXx8F7XU4qpXKZGEINZudJxlu-6L/w640-h500/image3.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Effect of model capacity on graph reasoning task for PaLM 2-XXS, XS, S, and L.</td></tr></tbody></table>



<div style="line-height: 40%;">
    <br />
</div>
<h3>Do different graph shapes confuse LLMs </h3>


<p>
We wondered if the "shape" of a graph (how nodes are connected) influences how well LLMs can solve problems on it. Think of the following figure as different examples of graph shapes.
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ9tU8x8LvDYvwwN9XL4j64tXEq-7fGwnzYvS5zpNcEjk9yjxLH2yYmOAfKwr7_w9dHTUD1xtnI6IMAswp0pyManGDEO1ej1WeH9yByu-5ivtlfU5N-7OWJDtnR1uMeG7oWs1eqyiZFOyUpUa5GddPtECkd4ZvNPSx9rtS8fh83ahArgXtpKtVy7tQES9N/s1400/image4.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgQ9tU8x8LvDYvwwN9XL4j64tXEq-7fGwnzYvS5zpNcEjk9yjxLH2yYmOAfKwr7_w9dHTUD1xtnI6IMAswp0pyManGDEO1ej1WeH9yByu-5ivtlfU5N-7OWJDtnR1uMeG7oWs1eqyiZFOyUpUa5GddPtECkd4ZvNPSx9rtS8fh83ahArgXtpKtVy7tQES9N/s16000/image4.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Samples of graphs generated with different graph generators from GraphQA. ER, BA, SBM, and SFN refers to <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model">Erdős–Rényi</a>, <a href="https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model">Barabási–Albert</a>, <a href="https://en.wikipedia.org/wiki/Stochastic_block_model">Stochastic Block Model</a>, and <a href="https://en.wikipedia.org/wiki/Scale-free_network">Scale-Free Network</a> respectively.</td></tr></tbody></table>




<p>
We found that graph structure has a big impact on LLM performance. For example, in a task asking if a cycle exists, LLMs did great on tightly interconnected graphs (cycles are common there) but struggled on path graphs (where cycles never happen). Interestingly, providing some mixed examples helped it adapt. For instance, for cycle check, we added some examples containing a cycle and some examples with no cycles as few-shot examples in our prompt. Similar patterns occurred with other tasks.
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqf5piX3TuSfL0DqpUG7ZkBgMKEqqeCIh0feFG4ddMiaHTFgLY3iPkI4UD3gZpAKeTHgfhItKeXo8P3M4sGSQRZJJsXMAVFutTDuWziSwt1CBvt7kV1VSOSHqGTu0yk7lAym4XYJERrS3FETWbj17agumgHaln1EevI_LyzqAbNFZjYNPZGKjw1fgKBydk/s1864/image5.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgqf5piX3TuSfL0DqpUG7ZkBgMKEqqeCIh0feFG4ddMiaHTFgLY3iPkI4UD3gZpAKeTHgfhItKeXo8P3M4sGSQRZJJsXMAVFutTDuWziSwt1CBvt7kV1VSOSHqGTu0yk7lAym4XYJERrS3FETWbj17agumgHaln1EevI_LyzqAbNFZjYNPZGKjw1fgKBydk/s16000/image5.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Comparing different graph generators on different graph tasks. The main observation here is that graph structure has a significant impact on the LLM’s performance. ER, BA, SBM, and SFN refers to <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model">Erdős–Rényi</a>, <a href="https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model">Barabási–Albert</a>, <a href="https://en.wikipedia.org/wiki/Stochastic_block_model">Stochastic Block Model</a>, and <a href="https://en.wikipedia.org/wiki/Scale-free_network">Scale-Free Network</a> respectively.</td></tr></tbody></table>


<div style="line-height: 40%;">
    <br />
</div>
<h2>Conclusion</h2>


<p>
In short, we dug deep into how to best represent graphs as text so LLMs can understand them. We found three major factors that make a difference:
</p>
<ul>

<li><em>How to translate the graph to text</em>: how we represent the graph as text significantly influences LLM performance. The incident encoding excelled for most of the tasks in general..

</li><li><em>Task type</em>: Certain types of graph questions tend to be harder for LLMs, even with a good translation from graph to text.

</li><li><em>Graph structure</em>: Surprisingly, the "shape" of the graph that on which we do inference (dense with connections, sparse, etc.) influences how well an LLM does.
</li>
</ul>
<p>
This study revealed key insights about how to prepare graphs for LLMs. The right encoding techniques can significantly boost an LLM's accuracy on graph problems (ranging from around 5% to over 60% improvement). Our new benchmark, GraphQA, will help drive further research in this area.
</p>



<div style="line-height: 40%;">
    <br />
</div>
<h2>Acknowledgements</h2>


<p>
<em>We would like to express our gratitude to our co-author, Jonathan Halcrow, for his valuable contributions to this work. We express our sincere gratitude to Anton Tsitsulin, Dustin Zelle, Silvio Lattanzi, Vahab Mirrokni, and the entire graph mining team at Google Research, for their insightful comments, thorough proofreading, and constructive feedback which greatly enhanced the quality of our work. We would also like to extend special thanks to Tom Small for creating the animation used in this post.</em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-470840348983280912</id>
            <title>Chain-of-table: Evolving tables in the reasoning chain for table understanding</title>
            <link>http://blog.research.google/2024/03/chain-of-table-evolving-tables-in.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-470840348983280912</guid>
            <pubDate></pubDate>
            <updated>2024-03-11T12:13:03.824-07:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1smBN07qkS32Aop4if0AeINQQea0Grv8dw7GiRFBNoHBlgkkftynVBNjO6BckpF4vq8d0VqC1v0LoeFAVFqOLrBGlqvMNiCMUtIhHxVvsBjbPxvZLcNcD_Sa1sI_bDlqDLWn_C39MbPNm8VUjr2vhTBuaL4qCc1LUB1VH5iM0UVsswIWWq_uQg88YRWmb/s72-c/Chain-of-Table.png"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Zilong Wang, Student Researcher, and Chen-Yu Lee, Research Scientist, Cloud AI Team</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1smBN07qkS32Aop4if0AeINQQea0Grv8dw7GiRFBNoHBlgkkftynVBNjO6BckpF4vq8d0VqC1v0LoeFAVFqOLrBGlqvMNiCMUtIhHxVvsBjbPxvZLcNcD_Sa1sI_bDlqDLWn_C39MbPNm8VUjr2vhTBuaL4qCc1LUB1VH5iM0UVsswIWWq_uQg88YRWmb/s832/Chain-of-Table.png" style="display: none;" />

<p>
People use tables every day to organize and interpret complex information in a structured, easily accessible format. Due to the ubiquity of such tables, reasoning over tabular data has long been a central topic in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> (NLP). Researchers in this field have aimed to leverage language models to help users answer questions, verify statements, and analyze data based on tables. However, language models are trained over large amounts of plain text, so the inherently structured nature of tabular data can be difficult for language models to fully comprehend and utilize.
</p>
<a name="more"></a>
<p>
Recently, <a href="https://en.wikipedia.org/wiki/Large_language_model">large language models</a> (LLMs) have achieved outstanding performance across diverse <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">natural language understanding</a> (NLU) tasks by generating reliable reasoning chains, as shown in works like <a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought</a> and <a href="https://arxiv.org/abs/2205.10625">Least-to-Most</a>. However, the most suitable way for LLMs to reason over tabular data remains an open question.
</p>
<p>
In “<a href="https://arxiv.org/abs/2401.04398">Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding</a>”, we propose a framework to tackle table understanding tasks, where we train LLMs to outline their reasoning step by step, updating a given table iteratively to reflect each part of a thought process, akin to how people solve the table-based problems. This enables the LLM to transform the table into simpler and more manageable segments so that it can understand and analyze each part of the table in depth. This approach has yielded significant improvements and achieved new state-of-the-art results on the <a href="https://arxiv.org/abs/1508.00305">WikiTQ</a>, <a href="https://arxiv.org/abs/1909.02164">TabFact</a>, and <a href="https://arxiv.org/abs/2104.00369">FeTaQA</a> benchmarks. The figure below shows the high-level overview of the proposed Chain-of-Table and other methods.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKT_df1rC8nK-ULOLPjtJ8gFaDHzRi7DX92Ix7OboQhOUNvqh_Melp9SVRWEsgL1Vu6IX9RuMgX7_UIuyeuHr7H0YwJdo6om2M2rX5d9wqOWsXWVAa9o0S75bIt7qG2DiGlhYypk0KKBMSxz2Z8vgmQqxTvy3bVrmH4nSC4Nzv8fZm6mOoA5yEXN_CgC4h/s1478/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjKT_df1rC8nK-ULOLPjtJ8gFaDHzRi7DX92Ix7OboQhOUNvqh_Melp9SVRWEsgL1Vu6IX9RuMgX7_UIuyeuHr7H0YwJdo6om2M2rX5d9wqOWsXWVAa9o0S75bIt7qG2DiGlhYypk0KKBMSxz2Z8vgmQqxTvy3bVrmH4nSC4Nzv8fZm6mOoA5yEXN_CgC4h/s16000/image2.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Given a complex table where a cyclist’s nationality and name are in the same cell, (a) generic, multi-step reasoning is unable to provide the correct answer (b) program-aided reasoning generates and executes programs (e.g., SQL queries) to deliver the answer, but falls short in accurately addressing the question. In contrast, (c) Chain-of-Table iteratively samples a chain of operations that effectively transform the complex table into a version specifically tailored to the question.</td></tr></tbody></table>
<br />

<h2>Chain-of-Table</h2>


<p>
In Chain-of-Table, we guide LLMs using <a href="https://arxiv.org/abs/2005.14165">in-context learning</a> to iteratively generate operations and to update the table to represent its reasoning chain over tabular data. This enables LLMs to dynamically plan the next operation based on the results of previous ones. This continuous evolution of the table forms a chain, which provides a more structured and clear representation of the reasoning process for a given problem and enables more accurate and reliable predictions from the LLM. 
</p>
<p>
For example, when asked, “Which actor has the most NAACP image awards?” the Chain-of-Table framework prompts an LLM to generate tabular operations mirroring tabular reasoning processes. It first identifies the relevant columns. Then, it aggregates rows based on shared content. Finally, it reorders the aggregated results to yield a final table that clearly answers the posed question. 
</p>
<p>
These operations transform the table to align with the question presented. To balance performance with computational expense on large tables, we construct the operation chain according to a subset of tabular rows.. Meanwhile, the step-by-step operations reveal the underlying reasoning process through the display of intermediate results from the tabular operations, fostering enhanced interpretability and understanding.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8JwNNHW6SR1PSRTj79oQKqE1K48onxbcM9uwIlacEGnUqtua0jgkXQ-CfyUukJ0qiBhqsKl1_YfeJmcqkMEe5TR08eo9ZEqymWYszwNyKfZjcx0T-wYwEnHqCvdlf9lJAG8UTBN6RZQngH7sv0hQ9szR1wgjyiFSaOIqVHC08bJv6HeaXvWJMHH41wI4_/s1999/image4.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8JwNNHW6SR1PSRTj79oQKqE1K48onxbcM9uwIlacEGnUqtua0jgkXQ-CfyUukJ0qiBhqsKl1_YfeJmcqkMEe5TR08eo9ZEqymWYszwNyKfZjcx0T-wYwEnHqCvdlf9lJAG8UTBN6RZQngH7sv0hQ9szR1wgjyiFSaOIqVHC08bJv6HeaXvWJMHH41wI4_/s16000/image4.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Illustration of the tabular reasoning process in Chain-of-Table. This iterative process involves dynamically planning an operation chain and accurately storing intermediate results in the transformed tables. These intermediate tables serve as a tabular thought process that can guide the LLM to land to the correct answer more reliably.</td></tr></tbody></table>
<br />

<p>
Chain-of-Table consists of three main stages. In the first stage, it instructs the LLM to dynamically plan the next operation by in-context learning. Specifically, the prompt involves three components as shown in the following figure: 
</p>
<ol>

<li>  The question <em>Q</em>: “Which country had the most cyclists finish in the top 3?”

</li><li>  The operation history <em>chain</em>: <code>f_add_col(Country)</code> and <code>f_select_row(1, 2, 3)</code>.

</li><li>  The latest intermediate table <em>T</em>: the transformed intermediate table. 
</li>
</ol>
<p>
By providing the triplet <em>(T, Q, chain)</em> in the prompt, the LLM can observe the previous tabular reasoning process and select the next operation from the operation pool to complete the reasoning chain step by step.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIBKUfxjF1_KB5gtaj8DRWoqLWQKe_DJXLV6-1sClG1oKutdKujDHyzYgvGlAhQDK235cBoKwNkj7cuA4kLzCt_sltdiyuZSMmEKdEoDS7_XkOFTujyekDI8gJfSLRZkT5yIdGPCVvEVQPoueDgK7dXgyAs04fK3AuwSMurECyNc3ywvzDLAyoNjobg0zk/s1958/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIBKUfxjF1_KB5gtaj8DRWoqLWQKe_DJXLV6-1sClG1oKutdKujDHyzYgvGlAhQDK235cBoKwNkj7cuA4kLzCt_sltdiyuZSMmEKdEoDS7_XkOFTujyekDI8gJfSLRZkT5yIdGPCVvEVQPoueDgK7dXgyAs04fK3AuwSMurECyNc3ywvzDLAyoNjobg0zk/s16000/image1.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Illustration of how Chain-of-Table selects the next operation from the operation pool and generates the arguments for the operation.(a) Chain-of-Table samples the next operation from the operation pool. (b) It takes the selected operation as input and generates its arguments.</td></tr></tbody></table>
<br />

<p>
After the next operation <em>f</em> is determined, in the second stage, we need to generate the arguments. As above, Chain-of-Table considers three components in the prompt as shown in the figure: (1) the question, (2) the selected operation and its required arguments, and (3) the latest intermediate table.  
</p>
<p>
For instance, when the operation <code>f_group_by</code> is selected, it requires a header name as its argument. 
</p>
<p>
The LLM selects a suitable header within the table. Equipped with the selected operation and the generated arguments, Chain-of-Table executes the operation and constructs a new intermediate table for the following reasoning.
</p>
<p>
Chain-of-Table iterates the previous two stages to plan the next operation and generate the required arguments. During this process, we create an operation chain acting as a proxy for the  tabular reasoning steps. These operations generate intermediate tables presenting the results of each step to the LLM. Consequently, the output table contains comprehensive information about the intermediate phases of tabular reasoning. In our final stage, we employ this output table in formulating the final query and prompt the LLM along with the question for the final answer.
</p>
<br /> 

<h2>Experimental setup</h2>


<p>
We use <a href="https://ai.google/discover/palm2/">PaLM 2-S</a>&nbsp;and&nbsp;<a href="https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates">GPT 3.5</a>&nbsp;as the backbone LLMs and conduct the experiments on three public table understanding benchmarks: <a href="https://arxiv.org/abs/1508.00305">WikiTQ</a>, <a href="https://arxiv.org/abs/1909.02164">TabFact</a>, and <a href="https://arxiv.org/abs/2104.00369">FeTaQA</a>. WikiTQ and FeTaQA are datasets for table-based question answering. TabFact is a table-based fact verification benchmark. In this blogpost, we will focus on the results on WikiTQ and TabFact. We compare Chain-of-Table with the generic reasoning methods (e.g., End-to-End QA, Few-Shot QA, and <a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought</a>) and the program-aided methods (e.g., <a href="https://arxiv.org/abs/2204.00498">Text-to-SQL</a>, <a href="https://arxiv.org/abs/2210.02875">Binder</a>, and <a href="https://arxiv.org/abs/2301.13808">Dater</a>). 
</p>
<div style="line-height: 40%;">
    <br />
</div>

<h3>More accurate answers</h3>


<p>
Compared to the generic reasoning methods and program-aided reasoning methods, Chain-of-Table achieves better performance across <a href="https://ai.google/discover/palm2/">PaLM 2</a>&nbsp;and&nbsp;<a href="https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates">GPT 3.5</a>. This is attributed to the dynamically sampled operations and the informative intermediate tables.
</p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglv7DlLdRCDhXh2D8EE8DaOlnyYOBET9usjjD4jQkBMDH_sdWzf72QL6qo8F6wXP6ThhxggSjh-F-z0aah7Qr36ghB3muAAn2k0cjfKV9hBSRaIooRI30qkAbn9nft00DNKG0WjCfVxyNYGD3AciTo282wQDItTceKuDKo03KGTOWvm76HXK2PGgQM8h5o/s1018/ChainOfTableUnderstanding.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglv7DlLdRCDhXh2D8EE8DaOlnyYOBET9usjjD4jQkBMDH_sdWzf72QL6qo8F6wXP6ThhxggSjh-F-z0aah7Qr36ghB3muAAn2k0cjfKV9hBSRaIooRI30qkAbn9nft00DNKG0WjCfVxyNYGD3AciTo282wQDItTceKuDKo03KGTOWvm76HXK2PGgQM8h5o/s16000/ChainOfTableUnderstanding.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><span style="text-align: left;">Understanding results on WikiTQ and TabFact with PaLM 2 and GPT 3.5 compared with various models.</span></td></tr></tbody></table>
<div style="line-height: 40%;">
    <br />
</div>

<h3>Better robustness on harder questions</h3>


<p>
In Chain-of-Table, longer operation chains indicate the higher difficulty and complexity of the questions and their corresponding tables. We categorize the test samples according to their operation lengths in Chain-of-Table. We compare Chain-of-Table with Chain-of-Thought and Dater, as representative generic and program-aided reasoning methods. We illustrate this using results from <a href="https://ai.google/discover/palm2/">PaLM 2</a> on <a href="https://arxiv.org/abs/1508.00305">WikiTQ</a>. 
</p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhONWxPX_gDzAJe0m3HLMjtdzFZ_EF_uCEvpxlMdex5KpSeo2iUzAzyETzzPEl8wbbawjtmw5JbVYXWSEjkwq-198INrSZEzXlLIly40_nr65KOcgQA96rC8Pz744FQaWdTfeIFbeBO6uhPD4NmOeU1dYUzXeoPUlNk2vZ4zd4JVB6TNIaEsHJohvlrSna7/s1548/CoTOpChainLength.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhONWxPX_gDzAJe0m3HLMjtdzFZ_EF_uCEvpxlMdex5KpSeo2iUzAzyETzzPEl8wbbawjtmw5JbVYXWSEjkwq-198INrSZEzXlLIly40_nr65KOcgQA96rC8Pz744FQaWdTfeIFbeBO6uhPD4NmOeU1dYUzXeoPUlNk2vZ4zd4JVB6TNIaEsHJohvlrSna7/s16000/CoTOpChainLength.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Performance of Chain-of-Thought, Dater, and the proposed Chain-of-Table on WikiTQ for questions that require an operation chain of varying lengths. Our proposed atomic operations significantly improve performance over generic and program-aided reasoning counterparts.</td></tr></tbody></table>
<br />

<p>
Notably, Chain-of-Table consistently surpasses both baseline methods across all operation chain lengths, with a significant margin up to 11.6% compared with <a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought</a>, and up to 7.9% compared with <a href="https://arxiv.org/abs/2301.13808">Dater</a>. Moreover, the performance of Chain-of-Table declines gracefully with increasing number of operations compared to other baseline methods, exhibiting only a minimal decrease when the number of operations increases from four to five.
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h3>Better robustness with larger tables</h3>


<p>
We categorize the tables from <a href="https://arxiv.org/abs/1508.00305">WikiTQ</a> into three groups based on token number: small (&lt;2000 tokens), medium (2000 to 4000 tokens) and large (&gt;4000 tokens). We then compare Chain-of-Table with <a href="https://arxiv.org/abs/2301.13808">Dater</a> and <a href="https://arxiv.org/abs/2210.02875">Binder</a>, the two latest and strongest baselines. 
</p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_SgXNNWocZbCKKXAju3cpc4r-cABNL8zsrRmXJYPTiS68R8GM3lkTdxJPXoT3niFVX1bvmL9_QHrozVdl4_vYCamVsaixakttU_-ha88xZhHSbg6M_I4VgG86iynnNwv9ywdcbh5vFtqTKAs2kMmFGZNx85WBM5-RBxI63vvMfau7WbLSkqA7yrOIguY_/s1999/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_SgXNNWocZbCKKXAju3cpc4r-cABNL8zsrRmXJYPTiS68R8GM3lkTdxJPXoT3niFVX1bvmL9_QHrozVdl4_vYCamVsaixakttU_-ha88xZhHSbg6M_I4VgG86iynnNwv9ywdcbh5vFtqTKAs2kMmFGZNx85WBM5-RBxI63vvMfau7WbLSkqA7yrOIguY_/s16000/image1.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><span style="text-align: left;">Performance of Binder, Dater, and the proposed Chain-of-Table on small (&lt;2000 tokens), medium (2000 to 4000 tokens), and large (&gt;4000 tokens) tables from WikiTQ. We observe that the performance decreases with larger input tables while Chain-of-Table diminishes gracefully, achieving significant improvements over competing methods. (As above, underlined text denotes the second-best performance; bold denotes the best performance.)</span></td></tr></tbody></table><br />

<p>
Performance of Binder, Dater, and the proposed Chain-of-Table on small (&lt;2000 tokens), medium (2000 to 4000 tokens), and large (&gt;4000 tokens) tables from WikiTQ. We observe that the performance decreases with larger input tables while Chain-of-Table diminishes gracefully, achieving significant improvements over competing methods. (As above, underlined text denotes the second-best performance; bold denotes the best performance.)
</p>
<p>
As anticipated, the performance decreases with larger input tables, as models are required to reason through longer contexts. Nevertheless, the performance of the proposed Chain-of-Table diminishes gracefully, achieving a significant 10+% improvement over the second best competing method when dealing with large tables. This demonstrates the efficacy of the reasoning chain in handling long tabular inputs.
</p>
<br />

<h2>Conclusion</h2>


<p>
Our proposed Chain-of-Table method enhances the reasoning capability of LLMs by leveraging the tabular structure to express intermediate steps for table-based reasoning. It instructs LLMs to dynamically plan an operation chain according to the input table and its associated question. This evolving table design sheds new light on the understanding of prompting LLMs for table understanding.
</p>
<br /> 

<h2>Acknowledgements</h2>


<p>
<em>This research was conducted by Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, Tomas Pfister. Thanks to Chih-Kuan Yeh and Sergey Ioffe for their valuable feedback.</em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-1106624361649572376</id>
            <title>Health-specific embedding tools for dermatology and pathology</title>
            <link>http://blog.research.google/2024/03/health-specific-embedding-tools-for.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-1106624361649572376</guid>
            <pubDate></pubDate>
            <updated>2024-03-13T09:18:01.747-07:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9zSpggPrlQvV-c0Lc2Sd79B58CwY0kDPJjgQfh-2SR8kiZuXO9A7LWZQ80zCqDNkYHm_IyNSQXF9xUOS-vPg8eJxkPR6HHuFr2VxoaAiAeG4J4ca6Pl8s9Jx1VX3tjQR0oA3I-oS2WujNwYJ2esmlfcyu1PZp7vh5MawdQc8Iu9aLM4fkAhycOXmumoKp/s72-c/Path%20+%20Derm%20hero.jpg"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Dave Steiner, Clinical Research Scientist, Google Health, and Rory Pilgrim, Product Manager, Google Research</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9zSpggPrlQvV-c0Lc2Sd79B58CwY0kDPJjgQfh-2SR8kiZuXO9A7LWZQ80zCqDNkYHm_IyNSQXF9xUOS-vPg8eJxkPR6HHuFr2VxoaAiAeG4J4ca6Pl8s9Jx1VX3tjQR0oA3I-oS2WujNwYJ2esmlfcyu1PZp7vh5MawdQc8Iu9aLM4fkAhycOXmumoKp/s16000/Path%20+%20Derm%20hero.jpg" style="display: none;" />

<p>
There’s a worldwide shortage of access to medical imaging expert interpretation across specialties including <a href="https://www.rsna.org/news/2022/may/Global-Radiologist-Shortage">radiology</a>, <a href="https://www.aad.org/dw/monthly/2021/december/feature-running-dry">dermatology</a> and <a href="https://proscia.com/infographic-the-state-of-the-pathology-workforce-2022/">pathology</a>. Machine learning (ML) technology can help ease this burden by powering tools that enable doctors to interpret these images more accurately and efficiently. However, the development and implementation of such ML tools are often limited by the availability of high-quality data, ML expertise, and computational resources. 
</p>
<a name="more"></a>
<p>
One way to catalyze the use of ML for medical imaging is via domain-specific models that utilize deep learning (DL) to capture the information in medical images as compressed numerical vectors (called embeddings). These embeddings represent a type of pre-learned understanding of the important features in an image. Identifying patterns in the embeddings reduces the amount of data, expertise, and compute needed to train performant models as compared to <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">working with high-dimensional data</a>, such as images, directly. Indeed, these embeddings can be used to perform a variety of downstream tasks within the specialized domain (see animated graphic below). This framework of leveraging pre-learned understanding to solve related tasks is similar to that of a seasoned guitar player quickly learning a new song by ear. Because the guitar player has already built up a foundation of skill and understanding, they can quickly pick up the patterns and groove of a new song. 
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglGgLglQSBBcJqiT_SsxQf9AKGyrenZw28xTiqVP9qljNyD8mhpv-m4kl27u4NLm0FGJShNOuK456JIzdQ269xBx3fBi1u2ke10iE4THphEkD9MCCGrHjhrddtAHJ27g3pyznABW3i_CxTNkONPsH-BOcoFgS4A8tscJsJ42eD5XAHJ3FVzkfmltMzUKkq/s1600/Path%20+%20Derm%20train%20LP.gif" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglGgLglQSBBcJqiT_SsxQf9AKGyrenZw28xTiqVP9qljNyD8mhpv-m4kl27u4NLm0FGJShNOuK456JIzdQ269xBx3fBi1u2ke10iE4THphEkD9MCCGrHjhrddtAHJ27g3pyznABW3i_CxTNkONPsH-BOcoFgS4A8tscJsJ42eD5XAHJ3FVzkfmltMzUKkq/s16000/Path%20+%20Derm%20train%20LP.gif" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Path Foundation is used to convert a small dataset of (image, label) pairs into (embedding, label) pairs. These pairs can then be used to train a task-specific classifier using a linear probe, (i.e., a lightweight linear classifier) as represented in this graphic, or other types of models using the embeddings as input.</td></tr></tbody></table>

<br />



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZeZ25Ea3ZXz8hd6YWMkECnI0jCWnsorTZ0Ob97G-94OZfE3vVtq27pAAmZufyRHfRjUVag-ViN2bIchtZ0eCl5mUIHldWQ8e0lEJAQhYy_Ae3JTCh9Sjc2izTny5I1fo5QxxZTzwvvIKzXNNugSpyYVnUplnm54zRNRKf38EhDU4hEcHYuqqbHdlxQyyz/s1600/Path%20+%20Derm%20-%20evaluate%20LP.gif" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhZeZ25Ea3ZXz8hd6YWMkECnI0jCWnsorTZ0Ob97G-94OZfE3vVtq27pAAmZufyRHfRjUVag-ViN2bIchtZ0eCl5mUIHldWQ8e0lEJAQhYy_Ae3JTCh9Sjc2izTny5I1fo5QxxZTzwvvIKzXNNugSpyYVnUplnm54zRNRKf38EhDU4hEcHYuqqbHdlxQyyz/s16000/Path%20+%20Derm%20-%20evaluate%20LP.gif" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Once the linear probe is trained, it can be used to make predictions on embeddings from new images. These predictions can be compared to ground truth information in order to evaluate the linear probe's performance.</td></tr></tbody></table>



<p>
In order to make this type of embedding model available and drive further development of ML tools in medical imaging, we are excited to release two domain-specific tools for research use: <a href="https://github.com/Google-Health/imaging-research/tree/master/derm-foundation">Derm Foundation</a> and <a href="https://github.com/Google-Health/imaging-research/tree/master/path-foundation">Path Foundation</a>. This follows on the strong response we’ve already received from researchers using the <a href="https://blog.research.google/2022/07/simplified-transfer-learning-for-chest.html">CXR Foundation</a> embedding tool for chest radiographs and represents a portion of our expanding research offerings across multiple medical-specialized modalities. These embedding tools take an image as input and produce a numerical vector (the embedding) that is specialized to the domains of dermatology and digital pathology images, respectively. By running a dataset of chest X-ray, dermatology, or pathology images through the respective embedding tool, researchers can obtain embeddings for their own images, and use these embeddings to quickly develop new models for their applications.
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h2>Path Foundation</h2>


<p>
In “<a href="https://arxiv.org/abs/2310.13259">Domain-specific optimization and diverse evaluation of self-supervised models for histopathology</a>”, we showed that self-supervised learning (SSL) models for pathology images outperform traditional pre-training approaches and enable efficient training of classifiers for downstream tasks. This effort focused on <a href="https://en.wikipedia.org/wiki/H%26E_stain">hematoxylin and eosin</a> (H&amp;E) stained slides, the principal tissue stain in diagnostic pathology that enables pathologists to visualize cellular features under a microscope. The performance of linear classifiers trained using the output of the SSL models matched that of prior DL models trained on orders of magnitude more labeled data. 
</p>

<p>
Due to substantial differences between digital pathology images and “natural image” photos, this work involved several pathology-specific optimizations during model training. One key element is that  <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7522141/">whole-slide images</a> (WSIs) in pathology can be 100,000 pixels across (thousands of times larger than typical smartphone photos) and are analyzed by experts at multiple magnifications (zoom levels). As such, the WSIs are typically broken down into smaller tiles or patches for computer vision and DL applications. The resulting images are information dense with cells or tissue structures distributed throughout the frame instead of having  distinct semantic objects or foreground vs. background variations, thus creating unique challenges for robust SSL and feature extraction. Additionally, physical (e.g., <a href="https://en.wikipedia.org/wiki/Microtome">cutting</a>) and chemical (e.g., <a href="https://en.wikipedia.org/wiki/Fixation_(histology)">fixing</a> and <a href="https://en.wikipedia.org/wiki/Staining">staining</a>) processes used to prepare the samples can influence image appearance dramatically. 
</p>

<p>
Taking these important aspects into consideration, pathology-specific SSL optimizations included helping the model learn <a href="https://arxiv.org/abs/2206.12694">stain-agnostic features</a>, generalizing the model to patches from multiple magnifications, <a href="https://blog.research.google/2020/02/generating-diverse-synthetic-medical.html">augmenting</a> the data to mimic scanning and image post processing, and custom data balancing to improve input heterogeneity for SSL training. These approaches were extensively evaluated using a broad set of benchmark tasks involving 17 different tissue types over 12 different tasks. 
</p>


<p>
Utilizing the vision transformer (<a href="https://github.com/google-research/vision_transformer">ViT-S/16</a>) architecture, Path Foundation was selected as the best performing model from the optimization and evaluation process described above (and illustrated in the figure below). This model thus provides an important balance between performance and model size to enable valuable and scalable use in generating embeddings over the many individual image patches of large pathology WSIs.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhG4jlO0GRCgYA3fe6CteF9PYvm3joBGIBPXakdWWaQ7ztTTBK36dmrtRpK1xoNVub8MTMvmCzkW0wfCCkYUH3fnvKk8hJb79o4vETQq0MhqS1JDBxWgYUwFkjtpnkgx5jBiDOxwovsfgqvpNzVGpz6CY6nTJzJgSgtuE2qDRzIb9O7fbHrhdNU1-IWPSXp/s1999/Path%20+%20Derm%20SSL.jpg" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhG4jlO0GRCgYA3fe6CteF9PYvm3joBGIBPXakdWWaQ7ztTTBK36dmrtRpK1xoNVub8MTMvmCzkW0wfCCkYUH3fnvKk8hJb79o4vETQq0MhqS1JDBxWgYUwFkjtpnkgx5jBiDOxwovsfgqvpNzVGpz6CY6nTJzJgSgtuE2qDRzIb9O7fbHrhdNU1-IWPSXp/s16000/Path%20+%20Derm%20SSL.jpg" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">SSL training with pathology-specific optimizations for Path Foundation.</td></tr></tbody></table>


<p>
The value of domain-specific image representations can also be seen in the figure below, which shows the linear probing performance improvement of Path Foundation (as measured by <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">AUROC</a>) compared to traditional pre-training on natural images (<a href="https://arxiv.org/abs/2104.10972">ImageNet-21k</a>). This includes evaluation for tasks such as <a href="https://jamanetwork.com/journals/jama/fullarticle/2665774">metastatic breast cancer detection in lymph nodes</a>, <a href="https://jamanetwork.com/journals/jamaoncology/fullarticle/2768225">prostate cancer grading</a>, and <a href="https://www.nature.com/articles/s41523-022-00478-y">breast cancer grading</a>, among others. 
</p>





<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtMvTwce8mL0GYA3YTZP0Xc7ub_BYOHIvd9k4FAfnbd-XhpVFU3T9wAl7adebAGVYSWv0RraeV_NHj-0ZiVKQ94wUM9D6GzLSg-FU9ad_L5wN4lksjbWMhN_53FhuY0yGcFvYBU8AgTY7UJKm8z9vz-rH7wkr_m5TOY8gFjWh3YkxHcPMr1wLAkS4hnGkJ/s1999/Path%20+%20Derm%20embeddings.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtMvTwce8mL0GYA3YTZP0Xc7ub_BYOHIvd9k4FAfnbd-XhpVFU3T9wAl7adebAGVYSWv0RraeV_NHj-0ZiVKQ94wUM9D6GzLSg-FU9ad_L5wN4lksjbWMhN_53FhuY0yGcFvYBU8AgTY7UJKm8z9vz-rH7wkr_m5TOY8gFjWh3YkxHcPMr1wLAkS4hnGkJ/s16000/Path%20+%20Derm%20embeddings.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Path Foundation embeddings significantly outperform traditional ImageNet embeddings as evaluated by linear probing across multiple evaluation tasks in histopathology.</td></tr></tbody></table>
<br />

 
<div style="line-height: 40%;">
    <br />
</div>
<h2>Derm Foundation</h2>


<p>
<a href="https://github.com/Google-Health/imaging-research/tree/master/derm-foundation">Derm Foundation</a> is an embedding tool derived from our research in applying DL to <a href="https://blog.research.google/2019/09/using-deep-learning-to-inform.html">interpret images of dermatology conditions</a> and includes our recent work that adds <a href="https://arxiv.org/abs/2402.15566">improvements to generalize better to new datasets</a>. Due to its dermatology-specific pre-training it has a latent understanding of features present in images of skin conditions and can be used to quickly develop models to classify skin conditions. The model underlying the API is a <a href="https://github.com/google-research/big_transfer">BiT ResNet-101x3</a> trained in two stages. The first pre-training stage uses contrastive learning, similar to <a href="https://arxiv.org/abs/2010.00747">ConVIRT</a>, to train on a large number of image-text pairs <a href="https://blog.research.google/2017/07/revisiting-unreasonable-effectiveness.html">from the internet</a>. In the second stage, the image component of this pre-trained model is then fine-tuned for condition classification using clinical datasets, such as those from teledermatology services.
</p>

<p>
Unlike histopathology images, dermatology images more closely resemble the real-world images used to train many of today's computer vision models. However, for specialized dermatology tasks, creating a high-quality model may still require a large dataset. With Derm Foundation, researchers can use their own smaller dataset to retrieve domain-specific embeddings, and use those to build smaller models (e.g., linear classifiers or other small non-linear models) that enable them to validate their research or product ideas. To evaluate this approach, we trained models on a downstream task using teledermatology data. Model training involved varying dataset sizes (12.5%, 25%, 50%, 100%) to compare embedding-based linear classifiers against fine-tuning.
</p>

<p>
The modeling variants considered were:
</p>

<ul>

<li>A linear classifier on frozen embeddings from <a href="https://github.com/google-research/big_transfer">BiT-M</a> (a standard pre-trained image model)

</li><li>Fine-tuned version of BiT-M with an extra dense layer for the downstream task

</li><li>A linear classifier on frozen embeddings from the Derm Foundation API

</li><li>Fine-tuned version of the model underlying the Derm Foundation API with an extra layer for the downstream task
</li>
</ul>
<p>
We found that models built on top of the Derm Foundation embeddings for dermatology-related tasks achieved significantly higher quality than those built solely on embeddings or fine tuned from BiT-M. This advantage was found to be most pronounced for smaller training dataset sizes.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3cFSDBqVdsZm4MaFhMXli6kEJazYEB4xEYPB6ebOPv24HPd57Puw1zfu85raJ0gqfpnwsLW99Wh6aShuoCKZNYLw1PiG7eIqUEm8nMvwTy2qQTNL8ptn7cqBll127x_iEIsDMjznY5pWRIYF89cvBP3uPiVfMTgJS8aQpXiOC3oCO1Xl8CxTc4LXrLnjY/s1240/Path%20+%20Derm%20task%20accuracy.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3cFSDBqVdsZm4MaFhMXli6kEJazYEB4xEYPB6ebOPv24HPd57Puw1zfu85raJ0gqfpnwsLW99Wh6aShuoCKZNYLw1PiG7eIqUEm8nMvwTy2qQTNL8ptn7cqBll127x_iEIsDMjznY5pWRIYF89cvBP3uPiVfMTgJS8aQpXiOC3oCO1Xl8CxTc4LXrLnjY/s16000/Path%20+%20Derm%20task%20accuracy.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">These results demonstrate that the Derm Foundation tooI can serve as a useful starting point to accelerate skin-related modeling tasks. We aim to enable other researchers to build on the underlying features and representations of dermatology that the model has learned. </td></tr></tbody></table>

<p>
However, there are limitations with this analysis. We're still exploring how well these embeddings generalize across task types, patient populations, and image settings. Downstream models built using Derm Foundation still require careful evaluation to understand their expected performance in the intended setting.
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h2>Access Path and Derm Foundation</h2>


<p>
We envision that the Derm Foundation and Path Foundation embedding tools will enable a range of use cases, including efficient development of models for diagnostic tasks, quality assurance and pre-analytical workflow improvements, image indexing and curation, and biomarker discovery and validation. We are releasing both tools to the research community so they can explore the utility of the embeddings for their own dermatology and pathology data.
</p>

<p>
To get access, please sign up to each tool's terms of service using the following Google Forms. 
</p>

<ul>

<li><a href="https://docs.google.com/forms/d/e/1FAIpQLSe5icNBzU_lO2CwjLLIOwbqIcWnJC-m4Sl7MgvI9Lng3QT6Zg/viewform?resourcekey=0-dahJtiVe2CqYkNEdWPcXgw">Derm Foundation Access Form</a>

</li><li><a href="https://docs.google.com/forms/d/1auyo2VkzlzuiAXavZy1AWUyQHAqO7T3BLK-7ofKUvug/edit?resourcekey=0-Z9pRxjDI-kaDEUIiNfMAWQ#question=1168037695&amp;field=173852432">Path Foundation Access Form</a>
</li>
</ul>

<p>
After gaining access to each tool, you can use the API to retrieve embeddings from dermatology images or digital pathology images stored in Google Cloud. Approved users who are just curious to see the model and embeddings in action can use the provided example Colab notebooks to train models using public data for classifying <a href="https://github.com/Google-Health/imaging-research/blob/master/derm-foundation/derm_foundation_demo.ipynb">six common skin conditions</a> or identifying tumors in <a href="https://github.com/Google-Health/imaging-research/blob/master/path-foundation/linear-classifier-demo.ipynb">histopathology patches</a>. We look forward to seeing the range of use-cases these tools can unlock.
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h2>Acknowledgements</h2>


<p>
<em>We would like to thank the many collaborators who helped make this work possible including Yun Liu, Can Kirmizi, Fereshteh Mahvar, Bram Sterling, Arman Tajback, Kenneth Philbrik, Arnav Agharwal, Aurora Cheung, Andrew Sellergren, Boris Babenko, Basil Mustafa, Jan Freyberg, Terry Spitz, Yuan Liu, Pinal Bavishi, Ayush Jain, Amit Talreja, Rajeev Rikhye, Abbi Ward, Jeremy Lai, Faruk Ahmed, Supriya Vijay,Tiam Jaroensri, Jessica Loo, Saurabh Vyawahare, Saloni Agarwal, Ellery Wulczyn, Jonathan Krause, Fayaz Jamil, Tom Small, Annisah Um'rani, Lauren Winer, Sami Lachgar, Yossi Matias, Greg Corrado, and Dale Webster.</em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-1765359719068432739</id>
            <title>Social learning: Collaborative learning with large language models</title>
            <link>http://blog.research.google/2024/03/social-learning-collaborative-learning.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-1765359719068432739</guid>
            <pubDate></pubDate>
            <updated>2024-03-07T10:19:31.177-08:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s72-c/image2.png"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Amirkeivan Mohtashami, Research Intern, and Florian Hartmann, Software Engineer, Google Research</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s1999/image2.png" style="display: none;" />

<p>
Large language models (LLMs) have significantly improved the state of the art for solving tasks specified using natural language, often reaching performance close to that of people. As these models increasingly enable assistive agents, it could be beneficial for them to learn effectively from each other, much like people do in social settings, which would allow LLM-based agents to improve each other’s performance. 
</p>
<a name="more"></a> 

<p>
To discuss the learning processes of humans, Bandura and Walters <a href="https://books.google.ch/books/about/Social_Learning_Theory.html?id=IXvuAAAAMAAJ&amp;redir_esc=y">described</a> the concept of <em>social learning</em> in 1977, outlining different models of observational learning used by people. One common method of learning from others is through a <em>verbal instruction</em> (e.g., from a teacher) that describes how to engage in a particular behavior. Alternatively, learning can happen through a <em>live model</em> by mimicking a live example of the behavior.
</p>
<p>
Given the success of LLMs mimicking human communication, in our paper “<a href="https://arxiv.org/abs/2312.11441">Social Learning: Towards Collaborative Learning with Large Language Models</a>”, we investigate whether LLMs are able to learn from each other using social learning. To this end, we outline a framework for social learning in which LLMs share knowledge with each other in a privacy-aware manner using natural language. We evaluate the effectiveness of our framework on various datasets, and propose quantitative methods that measure privacy in this setting. In contrast to previous approaches to collaborative learning, such as common <a href="https://blog.research.google/2017/04/federated-learning-collaborative.html">federated learning</a> approaches that often rely on gradients, in our framework, agents teach each other purely using natural language.
</p>
<br /> 

<h2>Social learning for LLMs</h2>


<p>
To extend social learning to language models, we consider the scenario where a student LLM should learn to solve a task from multiple teacher entities that already know that task. In our paper, we evaluate the student’s performance on a variety of tasks, such as <a href="https://dl.acm.org/doi/10.1145/2034691.2034742">spam detection</a> in short text messages (SMS), solving <a href="https://arxiv.org/abs/2110.14168">grade school math problems</a>, and <a href="https://arxiv.org/abs/1905.10044">answering questions</a> based on a given text.   
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAndq_MjAVBs4j3lmxEX71nMrCLpAasklndZyE8F7yj3slyafRsNauzW4yRxI_Ncg7Sp5jllAXpItsjA-BOmdB2O1jP3Awu09-DVRHBE_Urf58yzm5tDBBpM-aibZxmgA9O6CySCCRdSMMqG7vj-OU07jHa0OU0YixCxRB0Q3APMQbn8Vz5rEBp70ZNogH/s900/image3.gif" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAndq_MjAVBs4j3lmxEX71nMrCLpAasklndZyE8F7yj3slyafRsNauzW4yRxI_Ncg7Sp5jllAXpItsjA-BOmdB2O1jP3Awu09-DVRHBE_Urf58yzm5tDBBpM-aibZxmgA9O6CySCCRdSMMqG7vj-OU07jHa0OU0YixCxRB0Q3APMQbn8Vz5rEBp70ZNogH/s16000/image3.gif" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">A visualization of the social learning process: A teacher model provides instructions or few-shot examples to a student model without sharing its private data.</td></tr></tbody></table>

<p>
Language models have shown a remarkable capacity to perform tasks given only a handful of examples–a process called <a href="https://arxiv.org/abs/2005.14165">few-shot learning</a>. With this in mind, we provide human-labeled examples of a task that enables the teacher model to teach it to a student. One of the main use cases of social learning arises when these examples cannot be directly shared with the student due, for example, to privacy concerns. 
</p>
<p>
To illustrate this, let’s look at a hypothetical example for a spam detection task. A teacher model is located on device where some users volunteer to mark incoming messages they receive as either “spam” or “not spam”. This is useful data that could help train a student model to differentiate between spam and not spam, but sharing personal messages with other users is a breach of privacy and should be avoided. To prevent this, a social learning process can transfer the knowledge from the teacher model to the student so it learns what spam messages look like without needing to share the user’s personal text messages.
</p>
<p>
We investigate the effectiveness of this social learning approach by analogy with the established human social learning theory that we discussed above. In these experiments, we use <a href="https://blog.google/technology/ai/google-palm-2-ai-large-language-model/">PaLM 2-S</a> models for both the teacher and the student.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s1999/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicN2GYOp9oUj5x0F20i550WuF5KmpD8iRqrdHmJFU_HmkdFY3RBF4mfn_99q8jtEPVm56a4NfjMGFJ79y3rygqjX46h23tlzSDde7iEbp8ytHsPa5-IsNKFFituSoPmtGk666gjyypTvVhhuin8FahZfhWPyDWqF5yWBIQ-Cf_DxQ7vrmTWIkA_tAJtm4v/s16000/image2.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">A systems view of social learning: At training time, multiple teachers teach the student. At inference time, the student is using what it learned from the teachers.</td></tr></tbody></table>
<br />

<h3>Synthetic examples</h3>


<p>
As a counterpart to the live teaching model described for traditional social learning, we propose a learning method where the teachers generate new synthetic examples for the task and share them with the student. This is motivated by the idea that one can create a new example that is sufficiently different from the original one, but is just as educational. Indeed, we observe that our generated examples are sufficiently different from the real ones to preserve privacy while still enabling performance comparable to that achieved using the original examples.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBGMoLyGVpCFO2DkG61pJJwjfje3CZO9V_5YfK3FJlQrbqD8P1RnBt70-G1p0ifTVZ8hnN0upKFdnbZNkPeKpICUiYU0uoqftlq-1bvLXfwlzPFhsCf4uyD5Z4z_ML44YWVf-pjyWEbgsgKGEp_P5F7QzFH3P5TokVfw1QQhD2dSON4dDp3jXqZTHXYZSd/s1456/image5.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiBGMoLyGVpCFO2DkG61pJJwjfje3CZO9V_5YfK3FJlQrbqD8P1RnBt70-G1p0ifTVZ8hnN0upKFdnbZNkPeKpICUiYU0uoqftlq-1bvLXfwlzPFhsCf4uyD5Z4z_ML44YWVf-pjyWEbgsgKGEp_P5F7QzFH3P5TokVfw1QQhD2dSON4dDp3jXqZTHXYZSd/s16000/image5.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The 8 generated examples perform as well as the original data for several tasks (see our&nbsp;<a href="https://arxiv.org/abs/2312.11441">paper</a>).</td></tr></tbody></table>

<p>
We evaluate the efficacy of learning through synthetic examples on our task suite. Especially when the number of examples is high enough, e.g., n = 16, we observe no statistically significant difference between sharing original data and teaching with synthesized data via social learning for the majority of tasks, indicating that the privacy improvement does not have to come at the cost of model quality. 
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQPNMTVzgQW7O3o7Uz0a42vnT7kBhAjqRg5ZL1UrQVs7H5b5-FGdxJFcBmCGHr8sU3WkHsPKVlsQmVnzW-YAop1plz6oxYvTQyxEirorXE2WyGVfFvdOzAw5ydoMh7WUNykMJqasBqCr3C2n_pwBlAFZLO-WBiS-yXm9ExW_NTTIW8zYvfu17cMU8Y3_tp/s1456/image4.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQPNMTVzgQW7O3o7Uz0a42vnT7kBhAjqRg5ZL1UrQVs7H5b5-FGdxJFcBmCGHr8sU3WkHsPKVlsQmVnzW-YAop1plz6oxYvTQyxEirorXE2WyGVfFvdOzAw5ydoMh7WUNykMJqasBqCr3C2n_pwBlAFZLO-WBiS-yXm9ExW_NTTIW8zYvfu17cMU8Y3_tp/s16000/image4.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Generating 16 instead of just 8 examples further reduces the performance gap relative to the original examples.</td></tr></tbody></table>
<br />

<p>
The one exception is spam detection, for which teaching with synthesized data yields lower accuracy. This may be because the training procedure of current models makes them biased to only generate non-spam examples. In the <a href="https://arxiv.org/abs/2312.11441">paper</a>, we additionally look into aggregation methods for selecting good subsets of examples to use.
</p>
<div style="line-height: 40%;">
    <br />
</div>

<h3>Synthetic instruction</h3>


<p>
Given the success of language models in following instructions, the verbal instruction model can also be naturally adapted to language models by having the teachers generate an instruction for the task. Our experiments show that providing such a generated instruction effectively improves performance over zero-shot prompting, reaching accuracies comparable to few-shot prompting with original examples. However, we did find that the teacher model may fail on certain tasks to provide a good instruction, for example due to a complicated formatting requirement of the output. 
</p>
<p>
For <a href="https://arxiv.org/abs/1606.06031">Lambada</a>, <a href="https://arxiv.org/abs/2110.14168">GSM8k</a>, and <a href="https://arxiv.org/abs/2005.14165">Random Insertion</a>, providing synthetic examples performs better than providing generated instructions, whereas in the other tasks generated instruction obtains a higher accuracy. This observation suggests that the choice of the teaching model depends on the task at hand, similar to how the most effective method for teaching people varies by task.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlmIYiQiqu5BGxrgWq6kklbYjnf3cEIE8lYcoIDQBYY54-ZQCTO2bm7IwpElQCD9ZX0Kt9_egKLhFjlmQFh-oJejJuLHHFDC-d_FVS9DzxGQNzEHy8nFL6BTs5D0evWbiDFjhy1p2OZ9u-QixTWFfP73SEWa2L5iax9OGFvwfuGvi5bsr2EzCSEUYONJ5r/s1451/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlmIYiQiqu5BGxrgWq6kklbYjnf3cEIE8lYcoIDQBYY54-ZQCTO2bm7IwpElQCD9ZX0Kt9_egKLhFjlmQFh-oJejJuLHHFDC-d_FVS9DzxGQNzEHy8nFL6BTs5D0evWbiDFjhy1p2OZ9u-QixTWFfP73SEWa2L5iax9OGFvwfuGvi5bsr2EzCSEUYONJ5r/s16000/image1.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Depending on the task, generating instructions can work better than generating new examples.</td></tr></tbody></table>

<br />

<h2>Memorization of the private examples</h2>


<p>
We want teachers in social learning to teach the student without revealing specifics from the original data. To quantify how prone this process is to leaking information, we used <a href="https://research.google/pubs/the-secret-sharer-evaluating-and-testing-unintended-memorization-in-neural-networks/">Secret Sharer</a>, a popular method for quantifying to what extent a model memorizes its training data, and adapted it to the social learning setting. We picked this method since it had previously been <a href="https://blog.research.google/2023/03/distributed-differential-privacy-for.html">used</a> for evaluating memorization in federated learning.
</p>
<p>
To apply the Secret Sharer method to social learning, we design “canary” data points such that we can concretely measure how much the training process memorized them. These data points are included in the datasets used by teachers to generate new examples. After the social learning process completes, we can then measure how much more confident the student is in the secret data points the teacher used, compared to similar ones that were not shared even with the teachers.
</p>
<p>
In our analysis, discussed in detail in the <a href="https://arxiv.org/abs/2312.11441">paper</a>, we use canary examples that include names and codes. Our results show that the student is only slightly more confident in the canaries the teacher used. In contrast, when the original data points are directly shared with the student, the confidence in the included canaries is much higher than in the held-out set. This supports the conclusion that the teacher does indeed use its data to teach without simply copying it over.
</p>
<br /> 

<h2>Conclusion and next steps</h2>


<p>
We introduced a framework for social learning that allows language models with access to private data to transfer knowledge through textual communication while maintaining the privacy of that data. In this framework, we identified sharing examples and sharing instructions as basic models and evaluated them on multiple tasks. Furthermore, we adapted the Secret Sharer metric to our framework, proposing a metric for measuring data leakage.
</p>
<p>
As next steps, we are looking for ways of improving the teaching process, for example by adding feedback loops and iteration. Furthermore, we want to investigate using social learning for modalities other than text.
</p>
<br /> 

<h2>Acknowledgements</h2>


<p>
<em>We would like to acknowledge and thank Matt Sharifi, Sian Gooding, Lukas Zilka, and Blaise Aguera y Arcas, who are all co-authors on the paper. Furthermore, we would like to thank Victor Cărbune, Zachary Garrett, Tautvydas Misiunas, Sofia Neata and John Platt for their feedback, which greatly improved the paper. We’d also like to thank Tom Small for creating the animated figure.</em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-8393293208018757284</id>
            <title>Croissant: a metadata format for ML-ready datasets</title>
            <link>http://blog.research.google/2024/03/croissant-metadata-format-for-ml-ready.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-8393293208018757284</guid>
            <pubDate></pubDate>
            <updated>2024-03-06T14:44:03.387-08:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj09uSTHgWmPgOkD9W1nZZj5i8uW_-pgxm-T1O5PSacF-EKvHIeIwhMr7Rgft7O3A2Rk94GWe8WboO3dUlxrqt1xz9x4I2aMKJxCUtUkR2eukbsIa8xVyAAN_LJJyMABxRqJuktFkyfhoWPDMQK3O-XgbQNJXzAILlWl3su0fd-Q_uZ-8r5r_uAU2P4srnP/s72-c/CroissantHero.png"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Omar Benjelloun, Software Engineer, Google Research, and Peter Mattson, Software Engineer, Google Core ML and President, MLCommons Association</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj09uSTHgWmPgOkD9W1nZZj5i8uW_-pgxm-T1O5PSacF-EKvHIeIwhMr7Rgft7O3A2Rk94GWe8WboO3dUlxrqt1xz9x4I2aMKJxCUtUkR2eukbsIa8xVyAAN_LJJyMABxRqJuktFkyfhoWPDMQK3O-XgbQNJXzAILlWl3su0fd-Q_uZ-8r5r_uAU2P4srnP/s1600/CroissantHero.png" style="display: none;" />



<p>
Machine learning (ML) practitioners looking to reuse existing datasets to train an ML model often spend a lot of time understanding the data, making sense of its organization, or figuring out what subset to use as features. So much time, in fact, that progress in the field of ML is hampered by a fundamental obstacle: the wide variety of data representations. 
</p>
<a name="more"></a>


<p>
ML datasets cover a broad range of content types, from text and structured data to images, audio, and video. Even within datasets that cover the same types of content, every dataset has a unique <em>ad hoc</em> arrangement of files and data formats. This challenge reduces productivity throughout the entire ML development process, from finding the data to training the model. It also impedes development of badly needed tooling for working with datasets. 
</p>
<p>
There are general purpose metadata formats for datasets such as <a href="http://schema.org/Dataset">schema.org</a> and <a href="https://www.w3.org/TR/vocab-dcat-3/">DCAT</a>. However, these formats were designed for data discovery rather than for the specific needs of ML data, such as the ability to extract and combine data from structured and unstructured sources, to include metadata that would enable <a href="https://ai.google/responsibility/responsible-ai-practices/">responsible use</a> of the data, or to describe ML usage characteristics such as defining training, test and validation sets. 
</p>
<p>
Today, we're introducing <a href="https://mlcommons.org/croissant">Croissant</a>, a new metadata format for ML-ready datasets. Croissant was developed collaboratively by a community from industry and academia, as part of the <a href="https://mlcommons.org/">MLCommons</a> effort. The Croissant format doesn't change how the actual data is represented (e.g., image or text file formats) — it provides a standard way to describe and organize it. Croissant builds upon <a href="https://schema.org/">schema.org</a>, the de facto standard for publishing structured data on the Web, which is already used by over 40M datasets. Croissant augments it with comprehensive layers for ML relevant metadata, data resources, data organization, and default ML semantics.
</p>
<p>
In addition, we are announcing support from major tools and repositories: Today, three widely used collections of ML datasets — <a href="http://www.kaggle.com/datasets">Kaggle</a>, <a href="https://huggingface.co/datasets?other=croissant&amp;sort=trending">Hugging Face</a>, and <a href="https://openml.org/search?type=data">OpenML</a> — will begin supporting the Croissant format for the datasets they host; the <a href="http://g.co/datasetsearch">Dataset Search</a> tool lets users search for Croissant datasets across the Web; and popular ML frameworks, including <a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="https://pytorch.org/">PyTorch</a>, and <a href="https://github.com/google/jax">JAX</a>, can load Croissant datasets easily using the <a href="https://www.tensorflow.org/datasets">TensorFlow Datasets</a> (TFDS) package.
</p>


<div style="line-height: 40%;">
    <br />
</div>
<h2>Croissant</h2>


<p>
This 1.0 release of Croissant includes a complete <a href="https://mlcommons.org/croissant/1.0">specification</a> of the format, a set of <a href="https://github.com/mlcommons/croissant/tree/main/datasets">example datasets</a>, an open source <a href="https://github.com/mlcommons/croissant/tree/main/python/mlcroissant">Python library</a> to validate, consume and generate Croissant metadata, and an open source <a href="https://github.com/mlcommons/croissant/tree/main/editor">visual editor</a> to load, inspect and create Croissant dataset descriptions in an intuitive way.
</p>
<p>
Supporting Responsible AI (RAI) was a key goal of the Croissant effort from the start. We are also releasing the first version of the <a href="https://mlcommons.org/croissant/RAI/1.0">Croissant RAI vocabulary</a> extension, which augments Croissant with key properties needed to describe important RAI use cases such as data life cycle management, data labeling, participatory data, ML safety and fairness evaluation, explainability, and compliance.
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h2>Why a shared format for ML data?</h2>
<p>
The majority of ML work is actually data work. The training data is the “code” that determines the behavior of a model. Datasets can vary from a collection of text used to train a large language model (LLM) to a collection of driving scenarios (annotated videos) used to train a car’s collision avoidance system. However, the steps to develop an ML model typically follow the same iterative data-centric process: (1) find or collect data, (2) clean and refine the data, (3) train the model on the data, (4) test the model on more data, (5) discover the model does not work, (6) analyze the data to find out why, (7) repeat until a workable model is achieved. Many steps are made harder by the lack of a common format. This “data development burden” is especially heavy for resource-limited research and early-stage entrepreneurial efforts. 
</p>
<p>
The goal of a format like Croissant is to make this entire process easier. For instance, the metadata can be leveraged by search engines and dataset repositories to make it easier to find the right dataset. The data resources and organization information make it easier to develop tools for cleaning, refining, and analyzing data. This information and the default ML semantics make it possible for ML frameworks to use the data to train and test models with a minimum of code. Together, these improvements substantially reduce the data development burden.
</p>
<p>
Additionally, dataset authors care about the discoverability and ease of use of their datasets. Adopting Croissant improves the value of their datasets, while only requiring a minimal effort, thanks to the available creation tools and support from ML data platforms.
</p>



<div style="line-height: 40%;">
    <br />
</div>
<h2>What can Croissant do today?</h2>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN40ZSjgTFRIVwAwN2OXIn4vQhmshC8VhcKx-ijY-sCQBH9qDkV3nrFz_YapZ0iAD-Svkyxblt6lpJFFHa4JfDqfY6RIL0RnVhtgBlLyh-1DnH8DUz7-TUSdSUIg5V2piqjmQ5Dw9MISeeSBvnMsie8jRrXOeHXfcTGQi0AHIeOYFuHYwDFSyRmBT8BHum/s908/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN40ZSjgTFRIVwAwN2OXIn4vQhmshC8VhcKx-ijY-sCQBH9qDkV3nrFz_YapZ0iAD-Svkyxblt6lpJFFHa4JfDqfY6RIL0RnVhtgBlLyh-1DnH8DUz7-TUSdSUIg5V2piqjmQ5Dw9MISeeSBvnMsie8jRrXOeHXfcTGQi0AHIeOYFuHYwDFSyRmBT8BHum/s16000/image1.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The Croissant ecosystem: Users can Search for Croissant datasets, download them from major repositories, and easily load them into their favorite ML frameworks. They can create, inspect and modify Croissant metadata using the Croissant editor.</td></tr></tbody></table>




<p>
Today, users can find Croissant datasets at:
</p>
<ul>

<li>Google <a href="https://datasetsearch.research.google.com/">Dataset Search</a>, which offers a Croissant filter.

</li><li><a href="https://huggingface.co/datasets?other=croissant&amp;sort=trending">HuggingFace</a>

</li><li><a href="http://kaggle.com/datasets">Kaggle</a>

</li><li><a href="https://openml.org/search?type=data">OpenML</a>
</li>
</ul>
<p>
With a Croissant dataset, it is possible to:
</p>
<ul>

<li>Ingest data easily via <a href="https://www.tensorflow.org/datasets">TensorFlow Datasets</a> for use in popular ML frameworks like <a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="https://pytorch.org/">PyTorch</a>, and <a href="https://github.com/google/jax">JAX</a>.

</li><li>Inspect and modify the metadata using the <a href="https://huggingface.co/spaces/MLCommons/croissant-editor">Croissant editor UI</a> (<a href="https://github.com/mlcommons/croissant/tree/main/editor">github</a>).
</li>
</ul>
<p>
To publish a Croissant dataset, users can:
</p>
<ul>

<li>Use the <a href="https://huggingface.co/spaces/MLCommons/croissant-editor">Croissant editor UI</a> (<a href="https://github.com/mlcommons/croissant/tree/main/editor">github</a>) to generate a large portion of Croissant metadata automatically by analyzing the data the user provides, and to fill important metadata fields such as RAI properties.

</li><li>Publish the Croissant information as part of their dataset Web page to make it discoverable and reusable.

</li><li>Publish their data in one of the repositories that support Croissant, such as Kaggle, HuggingFace and OpenML, and automatically generate Croissant metadata.
</li>
</ul>



<div style="line-height: 40%;">
    <br />
</div>
<h2>Future direction</h2>


<p>
We are excited about Croissant's potential to help ML practitioners, but making this format truly useful requires the support of the community. We encourage dataset creators to consider providing Croissant metadata. We encourage platforms hosting datasets to provide Croissant files for download and embed Croissant metadata in dataset Web pages so that they can be made discoverable by dataset search engines. Tools that help users work with ML datasets, such as labeling or data analysis tools should also consider supporting Croissant datasets. Together, we can reduce the data development burden and enable a richer ecosystem of ML research and development.  
</p>
<p>
We encourage the community to <a href="http://mlcommons.org/croissant">join us</a> in contributing to the effort.
</p>


<div style="line-height: 40%;">
    <br />
</div>
<h2>Acknowledgements</h2>
<p>
<em>Croissant was developed by the <a href="https://datasetsearch.research.google.com/">Dataset Search</a>, <a href="https://www.kaggle.com/">Kaggle</a> and <a href="https://www.tensorflow.org/datasets">TensorFlow Datasets</a> teams from Google, as part of an <a href="http://mlcommons.org">MLCommons</a> community working group, which also includes contributors from these organizations: Bayer, cTuning Foundation, DANS-KNAW, Dotphoton, Harvard, Hugging Face, Kings College London, LIST, Meta, NASA, North Carolina State University, Open Data Institute, Open University of Catalonia, Sage Bionetworks, and TU Eindhoven.</em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-2754526782497247497</id>
            <title>Google at APS 2024</title>
            <link>http://blog.research.google/2024/03/google-at-aps-2024.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-2754526782497247497</guid>
            <pubDate></pubDate>
            <updated>2024-03-05T08:40:45.490-08:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy22Hfq3RN4qRUJcSMUpIau4ueOIcQ219mDvfu4FNJ9kf5PBMUI0x4Uf9BhoIHtnFUhtvE72GCVYixldOZRSeePJfef0P87Pc_djQeGIZOhyxv9nKsQCc57357tr3npWdS5fyWxiGjex4NxMpOIB2JE1Z2qXdLnzLkFM075WstFJD77xVNS2T9hckWZyLf/s72-c/lockup_GoogleResearch_FullColor_Hero.jpg"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Kate Weber and Shannon Leon, Google Research, Quantum AI Team</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjy22Hfq3RN4qRUJcSMUpIau4ueOIcQ219mDvfu4FNJ9kf5PBMUI0x4Uf9BhoIHtnFUhtvE72GCVYixldOZRSeePJfef0P87Pc_djQeGIZOhyxv9nKsQCc57357tr3npWdS5fyWxiGjex4NxMpOIB2JE1Z2qXdLnzLkFM075WstFJD77xVNS2T9hckWZyLf/s1600/lockup_GoogleResearch_FullColor_Hero.jpg" style="display: none;" />

<p>
Today the <a href="https://www.aps.org/meetings/meeting.cfm?name=MAR24">2024 March Meeting</a> of the <a href="https://www.aps.org/">American Physical Society</a> (APS) kicks off in Minneapolis, MN. A premier conference on topics ranging across physics and related fields, APS 2024 brings together researchers, students, and industry professionals to share their discoveries and build partnerships with the goal of realizing fundamental advances in physics-related sciences and technology. 
</p>
<a name="more"></a>
<p>
This year, Google has a strong presence at APS with a booth hosted by the Google <a href="https://quantumai.google/">Quantum AI</a> team, 50+ talks throughout the conference, and participation in conference organizing activities, special sessions and events. Attending APS 2024 in person? Come visit Google’s Quantum AI booth to learn more about the exciting work we’re doing to solve some of the field’s most interesting challenges. <!--Visit the <a href="https://twitter.com/GoogleAI">@GoogleAI</a> X (Twitter) account to find out about Google booth activities (e.g., demos and Q&amp;A sessions).-->
</p>
<p>
You can learn more about the latest cutting edge work we are presenting at the conference along with our schedule of booth events below (Googlers listed in <strong>bold</strong>).
</p>
<div style="line-height: 40%;">
    <br />
</div>

<h2>Organizing Committee</h2>
<div style="margin-left: 20px;">

<p>

    Session Chairs include: <strong>Aaron Szasz</strong>
</p>
</div>
<div style="line-height: 40%;">
    <br />
</div>

<h2>Booth Activities</h2>
<div style="margin-left: 20px;">

<p>

    <em>This schedule is subject to change. Please visit the Google Quantum AI booth for more information.</em>
</p>
<p>

    Crumble: A prototype interactive tool for visualizing QEC circuits
<br />
  Presenter: <strong>Matt McEwen</strong>
<br />
    Tue, Mar 5 | 11:00 AM CST
</p>
<p>

    Qualtran: An open-source library for effective resource estimation of fault tolerant algorithms
<br />

    Presenter: <strong>Tanuj Khattar</strong>
<br />

    Tue, Mar 5 | 2:30 PM CST
</p>
<p>

    Qualtran: An open-source library for effective resource estimation of fault tolerant algorithms
<br />

    Presenter: <strong>Tanuj Khattar</strong>
<br />

    Thu, Mar 7 | 11:00 AM CST
</p>
<p>

    $5M XPRIZE / Google Quantum AI competition to accelerate quantum applications Q&amp;A 
<br />
    Presenter: <strong>Ryan Babbush</strong>
<br />
    Thu, Mar 7 | 11:00 AM CST
</p>

</div>
<div style="line-height: 40%;">
    <br />
</div>


<h2>Talks</h2>


<h3>Monday</h3>

<div style="margin-left: 20px;">

<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/A45.1">Certifying highly-entangled states from few single-qubit measurements</a>
<br />
    Presenter: <strong>Hsin-Yuan Huang</strong>
<br />
    Author: <strong>Hsin-Yuan Huang</strong>
<br />
    <em>Session A45: New Frontiers in Machine Learning Quantum Physics</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/A51.2">Toward high-fidelity analog quantum simulation with superconducting qubits</a>
<br />
    Presenter: <strong>Trond Andersen</strong>
<br />
    Authors: <strong>Trond I Andersen</strong>, <strong>Xiao Mi</strong>, <strong>Amir H Karamlou</strong>, <strong>Nikita Astrakhantsev</strong>, <strong>Andrey Klots</strong>, <strong>Julia Berndtsson</strong>, <strong>Andre Petukhov</strong>, <strong>Dmitry Abanin</strong>, <strong>Lev B Ioffe</strong>, <strong>Yu Chen</strong>, <strong>Vadim Smelyanskiy</strong>, <strong>Pedram Roushan</strong>
<br />
    <em>Session A51: Applications on Noisy Quantum Hardware I</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/B50.6">Measuring circuit errors in context for surface code circuits</a>
<br />
    Presenter: <strong>Dripto M Debroy</strong>
<br />
    Authors: <strong>Dripto M Debroy</strong>, <strong>Jonathan A Gross</strong>, <strong>Élie Genois</strong>, <strong>Zhang Jiang</strong>
<br />
    <em>Session B50: Characterizing Noise with QCVV Techniques</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/B51.6">Quantum computation of stopping power for inertial fusion target design I: Physics overview and the limits of classical algorithms</a>
<br />
    Presenter: Andrew D. Baczewski
<br />
    Authors: <strong>Nicholas C. Rubin</strong>, Dominic W. Berry, Alina Kononov, <strong>Fionn D. Malone</strong>, <strong>Tanuj Khattar</strong>, Alec White, <strong>Joonho Lee</strong>, <strong>Hartmut Neven</strong>, <strong>Ryan Babbush</strong>, Andrew D. Baczewski
<br />
    <em>Session B51: Heterogeneous Design for Quantum Applications</em>
<br />
    <a href="https://arxiv.org/pdf/2308.12352.pdf">Link to Paper</a>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/B51.7">Quantum computation of stopping power for inertial fusion target design II: Physics overview and the limits of classical algorithms</a>
<br />
    Presenter: <strong>Nicholas C. Rubin</strong>
<br />
    Authors: <strong>Nicholas C. Rubin</strong>, Dominic W. Berry, Alina Kononov, <strong>Fionn D. Malone</strong>, <strong>Tanuj Khattar</strong>, Alec White, <strong>Joonho Lee</strong>, <strong>Hartmut Neven</strong>, <strong>Ryan Babbush</strong>, Andrew D. Baczewski
<br />
    <em>Session B51: Heterogeneous Design for Quantum Applications</em>
<br />
    <a href="https://arxiv.org/pdf/2308.12352.pdf">Link to Paper</a>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/B56.4">Calibrating Superconducting Qubits: From NISQ to Fault Tolerance</a>
<br />
    Presenter: <strong>Sabrina S Hong</strong>
<br />
    Author: <strong>Sabrina S Hong</strong>
  <br />
  <em>Session B56: From NISQ to Fault Tolerance</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/B31.9">Measurement and feedforward induced entanglement negativity transition</a>
<br />
    Presenter: <strong>Ramis Movassagh</strong>
<br />
    Authors: Alireza Seif, Yu-Xin Wang,<strong> Ramis Movassagh</strong>, Aashish A. Clerk
<br />
    <em>Session B31: Measurement Induced Criticality in Many-Body Systems</em>
<br />
    <a href="https://arxiv.org/pdf/2310.18305.pdf">Link to Paper</a>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/B52.9">Effective quantum volume, fidelity and computational cost of noisy quantum processing experiments</a>
<br />
    Presenter: <strong>Salvatore Mandra</strong>
<br />
    Authors: <strong>Kostyantyn Kechedzhi</strong>, <strong>Sergei V Isakov</strong>, <strong>Salvatore Mandra</strong>, <strong>Benjamin Villalonga</strong>, <strong>X. Mi</strong>, <strong>Sergio Boixo</strong>, <strong>Vadim Smelyanskiy</strong>
<br />
    <em>Session B52: Quantum Algorithms and Complexity</em>
<br />
    <a href="https://arxiv.org/pdf/2306.15970.pdf">Link to Paper</a>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/D60.4">Accurate thermodynamic tables for solids using Machine Learning Interaction Potentials and Covariance of Atomic Positions</a>
<br />
    Presenter: Mgcini K Phuthi
<br />
    Authors: Mgcini K Phuthi, Yang Huang, Michael Widom, <strong>Ekin D Cubuk</strong>, Venkat Viswanathan
<br />
    <em>Session D60: Machine Learning of Molecules and Materials: Chemical Space and Dynamics</em>
</p>
</div>

<div style="line-height: 40%;">
    <br />
</div>


<h3>Tuesday</h3>
<div style="margin-left: 20px;">

<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/F50.4">IN-Situ Pulse Envelope Characterization Technique (INSPECT)</a>
<br />
    Presenter: <strong>Zhang Jiang</strong>
<br />
    Authors: <strong>Zhang Jiang</strong>, <strong>Jonathan A Gross</strong>, <strong>Élie Genois</strong>
<br />
    <em>Session F50: Advanced Randomized Benchmarking and Gate Calibration</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/F50.11">Characterizing two-qubit gates with dynamical decoupling</a>
<br />
    Presenter: <strong>Jonathan A Gross</strong>
<br />
    Authors: <strong>Jonathan A Gross</strong>, <strong>Zhang Jiang</strong>, <strong>Élie Genois, Dripto M Debroy</strong>, Ze-Pei Cian*, <strong>Wojciech Mruczkiewicz</strong>
<br />
    <em>Session F50: Advanced Randomized Benchmarking and Gate Calibration</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/EE01.2">Statistical physics of regression with quadratic models</a>
<br />
    Presenter: Blake Bordelon
<br />
    Authors: Blake Bordelon, Cengiz Pehlevan, <strong>Yasaman Bahri</strong>
<br />
    <em>Session EE01: V: Statistical and Nonlinear Physics II</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/G51.2">Improved state preparation for first-quantized simulation of electronic structure</a>
<br /> 
  Presenter: <strong>William J Huggins</strong>
<br /> 
  Authors: <strong>William J Huggins</strong>, <strong>Oskar Leimkuhler</strong>, <strong>Torin F Stetina</strong>, <strong>Birgitta Whaley</strong>
<br /> 
  <em>Session G51: Hamiltonian Simulation</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/G30.2">Controlling large superconducting quantum processors</a>
<br />
    Presenter: <strong>Paul V. Klimov</strong>
<br />
    Authors: <strong>Paul V. Klimov</strong>, <strong>Andreas Bengtsson</strong>, <strong>Chris Quintana</strong>, <strong>Alexandre Bourassa</strong>, <strong>Sabrina Hong</strong>, <strong>Andrew Dunsworth</strong>, <strong>Kevin J. Satzinger</strong>, <strong>William P. Livingston</strong>, <strong>Volodymyr Sivak</strong>, <strong>Murphy Y. Niu</strong>, <strong>Trond I. Andersen</strong>, <strong>Yaxing Zhang</strong>, <strong>Desmond Chik</strong>, <strong>Zijun Chen</strong>, <strong>Charles Neill</strong>, <strong>Catherine Erickson</strong>, <strong>Alejandro Grajales Dau</strong>, <strong>Anthony Megrant</strong>, <strong>Pedram Roushan</strong>, <strong>Alexander N. Korotkov</strong>, <strong>Julian Kelly</strong>, <strong>Vadim Smelyanskiy</strong>, <strong>Yu Chen</strong>, <strong>Hartmut Neven</strong>
<br />
    <em>Session G30: Commercial Applications of Quantum Computing</em><br />
    <a href="https://arxiv.org/pdf/2308.02321.pdf">Link to Paper</a>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/G50.5">Gaussian boson sampling: Determining quantum advantage</a>
<br />
    Presenter: Peter D Drummond
<br />
    Authors: Peter D Drummond, Alex Dellios, Ned Goodman, Margaret D Reid, <strong>Ben Villalonga</strong>
<br />
    <em>Session G50: Quantum Characterization, Verification, and Validation II</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/G50.8">Attention to complexity III: learning the complexity of random quantum circuit states</a>
<br />
    Presenter: Hyejin Kim
<br />
    Authors: Hyejin Kim, Yiqing Zhou, Yichen Xu, Chao Wan, Jin Zhou, <strong>Yuri D Lensky</strong>, Jesse Hoke, <strong>Pedram Roushan</strong>, Kilian Q Weinberger, Eun-Ah Kim
<br />
    <em>Session G50: Quantum Characterization, Verification, and Validation II</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/K48.10">Balanced coupling in superconducting circuits</a>
<br />
    Presenter: <strong>Daniel T Sank</strong>
<br />
    Authors: <strong>Daniel T Sank</strong>, <strong>Sergei V Isakov</strong>, <strong>Mostafa Khezri</strong>, <strong>Juan Atalaya</strong>
<br />
    <em>Session K48: Strongly Driven Superconducting Systems</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/K49.12">Resource estimation of Fault Tolerant algorithms using Qᴜᴀʟᴛʀᴀɴ</a>
<br />
    Presenter: <strong>Tanuj Khattar</strong>
<br />
    Author: <strong>Tanuj Khattar</strong>, <b>Matthew Harrigan</b>, <b>Fionn D. Malone</b>, <b>Nour Yosri</b>, <b>Nicholas C. Rubin</b><br />
    <em>Session K49: Algorithms and Implementations on Near-Term Quantum Computers</em>
</p>
</div>

<div style="line-height: 40%;">
    <br />
</div>


<h3>Wednesday</h3>
<div style="margin-left: 20px;">

<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/M24.1">Discovering novel quantum dynamics with superconducting qubits</a>
<br />
    Presenter: <strong>Pedram Roushan</strong>
<br />
    Author: <strong>Pedram Roushan</strong>
<br />
    <em>Session M24: Analog Quantum Simulations Across Platforms</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/M27.7">Deciphering Tumor Heterogeneity in Triple-Negative Breast Cancer: The Crucial Role of Dynamic Cell-Cell and Cell-Matrix Interactions</a>
<br />
    Presenter: Susan Leggett
<br />
    Authors: Susan Leggett, Ian Wong, Celeste Nelson, Molly Brennan, <strong>Mohak Patel</strong>, Christian Franck, Sophia Martinez, Joe Tien, Lena Gamboa, Thomas Valentin, Amanda Khoo, Evelyn K Williams 
<br />
    <em>Session M27: Mechanics of Cells and Tissues II</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/N48.2">Toward implementation of protected charge-parity qubits</a>
<br />
    Presenter: Abigail Shearrow
<br />
    Authors: Abigail Shearrow, Matthew Snyder, Bradley G Cole, Kenneth R Dodge, Yebin Liu, Andrey Klots, <strong>Lev B Ioffe</strong>, Britton L Plourde, Robert McDermott
<br />
    <em>Session N48: Unconventional Superconducting Qubits</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/N48.3">Electronic capacitance in tunnel junctions for protected charge-parity qubits</a>
<br />
    Presenter: Bradley G Cole
<br />
    Authors: Bradley G Cole, Kenneth R Dodge, Yebin Liu, Abigail Shearrow, Matthew Snyder, <strong>Andrey Klots</strong>, <strong>Lev B Ioffe</strong>, Robert McDermott, B.L.T. Plourde
<br />
    <em>Session N48: Unconventional Superconducting Qubits</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/N51.7">Overcoming leakage in quantum error correction</a>
<br />
    Presenter: <strong>Kevin C. Miao</strong>
<br />
    Authors: <strong>Kevin C. Miao</strong>, <strong>Matt McEwen</strong>, <strong>Juan Atalaya</strong>, <strong>Dvir Kafri</strong>, <strong>Leonid P. Pryadko</strong>, <strong>Andreas Bengtsson</strong>, <strong>Alex Opremcak</strong>, <strong>Kevin J. Satzinger</strong>, <strong>Zijun Chen</strong>, <strong>Paul V. Klimov</strong>, <strong>Chris Quintana</strong>, <strong>Rajeev Acharya</strong>, <strong>Kyle Anderson</strong>, <strong>Markus Ansmann</strong>, <strong>Frank Arute</strong>, <strong>Kunal Arya</strong>, <strong>Abraham Asfaw</strong>, <strong>Joseph C. Bardin</strong>, <strong>Alexandre Bourassa</strong>, <strong>Jenna Bovaird</strong>, <strong>Leon Brill</strong>, <strong>Bob B. Buckley</strong>, <strong>David A. Buell</strong>, <strong>Tim Burger</strong>, <strong>Brian Burkett</strong>, <strong>Nicholas Bushnell</strong>, <strong>Juan Campero</strong>, <strong>Ben Chiaro</strong>, <strong>Roberto Collins</strong>, <strong>Paul Conner</strong>, <strong>Alexander L. Crook</strong>, <strong>Ben Curtin</strong>, <strong>Dripto M. Debroy</strong>, <strong>Sean Demura</strong>, <strong>Andrew Dunsworth</strong>, <strong>Catherine Erickson</strong>, <strong>Reza Fatemi</strong>, <strong>Vinicius S. Ferreira</strong>, <strong>Leslie Flores Burgos</strong>, <strong>Ebrahim Forati</strong>, <strong>Austin G. Fowler</strong>, <strong>Brooks Foxen</strong>, <strong>Gonzalo Garcia</strong>, <strong>William Giang</strong>, <strong>Craig Gidney</strong>, <strong>Marissa Giustina</strong>, <strong>Raja Gosula</strong>, <strong>Alejandro Grajales Dau</strong>, <strong>Jonathan A. Gross</strong>, <strong>Michael C. Hamilton</strong>, <strong>Sean D. Harrington</strong>, <strong>Paula Heu</strong>, <strong>Jeremy Hilton</strong>, <strong>Markus R. Hoffmann</strong>, <strong>Sabrina Hong</strong>, <strong>Trent Huang</strong>, <strong>Ashley Huff</strong>, <strong>Justin Iveland</strong>, <strong>Evan Jeffrey</strong>, <strong>Zhang Jiang</strong>, <strong>Cody Jones</strong>, <strong>Julian Kelly</strong>, <strong>Seon Kim</strong>, <strong>Fedor Kostritsa</strong>, <strong>John Mark Kreikebaum</strong>, <strong>David Landhuis</strong>, <strong>Pavel Laptev</strong>, <strong>Lily Laws</strong>, <strong>Kenny Lee</strong>, <strong>Brian J. Lester</strong>, <strong>Alexander T. Lill</strong>, <strong>Wayne Liu</strong>, <strong>Aditya Locharla</strong>, <strong>Erik Lucero</strong>, <strong>Steven Martin</strong>, <strong>Anthony Megrant</strong>, <strong>Xiao Mi</strong>, <strong>Shirin Montazeri</strong>, <strong>Alexis Morvan</strong>, <strong>Ofer Naaman</strong>, <strong>Matthew Neeley</strong>, <strong>Charles Neill</strong>, <strong>Ani Nersisyan</strong>, <strong>Michael Newman</strong>, <strong>Jiun How Ng</strong>, <strong>Anthony Nguyen</strong>, <strong>Murray Nguyen</strong>, <strong>Rebecca Potter</strong>, <strong>Charles Rocque</strong>, <strong>Pedram Roushan</strong>, <strong>Kannan Sankaragomathi</strong>, <strong>Christopher Schuster</strong>, <strong>Michael J. Shearn</strong>, <strong>Aaron Shorter</strong>, <strong>Noah Shutty</strong>, <strong>Vladimir Shvarts</strong>, <strong>Jindra Skruzny</strong>, <strong>W. Clarke Smith</strong>, <strong>George Sterling</strong>, <strong>Marco Szalay</strong>, <strong>Douglas Thor</strong>, <strong>Alfredo Torres</strong>, <strong>Theodore White</strong>, <strong>Bryan W. K. Woo</strong>, <strong>Z. Jamie Yao</strong>, <strong>Ping Yeh</strong>, <strong>Juhwan Yoo</strong>, <strong>Grayson Young</strong>, <strong>Adam Zalcman</strong>, <strong>Ningfeng Zhu</strong>, <strong>Nicholas Zobrist</strong>, <strong>Hartmut Neven</strong>, <strong>Vadim Smelyanskiy</strong>, <strong>Andre Petukhov</strong>, <strong>Alexander N. Korotkov</strong>, <strong>Daniel Sank</strong>, <strong>Yu Chen</strong>
<br />
    <em>Session N51: Quantum Error Correction Code Performance and Implementation I</em>
<br />
    <a href="https://www.nature.com/articles/s41567-023-02226-w">Link to Paper</a>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/N51.11">Modeling the performance of the surface code with non-uniform error distribution: Part 1</a>
<br />
    Presenter: <strong>Yuri D Lensky</strong>
<br />
    Authors: <strong>Yuri D Lensky</strong>, <strong>Volodymyr Sivak</strong>, <strong>Kostyantyn Kechedzhi</strong>, <strong>Igor Aleiner</strong>
<br />
    <em>Session N51: Quantum Error Correction Code Performance and Implementation I</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/N51.12">Modeling the performance of the surface code with non-uniform error distribution: Part 2</a>
<br />
    Presenter: <strong>Volodymyr Sivak</strong>
<br />
    Authors: <strong>Volodymyr Sivak</strong>, <strong>Michael Newman</strong>, <strong>Cody Jones</strong>, <strong>Henry Schurkus</strong>, <strong>Dvir Kafri</strong>, <strong>Yuri D Lensky</strong>, <strong>Paul Klimov</strong>, <strong>Kostyantyn Kechedzhi</strong>, <strong>Vadim Smelyanskiy</strong>
<br />
    <em>Session N51: Quantum Error Correction Code Performance and Implementation I</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/Q51.7">Highly optimized tensor network contractions for the simulation of classically challenging quantum computations</a>
<br />
    Presenter: <strong>Benjamin Villalonga</strong>
<br />
    Author: <strong>Benjamin Villalonga</strong>
<br />
    <em>Session Q51: Co-evolution of Quantum Classical Algorithms</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/Q61.7">Teaching modern quantum computing concepts using hands-on open-source software at all levels</a>
<br />
    Presenter: <strong>Abraham Asfaw</strong>
<br />
    Author: <strong>Abraham Asfaw</strong>
<br />
    <em>Session Q61: Teaching Quantum Information at All Levels II</em>
</p>
</div>

<div style="line-height: 40%;">
    <br />
</div>
  
<h3>Thursday</h3>
<div style="margin-left: 20px;">

<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/S51.1">New circuits and an open source decoder for the color code</a>
<br />
    Presenter: <strong>Craig Gidney</strong>
<br />
    Authors: <strong>Craig Gidney</strong>, <strong>Cody Jones</strong>
<br />
    <em>Session S51: Quantum Error Correction Code Performance and Implementation II</em>
<br />
    <a href="https://arxiv.org/pdf/2312.08813.pdf">Link to Paper</a>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/S18.2">Performing Hartree-Fock many-body physics calculations with large language models</a>
<br />
    Presenter: <strong>Eun-Ah Kim</strong>
<br />
    Authors: <strong>Eun-Ah Kim</strong>, Haining Pan, <strong>Nayantara Mudur</strong>, William Taranto,<strong> Subhashini Venugopalan</strong>, <strong>Yasaman Bahri</strong>, <strong>Michael P Brenner</strong>
<br />
    <em>Session S18: Data Science, AI and Machine Learning in Physics I</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/S51.5">New methods for reducing resource overhead in the surface code</a>
<br />
    Presenter: <strong>Michael Newman</strong>
<br />
    Authors: <strong>Craig M Gidney</strong>, <strong>Michael Newman</strong>, <strong>Peter Brooks</strong>, <strong>Cody Jones</strong>
<br />
    <em>Session S51: Quantum Error Correction Code Performance and Implementation II</em>
<br />
    <a href="https://arxiv.org/pdf/2312.04522.pdf">Link to Paper</a>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/S49.10">Challenges and opportunities for applying quantum computers to drug design</a>
<br />
    Presenter: Raffaele Santagati
<br />
    Authors: Raffaele Santagati, Alan Aspuru-Guzik, <strong>Ryan Babbush</strong>, Matthias Degroote, Leticia Gonzalez, Elica Kyoseva, Nikolaj Moll, Markus Oppel, Robert M. Parrish, <strong>Nicholas C. Rubin</strong>, Michael Streif, Christofer S. Tautermann, Horst Weiss, Nathan Wiebe, Clemens Utschig-Utschig
<br />
    <em>Session S49: Advances in Quantum Algorithms for Near-Term Applications</em>
<br />
    <a href="https://arxiv.org/pdf/2301.04114.pdf">Link to Paper</a>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/T45.1">Dispatches from Google's hunt for super-quadratic quantum advantage in new applications</a>
<br />
    Presenter: <strong>Ryan Babbush</strong>
<br />
    Author: <strong>Ryan Babbush</strong>
<br />
    <em>Session T45: Recent Advances in Quantum Algorithms</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/T48.11">Qubit as a reflectometer</a>
<br />
    Presenter: <strong>Yaxing Zhang</strong>
<br />
    Authors: <strong>Yaxing Zhang</strong>, <strong>Benjamin Chiaro</strong>
<br />
    <em>Session T48: Superconducting Fabrication, Packaging, &amp; Validation</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/W14.3">Random-matrix theory of measurement-induced phase transitions in nonlocal Floquet quantum circuits</a>
<br />
    Presenter: Aleksei Khindanov
<br />
    Authors: Aleksei Khindanov, <strong>Lara Faoro</strong>, <strong>Lev Ioffe</strong>, <strong>Igor Aleiner</strong>
<br />
    <em>Session W14: Measurement-Induced Phase Transitions</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/W58.5">Continuum limit of finite density many-body ground states with MERA</a>
<br />
    Presenter: Subhayan Sahu
<br />
    Authors: Subhayan Sahu, <strong>Guifré Vidal</strong>
<br />
    <em>Session W58: Extreme-Scale Computational Science Discovery in Fluid Dynamics and Related Disciplines II</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/W50.8">Dynamics of magnetization at infinite temperature in a Heisenberg spin chain</a>
<br />
    Presenter: <strong>Eliott Rosenberg</strong>
<br />
    Authors: <strong>Eliott Rosenberg</strong>, <strong>Trond Andersen</strong>, Rhine Samajdar, <strong>Andre Petukhov</strong>, Jesse Hoke*,<strong> Dmitry Abanin</strong>, <strong>Andreas Bengtsson</strong>, <strong>Ilya Drozdov</strong>, <strong>Catherine Erickson</strong>,<strong> Paul Klimov</strong>, <strong>Xiao Mi</strong>, <strong>Alexis Morvan</strong>, <strong>Matthew Neeley</strong>, <strong>Charles Neill</strong>, <strong>Rajeev Acharya</strong>, <strong>Richard Allen</strong>, <strong>Kyle Anderson</strong>, <strong>Markus Ansmann</strong>, <strong>Frank Arute</strong>, <strong>Kunal Arya</strong>, <strong>Abraham Asfaw</strong>, <strong>Juan Atalaya</strong>, <strong>Joseph Bardin</strong>, <strong>A. Bilmes</strong>, <strong>Gina Bortoli</strong>, <strong>Alexandre Bourassa</strong>, <strong>Jenna Bovaird</strong>, <strong>Leon Brill</strong>, <strong>Michael Broughton</strong>, <strong>Bob B. Buckley</strong>, <strong>David Buell</strong>, <strong>Tim Burger</strong>, <strong>Brian Burkett</strong>, <strong>Nicholas Bushnell</strong>, <strong>Juan Campero</strong>, <strong>Hung-Shen Chang</strong>, <strong>Zijun Chen</strong>, <strong>Benjamin Chiaro</strong>, <strong>Desmond Chik</strong>, <strong>Josh Cogan</strong>, <strong>Roberto Collins</strong>, <strong>Paul Conner</strong>, <strong>William Courtney</strong>, <strong>Alexander Crook</strong>, <strong>Ben Curtin</strong>, <strong>Dripto Debroy</strong>, <strong>Alexander Del Toro Barba</strong>, <strong>Sean Demura</strong>, <strong>Agustin Di Paolo</strong>, <strong>Andrew Dunsworth</strong>, <strong>Clint Earle</strong>, <strong>E. Farhi</strong>, <strong>Reza Fatemi</strong>, <strong>Vinicius Ferreira</strong>, <strong>Leslie Flores</strong>, <strong>Ebrahim Forati</strong>, <strong>Austin Fowler</strong>, <strong>Brooks Foxen</strong>, <strong>Gonzalo Garcia</strong>, <strong>Élie Genois</strong>, <strong>William Giang</strong>, <strong>Craig Gidney</strong>, <strong>Dar Gilboa</strong>, <strong>Marissa Giustina</strong>, <strong>Raja Gosula</strong>, <strong>Alejandro Grajales Dau</strong>, <strong>Jonathan Gross</strong>, <strong>Steve Habegger</strong>, <strong>Michael Hamilton</strong>, <strong>Monica Hansen</strong>, <strong>Matthew Harrigan</strong>, <strong>Sean Harrington</strong>, <strong>Paula Heu</strong>, <strong>Gordon Hill</strong>, <strong>Markus Hoffmann</strong>, <strong>Sabrina Hong</strong>, <strong>Trent Huang</strong>, <strong>Ashley Huff</strong>, <strong>William Huggins</strong>, <strong>Lev Ioffe</strong>, <strong>Sergei Isakov</strong>, <strong>Justin Iveland</strong>, <strong>Evan Jeffrey</strong>, <strong>Zhang Jiang</strong>, <strong>Cody Jones</strong>, <strong>Pavol Juhas</strong>, <strong>D. Kafri</strong>, <strong>Tanuj Khattar</strong>, <strong>Mostafa Khezri</strong>, <strong>Mária Kieferová</strong>, <strong>Seon Kim</strong>, <strong>Alexei Kitaev</strong>, <strong>Andrey Klots</strong>, <strong>Alexander Korotkov</strong>, <strong>Fedor Kostritsa</strong>, <strong>John Mark Kreikebaum</strong>, <strong>David Landhuis</strong>, <strong>Pavel Laptev</strong>, <strong>Kim Ming Lau</strong>, <strong>Lily Laws</strong>, <strong>Joonho Lee</strong>, <strong>Kenneth Lee</strong>, <strong>Yuri Lensky</strong>, <strong>Brian Lester</strong>, <strong>Alexander Lill</strong>, <strong>Wayne Liu</strong>, <strong>William P. Livingston</strong>, <strong>A. Locharla</strong>, <strong>Salvatore Mandrà</strong>, <strong>Orion Martin</strong>, <strong>Steven Martin</strong>, <strong>Jarrod McClean</strong>, <strong>Matthew McEwen</strong>, <strong>Seneca Meeks</strong>, <strong>Kevin Miao</strong>, <strong>Amanda Mieszala</strong>, <strong>Shirin Montazeri</strong>, <strong>Ramis Movassagh</strong>, <strong>Wojciech Mruczkiewicz</strong>, <strong>Ani Nersisyan</strong>, <strong>Michael Newman</strong>, <strong>Jiun How Ng</strong>, <strong>Anthony Nguyen</strong>, <strong>Murray Nguyen</strong>, <strong>M. Niu</strong>, <strong>Thomas O'Brien</strong>, <strong>Seun Omonije</strong>, <strong>Alex Opremcak</strong>, <strong>Rebecca Potter</strong>, <strong>Leonid Pryadko</strong>, <strong>Chris Quintana</strong>, <strong>David Rhodes</strong>, <strong>Charles Rocque</strong>, <strong>N. Rubin</strong>, <strong>Negar Saei</strong>, <strong>Daniel Sank</strong>, <strong>Kannan Sankaragomathi</strong>, <strong>Kevin Satzinger</strong>, <strong>Henry Schurkus</strong>, <strong>Christopher Schuster</strong>, <strong>Michael Shearn</strong>, <strong>Aaron Shorter</strong>, <strong>Noah Shutty</strong>, <strong>Vladimir Shvarts</strong>, <strong>Volodymyr Sivak</strong>, <strong>Jindra Skruzny</strong>, <strong>Clarke Smith</strong>, <strong>Rolando Somma</strong>, <strong>George Sterling</strong>, <strong>Doug Strain</strong>, <strong>Marco Szalay</strong>, <strong>Douglas Thor</strong>, <strong>Alfredo Torres</strong>, <strong>Guifre Vidal</strong>, <strong>Benjamin Villalonga</strong>, <strong>Catherine Vollgraff Heidweiller</strong>, <strong>Theodore White</strong>, <strong>Bryan Woo</strong>, <strong>Cheng Xing</strong>, <strong>Jamie Yao</strong>, <strong>Ping Yeh</strong>, <strong>Juhwan Yoo</strong>, <strong>Grayson Young</strong>, <strong>Adam Zalcman</strong>, <strong>Yaxing Zhang</strong>, <strong>Ningfeng Zhu</strong>, <strong>Nicholas Zobrist</strong>, <strong>Hartmut Neven</strong>, <strong>Ryan Babbush</strong>, <strong>Dave Bacon</strong>, <strong>Sergio Boixo</strong>, <strong>Jeremy Hilton</strong>, <strong>Erik Lucero</strong>, <strong>Anthony Megrant</strong>, <strong>Julian Kelly</strong>, <strong>Yu Chen</strong>, <strong>Vadim Smelyanskiy</strong>, Vedika Khemani, Sarang Gopalakrishnan,<strong> Tomaž Prosen</strong>, <strong>Pedram Roushan</strong>
<br />
    <em>Session W50: Quantum Simulation of Many-Body Physics</em>
<br />
    <a href="https://arxiv.org/pdf/2306.09333.pdf">Link to Paper</a>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/W50.13">The fast multipole method on a quantum computer</a>
<br />
    Presenter: Kianna Wan
<br />
    Authors: Kianna Wan, Dominic W Berry, <strong>Ryan Babbush</strong>
<br />
    <em>Session W50: Quantum Simulation of Many-Body Physics</em>
</p>
</div>

<div style="line-height: 40%;">
    <br />
</div>

<h3>Friday</h3>
<div style="margin-left: 20px;">

<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/Y43.1">The quantum computing industry and protecting national security: what tools will work?</a>
<br />
    Presenter: <strong>Kate Weber</strong>
<br />
    Author: <strong>Kate Weber</strong>
  <br />
  <em>Session Y43: Industry, Innovation, and National Security: Finding the Right Balance</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/Y46.3">Novel charging effects in the fluxonium qubit</a>
<br />
    Presenter: <strong>Agustin Di Paolo</strong>
<br />
    Authors: <strong>Agustin Di Paolo</strong>, Kyle Serniak, Andrew J Kerman, <strong>William D Oliver</strong>
<br />
    <em>Session Y46: Fluxonium-Based Superconducting Quibits</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/Z46.3">Microwave Engineering of Parametric Interactions in Superconducting Circuits</a>
<br />
    Presenter: <strong>Ofer Naaman</strong>
<br />
    Author: <strong>Ofer Naaman</strong>
<br />
    <em>Session Z46: Broadband Parametric Amplifiers and Circulators</em>
</p>
<p>

    <a href="https://meetings.aps.org/Meeting/MAR24/Session/Z62.3">Linear spin wave theory of large magnetic unit cells using the Kernel Polynomial Method</a>
<br />
    Presenter: Harry Lane
<br />
    Authors: Harry Lane, Hao Zhang, David A Dahlbom, Sam Quinn, <strong>Rolando D Somma</strong>, Martin P Mourigal, Cristian D Batista, Kipton Barros
<br />
    <em>Session Z62: Cooperative Phenomena, Theory</em>
</p>
</div>

<!--Footnotes-->
<hr width="80%" />
<p>
  <span class="Apple-style-span" style="font-size: x-small;"><sup><b>*</b></sup>Work done while at Google</span></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-1695264277638670894</id>
            <title>VideoPrism: A foundational visual encoder for video understanding</title>
            <link>http://blog.research.google/2024/02/videoprism-foundational-visual-encoder.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-1695264277638670894</guid>
            <pubDate></pubDate>
            <updated>2024-02-23T10:07:08.500-08:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4kKy9Vqp7LE__mAG3METzRxmp6Z5PCH8AyfXzxQ_mNeIgOwYitblprQbb1fOTSUDgNgdmgsm7QwyXgkBcUDs2iIkxGue1n1sxdaomCyAo_eZD1-NFJEbn0fct-gJSNNs_MXHQQCxA79hVbd2CHzg2Nkpw1RnsOQWLq4Y7A7mxXTAFjR9NEE42A6pMOaDi/s72-c/VideoPrismSample.gif"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Long Zhao, Senior Research Scientist, and Ting Liu, Senior Staff Software Engineer, Google Research</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4kKy9Vqp7LE__mAG3METzRxmp6Z5PCH8AyfXzxQ_mNeIgOwYitblprQbb1fOTSUDgNgdmgsm7QwyXgkBcUDs2iIkxGue1n1sxdaomCyAo_eZD1-NFJEbn0fct-gJSNNs_MXHQQCxA79hVbd2CHzg2Nkpw1RnsOQWLq4Y7A7mxXTAFjR9NEE42A6pMOaDi/s450/VideoPrismSample.gif" style="display: none;" />

<p>
An astounding number of videos are available on the Web, covering a variety of content from everyday moments people share to historical moments to scientific observations, each of which contains a unique record of the world. The right tools could help researchers analyze these videos, transforming how we understand the world around us.
</p>
<a name="more"></a> 
<p>
Videos offer dynamic visual content far more rich than static images, capturing movement, changes, and dynamic relationships between entities. Analyzing this complexity, along with the immense diversity of publicly available video data, demands models that go beyond traditional image understanding. Consequently, many of the approaches that best perform on video understanding still rely on specialized models tailor-made for particular tasks. Recently, there has been exciting progress in this area using video foundation models (ViFMs), such as <a href="https://arxiv.org/abs/2109.14084">VideoCLIP</a>, <a href="https://arxiv.org/abs/2212.03191">InternVideo</a>, <a href="https://arxiv.org/abs/2212.04979">VideoCoCa</a>, and <a href="https://arxiv.org/abs/2303.16058">UMT</a>. However, building a ViFM that handles the sheer diversity of video data remains a challenge.
</p>
<p>
With the goal of building a single model for general-purpose video understanding, we introduce “<a href="https://arxiv.org/abs/2402.13217">VideoPrism: A Foundational Visual Encoder for Video Understanding</a>”. VideoPrism is a ViFM designed to handle a wide spectrum of video understanding tasks, including classification, localization, retrieval, captioning, and question answering (QA). We propose innovations in both the pre-training data as well as the modeling strategy. We pre-train VideoPrism on a massive and diverse dataset: 36 million high-quality video-text pairs and 582 million video clips with noisy or machine-generated parallel text. Our pre-training approach is designed for this hybrid data, to learn both from video-text pairs and the videos themselves. VideoPrism is incredibly easy to adapt to new video understanding challenges, and achieves state-of-the-art performance using a single frozen model.
</p><p></p>

<video loop="" width="100%"> <source src="https://github.com/garyzhao/videoprism-blog/raw/main/teaser.mp4" type="video/mp4" /> </video>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td class="tr-caption" style="text-align: center;">VideoPrism is a general-purpose video encoder that enables state-of-the-art results over a wide spectrum of video understanding tasks, including classification, localization, retrieval, captioning, and question answering, by producing video representations from a single frozen model.</td></tr></tbody></table>

<br /> 

<h2>Pre-training data</h2>


<p>
A powerful ViFM needs a very large collection of videos on which to train — similar to other foundation models (FMs), such as those for large language models (LLMs). Ideally, we would want the pre-training data to be a representative sample of all the videos in the world. While naturally most of these videos do not have perfect captions or descriptions, even imperfect text can provide useful information about the semantic content of the video.
</p>
<p>
To give our model the best possible starting point, we put together a massive pre-training corpus consisting of several public and private datasets, including <a href="https://rowanzellers.com/merlot/">YT-Temporal-180M</a>, <a href="https://arxiv.org/abs/2307.06942">InternVid</a>, <a href="https://arxiv.org/abs/2204.00679">VideoCC</a>, <a href="https://arxiv.org/abs/2007.14937">WTS-70M</a>, etc. This includes 36 million carefully selected videos with high-quality captions, along with an additional 582 million clips with varying levels of noisy text (like auto-generated transcripts). To our knowledge, this is the largest and most diverse video training corpus of its kind.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrhfnM1Rg_xbS1b3ZtydWc0M7zOchLpi5qdj65UaR3mOYbV8SQQqKhUhltYwmkPNqrULdeVeE1nU3gnRkjR7pE-yFaiVRC1al-BxZecsO0aojXFzSDhfv45oZoOBeYA93IiNeCGdnUryh4HLc3w7Qr2PX0fy6-4qFMTKBORA_PfHspp7Nr1OW0WnAvn-S9/s1999/image18.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrhfnM1Rg_xbS1b3ZtydWc0M7zOchLpi5qdj65UaR3mOYbV8SQQqKhUhltYwmkPNqrULdeVeE1nU3gnRkjR7pE-yFaiVRC1al-BxZecsO0aojXFzSDhfv45oZoOBeYA93IiNeCGdnUryh4HLc3w7Qr2PX0fy6-4qFMTKBORA_PfHspp7Nr1OW0WnAvn-S9/s16000/image18.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Statistics on the video-text pre-training data. The large variations of the&nbsp;<a href="https://arxiv.org/abs/2104.14806">CLIP similarity scores</a>&nbsp;(the higher, the better) demonstrate the diverse caption quality of our pre-training data, which is a byproduct of the various ways used to harvest the text.</td></tr></tbody></table>

<br /> 

<h2>Two-stage training</h2>


<p>
The VideoPrism model architecture stems from the standard <a href="https://arxiv.org/abs/2010.11929">vision transformer</a> (ViT) with a factorized design that sequentially encodes spatial and temporal information following <a href="https://arxiv.org/abs/2103.15691">ViViT</a>. Our training approach leverages both the high-quality video-text data and the video data with noisy text mentioned above. To start, we use <a href="https://en.wikipedia.org/wiki/Self-supervised_learning#Contrastive_self-supervised_learning">contrastive learning</a> (an approach that minimizes the distance between positive video-text pairs while maximizing the distance between negative video-text pairs) to teach our model to match videos with their own text descriptions, including imperfect ones. This builds a foundation for matching semantic language content to visual content.
</p>
<p>
After video-text contrastive training, we leverage the collection of videos without text descriptions. Here, we build on the <a href="https://arxiv.org/abs/2212.04500">masked video modeling framework</a> to predict masked patches in a video, with a few improvements. We train the model to predict both the video-level global embedding and token-wise embeddings from the first-stage model to effectively leverage the knowledge acquired in that stage. We then randomly shuffle the predicted tokens to prevent the model from learning shortcuts.
</p>
<p>
What is unique about VideoPrism’s setup is that we use two complementary pre-training signals: text descriptions and the visual content within a video. Text descriptions often focus on what things look like, while the video content provides information about movement and visual dynamics. This enables VideoPrism to excel in tasks that demand an understanding of both appearance and motion.
</p>
<br /> 

<h2>Results</h2>


<p>
We conduct extensive evaluation on VideoPrism across four broad categories of video understanding tasks, including video classification and localization, video-text retrieval, video captioning, question answering, and scientific video understanding. VideoPrism achieves state-of-the-art performance on 30 out of 33 video understanding benchmarks — all with minimal adaptation of a single, frozen model.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiUtXCxgEXrgAZJ2B-Mn8L0DP7VkFUfUbI1yLTgGYSbWtn_Q5AjgGRgi3yQ5PMB3fVFlHLzDP4yhlCeGaPpdXr5I1-TNYelYMUBYiXx16qNYTpqKwAqXX7-EFV-4Asn6qYFWOb6_5p71n5Zzxbt-ZeUy5yIj2aieKXl0LnFOqdhKXa56xm4ZoXbccYDz3H/s1999/image20.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="640" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiUtXCxgEXrgAZJ2B-Mn8L0DP7VkFUfUbI1yLTgGYSbWtn_Q5AjgGRgi3yQ5PMB3fVFlHLzDP4yhlCeGaPpdXr5I1-TNYelYMUBYiXx16qNYTpqKwAqXX7-EFV-4Asn6qYFWOb6_5p71n5Zzxbt-ZeUy5yIj2aieKXl0LnFOqdhKXa56xm4ZoXbccYDz3H/w628-h640/image20.png" width="628" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">VideoPrism compared to the previous best-performing FMs.</td></tr></tbody></table>

<div style="line-height: 40%;"><br />
</div> 

<h3>Classification and localization</h3>


<p>
We evaluate VideoPrism on an existing large-scale video understanding benchmark (<a href="https://arxiv.org/abs/2307.03166">VideoGLUE</a>) covering classification and localization tasks. We find that (1) VideoPrism outperforms all of the other state-of-the-art FMs, and (2) no other single model consistently came in second place. This tells us that VideoPrism has learned to effectively pack a variety of video signals into one encoder — from semantics at different granularities to appearance and motion cues — and it works well across a variety of video sources.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNnyg_lnLfwDIsJElqFwLKJleb1quzOR4h7X5jBf_bAnxwo_Em-_XLtWkkyMkyMPcLGdm0F25tLmccw3eK9qt6NN4LrLvfF45Wu8J2ylCqi4hPE-rFOwzmGuV8II6Nq8hileMNrS1lMwCuOHTVNGS04Dsxc7yVztaMCu0sRvuMUHnN4u9IKEvv2g8fRYWo/s1816/image12.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNnyg_lnLfwDIsJElqFwLKJleb1quzOR4h7X5jBf_bAnxwo_Em-_XLtWkkyMkyMPcLGdm0F25tLmccw3eK9qt6NN4LrLvfF45Wu8J2ylCqi4hPE-rFOwzmGuV8II6Nq8hileMNrS1lMwCuOHTVNGS04Dsxc7yVztaMCu0sRvuMUHnN4u9IKEvv2g8fRYWo/s16000/image12.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">VideoPrism outperforms state-of-the-art approaches (including <a href="https://arxiv.org/abs/2103.00020">CLIP</a>, <a href="https://arxiv.org/abs/2104.11178">VATT</a>, <a href="https://arxiv.org/abs/2212.03191">InternVideo</a>, and <a href="https://arxiv.org/abs/2303.16058">UMT</a>) on the <a href="https://arxiv.org/abs/2307.03166">video understanding benchmark</a>. In this plot, we show the absolute score differences compared with the previous best model to highlight the relative improvements of VideoPrism. On <a href="http://vuchallenge.org/charades.html">Charades</a>, <a href="http://activity-net.org/">ActivityNet</a>, <a href="https://research.google.com/ava/">AVA</a>, and <a href="https://research.google.com/ava/">AVA-K</a>, we use <a href="https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision">mean average precision</a> (mAP) as the evaluation metric. On the other datasets, we report top-1 accuracy.</td></tr></tbody></table>
<div style="line-height: 40%;">
    <br />
</div> 

<h3>Combining with LLMs</h3>


<p>
We further explore combining VideoPrism with LLMs to unlock its ability to handle various video-language tasks. In particular, when paired with a text encoder (following <a href="https://arxiv.org/abs/2111.07991">LiT</a>) or a language decoder (such as <a href="https://arxiv.org/abs/2305.10403">PaLM-2</a>), VideoPrism can be utilized for video-text retrieval, video captioning, and video QA tasks. We compare the combined models on a broad and challenging set of vision-language benchmarks. VideoPrism sets the new state of the art on most benchmarks. From the visual results, we find that VideoPrism is capable of understanding complex motions and appearances in videos (e.g., the model can recognize the different colors of spinning objects on the window in the visual examples below). These results demonstrate that VideoPrism is strongly compatible with language models.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd7V86xYM18_i3s0aemjiiYxaJeBiooZrEicQ5VVkLK3QnWTR96hKVsobSO4qRiN0f253JPX4y-T_h17E2Rx80PIVtVed0q499uCv42RzxZ7crkr21nuCR0zwalkSUX9FxIbjWVmlQGb1yx9Y5J8aVT_ROkY4DB1skUkk-bc9FaCc6tc-XLumHk5P65_UR/s1028/VideoPrismResults.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="580" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjd7V86xYM18_i3s0aemjiiYxaJeBiooZrEicQ5VVkLK3QnWTR96hKVsobSO4qRiN0f253JPX4y-T_h17E2Rx80PIVtVed0q499uCv42RzxZ7crkr21nuCR0zwalkSUX9FxIbjWVmlQGb1yx9Y5J8aVT_ROkY4DB1skUkk-bc9FaCc6tc-XLumHk5P65_UR/w640-h580/VideoPrismResults.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">VideoPrism achieves competitive results compared with state-of-the-art approaches (including <a href="https://arxiv.org/abs/2212.04979">VideoCoCa</a>, <a href="https://arxiv.org/abs/2303.16058">UMT</a> and <a href="https://arxiv.org/abs/2204.14198">Flamingo</a>) on multiple video-text retrieval (top) and video captioning and video QA (bottom) benchmarks. We also show the absolute score differences compared with the previous best model to highlight the relative improvements of VideoPrism. We report the Recall@1 on <a href="https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/">MASRVTT</a>, <a href="https://eric-xw.github.io/vatex-website/index.html">VATEX</a>, and <a href="https://cs.stanford.edu/people/ranjaykrishna/densevid/">ActivityNet</a>, <a href="https://arxiv.org/abs/1411.5726">CIDEr score</a> on <a href="https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/">MSRVTT-Cap</a>, <a href="https://eric-xw.github.io/vatex-website/index.html">VATEX-Cap</a>, and <a href="http://youcook2.eecs.umich.edu/">YouCook2</a>, top-1 accuracy on <a href="https://github.com/xudejing/video-question-answering">MSRVTT-QA</a> and <a href="https://github.com/xudejing/video-question-answering">MSVD-QA</a>, and <a href="https://arxiv.org/abs/cmp-lg/9406033">WUPS index</a> on <a href="https://doc-doc.github.io/docs/nextqa.html">NExT-QA</a>.</td></tr></tbody></table>
<br />

<video loop="" width="100%"> <source src="https://github.com/garyzhao/videoprism-blog/raw/main/snowball_water_bottle_drum.mp4" type="video/mp4" /> </video>
<video loop="" width="100%"> <source src="https://github.com/garyzhao/videoprism-blog/raw/main/spin_roller_skating.mp4" type="video/mp4" /> </video>
<video loop="" width="100%"> <source src="https://github.com/garyzhao/videoprism-blog/raw/main/making_ice_cream_ski_lifting.mp4" type="video/mp4" /> </video>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td class="tr-caption" style="text-align: center;">We show qualitative results using VideoPrism with a text encoder for video-text retrieval (first row) and adapted to a language decoder for video QA (second and third row). For video-text retrieval examples, the blue bars indicate the embedding similarities between the videos and the text queries.</td></tr></tbody></table>

<div style="line-height: 40%;">
    <br />
</div> 

<h3>Scientific applications</h3>


<p>
Finally, we test VideoPrism on datasets used by scientists across domains, including fields such as ethology, behavioral neuroscience, and ecology. These datasets typically require domain expertise to annotate, for which we leverage existing scientific datasets open-sourced by the community including <a href="https://data.caltech.edu/records/zrznw-w7386">Fly vs. Fly</a>, <a href="https://data.caltech.edu/records/s0vdx-0k302">CalMS21</a>, <a href="https://shirleymaxx.github.io/ChimpACT/">ChimpACT</a>, and <a href="https://dirtmaxim.github.io/kabr/">KABR</a>. VideoPrism not only performs exceptionally well, but actually surpasses models designed specifically for those tasks. This suggests tools like VideoPrism have the potential to transform how scientists analyze video data across different fields.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3v-C36GWUp8CkaCVqFvaXYKW6-1SvCo99Ogiul-fSTkftyc-t4z5CNUgEWlJkRmzranQrYHldtBvjeJXsqdB4ZbgBkyaZv-_I9QE5U7kus_Z8QWlVqfzX0JfELSDPfGj9V4QqhUMwX_EkyPM-vG7pdYMXN0kj1-s98IZJl3U8CpvqoOHyAsuwXIVt7M4_/s1200/image5.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="397" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3v-C36GWUp8CkaCVqFvaXYKW6-1SvCo99Ogiul-fSTkftyc-t4z5CNUgEWlJkRmzranQrYHldtBvjeJXsqdB4ZbgBkyaZv-_I9QE5U7kus_Z8QWlVqfzX0JfELSDPfGj9V4QqhUMwX_EkyPM-vG7pdYMXN0kj1-s98IZJl3U8CpvqoOHyAsuwXIVt7M4_/w640-h397/image5.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">VideoPrism outperforms the domain experts on various scientific benchmarks. We show the absolute score differences to highlight the relative improvements of VideoPrism. We report mean average precision (mAP) for all datasets, except for KABR which uses class-averaged top-1 accuracy.</td></tr></tbody></table>
<br /> 

<h2>Conclusion</h2>


<p>
With VideoPrism, we introduce a powerful and versatile video encoder that sets a new standard for general-purpose video understanding. Our emphasis on both building a massive and varied pre-training dataset and innovative modeling techniques has been validated through our extensive evaluations. Not only does VideoPrism consistently outperform strong baselines, but its unique ability to generalize positions it well for tackling an array of real-world applications. Because of its potential broad use, we are committed to continuing further responsible research in this space, guided by our <a href="http://ai.google/principles">AI Principles</a>. We hope VideoPrism paves the way for future breakthroughs at the intersection of AI and video analysis, helping to realize the potential of ViFMs across domains such as scientific discovery, education, and healthcare.
</p>
<br /> 

<h2>Acknowledgements</h2>


<p>
<em>This blog post is made on behalf of all the VideoPrism authors: Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel Hornung, Florian Schroff, Ming-Hsuan Yang, David A. Ross, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Ting Liu, and Boqing Gong. We sincerely thank David Hendon for their product management efforts, and Alex Siegman, Ramya Ganeshan, and Victor Gomes for their program and resource management efforts. We also thank Hassan Akbari, Sherry Ben, Yoni Ben-Meshulam, Chun-Te Chu, Sam Clearwater, Yin Cui, Ilya Figotin, Anja Hauth, Sergey Ioffe, Xuhui Jia, Yeqing Li, Lu Jiang, Zu Kim, Dan Kondratyuk, Bill Mark, Arsha Nagrani, Caroline Pantofaru, Sushant Prakash, Cordelia Schmid, Bryan Seybold, Mojtaba Seyedhosseini, Amanda Sadler, Rif A. Saurous, Rachel Stigler, Paul Voigtlaender, Pingmei Xu, Chaochao Yan, Xuan Yang, and Yukun Zhu for the discussions, support, and feedback that greatly contributed to this work. We are grateful to Jay Yagnik, Rahul Sukthankar, and Tomas Izo for their enthusiastic support for this project. Lastly, we thank Tom Small, Jennifer J. Sun, Hao Zhou, Nitesh B. Gundavarapu, Luke Friedman, and Mikhail Sirotenko for the tremendous help with making this blog post.</em>
</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-4343235509909091741</id>
            <title>Advances in private training for production on-device language models</title>
            <link>http://blog.research.google/2024/02/advances-in-private-training-for.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-4343235509909091741</guid>
            <pubDate></pubDate>
            <updated>2024-02-21T12:15:36.694-08:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifnCZ_XGUoUG0hESM0dF5B8Rsoqo4YrT_-uv0hlDM1iTADhtEEyEvBM4hOWT0rxgpVtZKyuFoj2xeXmkeXwGe-XTmvBuwBDJOCqgN8Ba7Wcjh_s1seWUaCRl1xNpNe_6MqxcFFZoAvhfCge5vq9UATjXG_BnTiGdQ6YLLo7AK7ABS3KLFMKmjAtA1gkcBk/s72-c/GBoard%20PrivacyHero.gif"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Zheng Xu, Research Scientist, and Yanxiang Zhang, Software Engineer, Google</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEifnCZ_XGUoUG0hESM0dF5B8Rsoqo4YrT_-uv0hlDM1iTADhtEEyEvBM4hOWT0rxgpVtZKyuFoj2xeXmkeXwGe-XTmvBuwBDJOCqgN8Ba7Wcjh_s1seWUaCRl1xNpNe_6MqxcFFZoAvhfCge5vq9UATjXG_BnTiGdQ6YLLo7AK7ABS3KLFMKmjAtA1gkcBk/s1600/GBoard%20PrivacyHero.gif" style="display: none;" />

<p>
Language models (LMs) trained to predict the next word given input text are the key technology for many applications [<a href="https://blog.google/technology/ai/google-palm-2-ai-large-language-model/">1</a>, <a href="https://blog.google/technology/ai/google-gemini-ai/">2</a>]. In <a href="https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin&amp;hl=en_US&amp;gl=US">Gboard</a>, LMs are used to improve users’ typing experience by supporting features like <a href="https://arxiv.org/abs/1811.03604">next word prediction</a> (NWP), <a href="https://support.google.com/gboard/answer/7068415">Smart Compose</a>,<a href="https://support.google.com/gboard/answer/7068415"> smart completion</a> and <a href="https://support.google.com/gboard/answer/7068415">suggestion</a>, <a href="https://support.google.com/gboard/answer/2811346">slide to type</a><span style="text-decoration: underline;">,</span> and <a href="https://support.google.com/gboard/answer/7068415">proofread</a>. Deploying models on users’ devices rather than enterprise servers has advantages like lower latency and better privacy for model usage. While training on-device models directly from user data effectively improves the utility performance for applications such as NWP and <a href="https://blog.research.google/2021/11/predicting-text-selections-with.html">smart text selection</a>, protecting the privacy of user data for model training is important. 
</p>

<a name="more"></a>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWvaPvikHjeVBb9njeoP2z499_LU0a4VfEgI2kOVxYEoApqgZ49-Ej_TpY6pyoy9HKU2jASzSBsKhdXuOhP-ykpsK_makFmWzVF67BPS3PSpRrCIxC0hYHogBVcDM74AXmjD5hh2mP22tPmXQqEkOak9QXXLyJOCsJB94dv0P-W3IINYyah2O-nF1HLTXE/s1996/image45.gif" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWvaPvikHjeVBb9njeoP2z499_LU0a4VfEgI2kOVxYEoApqgZ49-Ej_TpY6pyoy9HKU2jASzSBsKhdXuOhP-ykpsK_makFmWzVF67BPS3PSpRrCIxC0hYHogBVcDM74AXmjD5hh2mP22tPmXQqEkOak9QXXLyJOCsJB94dv0P-W3IINYyah2O-nF1HLTXE/s16000/image45.gif" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Gboard features powered by on-device language models.</td></tr></tbody></table>

<p>
In this blog we discuss how years of research advances now power the private training of Gboard LMs, since the proof-of-concept development of <a href="https://blog.research.google/2017/04/federated-learning-collaborative.html">federated learning</a> (FL) in 2017 and formal <a href="https://blog.research.google/2022/02/federated-learning-with-formal.html">differential privacy</a> (DP) guarantees in 2022. <a href="https://blog.research.google/2017/04/federated-learning-collaborative.html">FL</a> enables mobile phones to collaboratively learn a model while keeping all the training data on device, and <a href="https://en.wikipedia.org/wiki/Differential_privacy">DP</a> provides a quantifiable measure of data anonymization. Formally, DP is often characterized by (<em>ε</em>, <em>δ</em>) with smaller values representing stronger guarantees. Machine learning (ML) models are considered to have <a href="https://blog.research.google/2023/05/making-ml-models-differentially-private.html">reasonable DP guarantees for ε=10 and strong DP guarantees for ε=1</a> when <em>δ</em> is small. 
</p>
<p>
As of today, all NWP neural network LMs in Gboard are trained with FL with formal DP guarantees, and all future launches of Gboard LMs trained on user data require DP. These 30+ Gboard on-device LMs are launched in 7+ languages and 15+ countries, and satisfy (<em>ɛ</em>, <em>δ</em>)-DP guarantees of small <em>δ</em> of 10<sup>-10</sup> and ɛ between 0.994 and 13.69. To the best of our knowledge, this is the largest known deployment of user-level DP in production at Google or anywhere, and the first time a strong DP guarantee of <em>ɛ</em> &lt; 1 is announced for models trained directly on user data. 
</p>


<div style="line-height: 40%;">
    <br />
</div>
<h2>Privacy principles and practices in Gboard</h2>


<p>
In “<a href="https://arxiv.org/abs/2306.14793">Private Federated Learning in Gboard</a>”, we discussed how different <a href="https://queue.acm.org/detail.cfm?id=3501293">privacy principles</a> are currently reflected in production models, including:
</p>
<ul>

<li><em>Transparency and user control</em>: We provide disclosure of what data is used, what purpose it is used for, how it is processed in various channels, and how Gboard users can easily <a href="https://support.google.com/gboard/answer/12373137">configure</a> the data usage in learning models. 

</li><li><em>Data minimization</em>: FL immediately aggregates only focused updates that improve a specific model. <a href="https://eprint.iacr.org/2017/281.pdf">Secure aggregation</a> (SecAgg) is an encryption method to further guarantee that only aggregated results of the ephemeral updates can be accessed.   

</li><li><em>Data anonymization</em>: DP is applied by the server to prevent models from memorizing the unique information in individual user’s training data. 

</li><li><em>Auditability and verifiability</em>: We have made public the key algorithmic approaches and privacy accounting in open-sourced code (<a href="https://github.com/tensorflow/federated/blob/main/tensorflow_federated/python/aggregators/differential_privacy.py">TFF aggregator</a>, <a href="https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/dp_query/tree_aggregation_query.py">TFP DPQuery</a>, <a href="https://github.com/google-research/federated/blob/master/dp_ftrl/blogpost_supplemental_privacy_accounting.ipynb">DP accounting</a>, and <a href="https://github.com/google/federated-compute">FL system</a>). 
</li>
</ul>


<div style="line-height: 40%;">
    <br />
</div>
<h3>A brief history</h3>


<p>
In recent years, FL has become the default method for training <a href="https://arxiv.org/abs/1811.03604">Gboard on-device LMs</a> from user data. In 2020, a DP mechanism that <a href="https://arxiv.org/abs/1710.06963">clips and adds noise</a> to model updates was used to <a href="https://arxiv.org/abs/2009.10031">prevent memorization</a> for training the Spanish LM in Spain, which satisfies finite DP guarantees (<a href="https://blog.research.google/2023/05/making-ml-models-differentially-private.html">Tier 3</a> described in “<a href="https://arxiv.org/abs/2303.00654">How to DP-fy ML“</a> guide). In 2022, with the help of the <a href="https://arxiv.org/abs/2103.00039">DP-Follow-The-Regularized-Leader (DP-FTRL) algorithm</a>, the Spanish LM became the first production neural network trained directly on user data announced with <a href="https://blog.research.google/2022/02/federated-learning-with-formal.html">a formal DP guarantee of (ε=8.9, δ=10<sup>-10</sup>)-DP</a> (equivalent to the reported <em><a href="https://blog.research.google/2022/02/federated-learning-with-formal.html">ρ=0.81</a></em> <a href="https://arxiv.org/abs/1605.02065">zero-Concentrated-Differential-Privacy</a>), and therefore satisfies <a href="https://blog.research.google/2023/05/making-ml-models-differentially-private.html">reasonable privacy guarantees</a> (<a href="https://blog.research.google/2023/05/making-ml-models-differentially-private.html">Tier 2</a>). 
</p>


<div style="line-height: 40%;">
    <br />
</div>
<h2>Differential privacy by default in federated learning </h2>


<p>
In “<a href="https://arxiv.org/abs/2305.18465">Federated Learning of Gboard Language Models with Differential Privacy</a>”, we announced that all the NWP neural network LMs in Gboard have DP guarantees, and all future launches of Gboard LMs trained on user data require DP guarantees. DP is enabled in FL by applying the following practices:
</p>
<ul>

<li>Pre-train the model with the <a href="https://arxiv.org/abs/2010.11934">multilingual</a> <a href="https://arxiv.org/abs/1910.10683">C4</a> dataset.  

</li><li>Via simulation experiments on public datasets, find a large DP-noise-to-signal ratio that allows for high utility. Increasing the number of clients contributing to one round of model update improves privacy while keeping the noise ratio fixed for good utility, up to the point the DP target is met, or the maximum allowed by the system and the size of the population.

</li><li>Configure the parameter to restrict the frequency each client can contribute (e.g., once every few days) based on computation budget and estimated population in <a href="https://arxiv.org/abs/1902.01046">the FL system</a>. 

</li><li>Run <a href="https://arxiv.org/abs/2103.00039">DP-FTRL</a> training with limits on the magnitude of per-device updates chosen either via <a href="https://github.com/tensorflow/federated/commit/ee9d08368828ea730662e5e2b3a90e103368b6b6">adaptive clipping</a>, or fixed based on experience. 
</li>
</ul>
<p>
SecAgg can be additionally applied by adopting the <a href="https://blog.research.google/2023/03/distributed-differential-privacy-for.html">advances in improving computation and communication for scales and sensitivity</a>.
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht2ZweKyxBRqShB6i41lTpZmfS2gEi2rbNHFGgT-36di1HMxwV6caxFJ2lUXpznxuXYHEb928yfHwueojKlB-gxfKfT4aEv-_2mUlO5zlaWNPceMDGdnOVWp4M8T5qCzMPTuinPOtRy1WmXMtsaSpNpMLvokQKlOnWYFMJF0tXbhmc-dkpI-o7T4FBn8-N/s1600/image3.gif" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht2ZweKyxBRqShB6i41lTpZmfS2gEi2rbNHFGgT-36di1HMxwV6caxFJ2lUXpznxuXYHEb928yfHwueojKlB-gxfKfT4aEv-_2mUlO5zlaWNPceMDGdnOVWp4M8T5qCzMPTuinPOtRy1WmXMtsaSpNpMLvokQKlOnWYFMJF0tXbhmc-dkpI-o7T4FBn8-N/s16000/image3.gif" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Federated learning with differential privacy and (SecAgg).</td></tr></tbody></table>



<div style="line-height: 40%;">
    <br />
</div>
<h3>Reporting DP guarantees</h3>


<p>
The DP guarantees of launched Gboard NWP LMs are visualized in the barplot below. The <em>x</em>-axis shows LMs labeled by language-locale and trained on corresponding populations; the <em>y</em>-axis shows the <em>ε</em> value when <em>δ</em> is fixed to a small value of 10<sup>-10</sup> for <a href="https://www.iacr.org/archive/eurocrypt2006/40040493/40040493.pdf">(ε, δ)-DP</a> (lower is better). The utility of these models are either significantly better than previous non-neural models in production, or comparable with previous LMs without DP, measured based on user-interactions metrics during A/B testing. For example, by applying the best practices, the DP guarantee of the Spanish model in Spain is improved from <em><a href="https://blog.research.google/2022/02/federated-learning-with-formal.html">ε=8.9</a></em> to <em>ε</em>=5.37. SecAgg is additionally used for training the Spanish model in Spain and English model in the US. More details of the DP guarantees are reported in <a href="https://arxiv.org/abs/2305.18465">the appendix </a>following the <a href="https://blog.research.google/2023/05/making-ml-models-differentially-private.html">guidelines outlined</a> in “<a href="https://arxiv.org/abs/2303.00654">How to DP-fy ML</a>”. 
</p>



<div style="line-height: 40%;">
    <br />
</div>
<h2>Towards stronger DP guarantees</h2>


<p>
The <em>ε</em>~10 DP guarantees of many launched LMs are already considered <a href="https://blog.research.google/2023/05/making-ml-models-differentially-private.html">reasonable</a> for ML models in practice, while the journey of DP FL in Gboard continues for improving user typing experience while protecting data privacy. We are excited to announce that, for the first time, production LMs of Portuguese in Brazil and Spanish in Latin America are trained and launched with a DP guarantee of  <em>ε</em> ≤ 1, which satisfies <a href="https://blog.research.google/2023/05/making-ml-models-differentially-private.html">Tier 1 strong privacy guarantees</a>. Specifically, the (<em>ε</em>=0.994, <em>δ</em>=10<sup>-10</sup>)-DP guarantee is achieved by running the advanced <a href="https://arxiv.org/abs/2306.08153">Matrix Factorization DP-FTRL</a> (MF-DP-FTRL) algorithm, with 12,000+ devices participating in every training round of server model update larger than the <a href="https://arxiv.org/abs/2305.18465">common setting of 6500+ devices</a>, and a carefully configured policy to restrict each client to at most participate twice in the total 2000 rounds of training in 14 days in the large Portuguese user population of Brazil. Using a similar setting, the es-US Spanish LM was trained in a large population combining multiple countries in Latin America to achieve (<em>ε</em>=0.994, <em>δ</em>=10<sup>-10</sup>)-DP. The <em>ε</em> ≤ 1 es-US model significantly improved the utility in many countries, and launched in Colombia, Ecuador, Guatemala, Mexico, and Venezuela. For the smaller population in Spain, the DP guarantee of es-ES LM is improved from <em><a href="https://arxiv.org/abs/2305.18465">ε=5.37</a></em> to <em>ε</em>=3.42 by only replacing <a href="https://arxiv.org/abs/2103.00039">DP-FTRL</a> with <a href="https://arxiv.org/abs/2306.08153">MF-DP-FTRL</a> without increasing the number of devices participating every round. More technical details are disclosed in the <a href="https://colab.sandbox.google.com/github/google-research/federated/blob/master/mf_dpftrl_matrices/privacy_accounting.ipynb">colab</a> for privacy accounting. 
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp1yNOAbd8IRoisQDX-OHq-a8PUDH2V1OF7btRsUXI86-tuEXwrR8otAGEqPN8J2HGcpH9aB25s04Nybm_Vn6bpRmfD_AHnHYkGJtld7ockal6mhdRXcsA-M6rf3vM7kzQ5hXfdPbw9hk7bsQU8EV4ul5QAn3Hw4b1yXIKjnokfhrkEF0hNXGt9DbLU3yk/s1999/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp1yNOAbd8IRoisQDX-OHq-a8PUDH2V1OF7btRsUXI86-tuEXwrR8otAGEqPN8J2HGcpH9aB25s04Nybm_Vn6bpRmfD_AHnHYkGJtld7ockal6mhdRXcsA-M6rf3vM7kzQ5hXfdPbw9hk7bsQU8EV4ul5QAn3Hw4b1yXIKjnokfhrkEF0hNXGt9DbLU3yk/s16000/image1.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">DP guarantees for Gboard NWP LMs (the purple bar represents the first es-ES launch of ε=8.9; cyan bars represent privacy improvements for models trained with <a href="https://arxiv.org/abs/2306.08153">MF-DP-FTRL</a>; <a href="https://blog.research.google/2023/05/making-ml-models-differentially-private.html">tiers </a>are from “<a href="https://arxiv.org/abs/2303.00654">How to DP-fy ML</a>“ guide; en-US* and es-ES* are additionally trained with SecAgg).</td></tr></tbody></table>



<div style="line-height: 40%;">
    <br />
</div>
<h2>Discussion and next steps</h2>


<p>
Our experience suggests that DP can be achieved in practice through system algorithm co-design on client participation, and that both privacy and utility can be strong when populations are large <em>and</em> a large number of devices' contributions are aggregated. Privacy-utility-computation trade-offs can be improved by <a href="https://arxiv.org/abs/2305.18465">using public data</a>, the <a href="https://arxiv.org/abs/2306.08153">new MF-DP-FTRL algorithm</a>, <a href="https://github.com/google/differential-privacy">and tightening accounting</a>. With these techniques, a strong DP guarantee of <em>ε</em> ≤ 1 is possible but still challenging. Active research on empirical privacy auditing [<a href="https://arxiv.org/abs/2302.03098">1</a>, <a href="https://arxiv.org/abs/2305.08846">2</a>] suggests that DP models are potentially more private than the worst-case DP guarantees imply. While we keep pushing the frontier of algorithms, which dimension of privacy-utility-computation should be prioritized?
</p>
<p>
We are actively working on all privacy aspects of ML, including extending DP-FTRL to <a href="https://blog.research.google/2023/03/distributed-differential-privacy-for.html">distributed DP</a> and improving <a href="https://arxiv.org/abs/2306.14793">auditability and verifiability</a>. <a href="https://en.wikipedia.org/wiki/Trusted_execution_environment">Trusted Execution Environment</a> opens the opportunity for substantially increasing the model size with verifiable privacy. The recent <a href="https://blog.google/technology/ai/google-gemini-ai/">breakthrough in large LMs</a> (LLMs) motivates us to <a href="https://arxiv.org/abs/2305.12132">rethink</a> the usage of <a href="https://arxiv.org/abs/2212.06470">public</a> information in private training and more future interactions between LLMs, on-device LMs, and Gboard production.  
</p>



<div style="line-height: 40%;">
    <br />
</div>
<h2>Acknowledgments</h2>


<p>
<em>The authors would like to thank Peter Kairouz, Brendan McMahan, and Daniel Ramage for their early feedback on the blog post itself, Shaofeng Li and Tom Small for helping with the animated figures, and the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance. The collaborators below directly contribute to the presented results:</em>
</p>
<p>
<em>Research and algorithm development: Galen Andrew, Stanislav Chiknavaryan, Christopher A. Choquette-Choo, Arun Ganesh, Peter Kairouz, Ryan McKenna, H. Brendan McMahan, Jesse Rosenstock, Timon Van Overveldt, Keith Rush, Shuang Song, Thomas Steinke, Abhradeep Guha Thakurta, Om Thakkar, and Yuanbo Zhang.</em>
</p>
<p>
<em>Infrastructure, production and leadership support: Mingqing Chen, Stefan Dierauf, Billy Dou, Hubert Eichner, Zachary Garrett, Jeremy Gillula, Jianpeng Hou, Hui Li, Xu Liu, Wenzhi Mao, Brett McLarnon, Mengchen Pei, Daniel Ramage, Swaroop Ramaswamy, Haicheng Sun, Andreas Terzis, Yun Wang, Shanshan Wu, Yu Xiao, and Shumin Zhai.</em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-5605933033299261025</id>
            <title>Learning the importance of training data under concept drift</title>
            <link>http://blog.research.google/2024/02/learning-importance-of-training-data.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-5605933033299261025</guid>
            <pubDate></pubDate>
            <updated>2024-02-14T10:32:25.557-08:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUeskw4YD6cFTpLaRnv7OwMsljyeipfAb1riYxIuBsiWd6TBmUXMJ4QoI9tlvUzWX9NzBbEjz3-P2Zl2kuXe5BrVclmqQFrLButoya5phiEELq1azrhsIaGaCz-ov_jXaMsFrGRDE0EjotyRQPOX3xV5MAkVJfKp9xecX4t2CoLBiZ8r2RpZ25Y5KRitFG/s72-c/temporalreweightinghero.png"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Nishant Jain, Pre-doctoral Researcher, and Pradeep Shenoy, Research Scientist, Google Research</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUeskw4YD6cFTpLaRnv7OwMsljyeipfAb1riYxIuBsiWd6TBmUXMJ4QoI9tlvUzWX9NzBbEjz3-P2Zl2kuXe5BrVclmqQFrLButoya5phiEELq1azrhsIaGaCz-ov_jXaMsFrGRDE0EjotyRQPOX3xV5MAkVJfKp9xecX4t2CoLBiZ8r2RpZ25Y5KRitFG/s1600/temporalreweightinghero.png" style="display: none;" />

<p>
The constantly changing nature of the world around us poses a significant challenge for the development of AI models. Often, models are trained on longitudinal data with the hope that the training data used will accurately represent inputs the model may receive in the future. More generally, the default assumption that all training data are equally relevant often breaks in practice. For example, the figure below shows images from the <a href="https://arxiv.org/abs/2201.06289">CLEAR</a> nonstationary learning benchmark, and it illustrates how visual features of objects evolve significantly over a 10 year span (a phenomenon we refer to as <em>slow concept drift</em>), posing a challenge for object categorization models. 
</p>

<a name="more"></a>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBAkCetRQiAPA4cmiXvtwa2SJ0pMwvRYDcuL7rQEDHxEgi9lAyU69bBeeEw-_k182BITn4w2WtdE5QfUwaF-Ny-Dkai-pLeHV23mlgAwrX_0le28l5hba9q9QUO3LeYl2jgkPGkKcLW7dtnGFMiY7PrZbpigSggAiOSrRB8X9eQZGHLE8H7TZoxYy4AD2Q/s1999/image4.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBAkCetRQiAPA4cmiXvtwa2SJ0pMwvRYDcuL7rQEDHxEgi9lAyU69bBeeEw-_k182BITn4w2WtdE5QfUwaF-Ny-Dkai-pLeHV23mlgAwrX_0le28l5hba9q9QUO3LeYl2jgkPGkKcLW7dtnGFMiY7PrZbpigSggAiOSrRB8X9eQZGHLE8H7TZoxYy4AD2Q/s16000/image4.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Sample images from the CLEAR benchmark. (Adapted from Lin et al<a href="https://arxiv.org/abs/2201.06289">.</a>)</td></tr></tbody></table>

<br />


<p>
Alternative approaches, such as <a href="https://en.wikipedia.org/wiki/Online_machine_learning">online</a> and <a href="https://wiki.continualai.org/the-continualai-wiki/introduction-to-continual-learning">continual learning</a>, repeatedly update a model with small amounts of recent data in order to keep it current. This implicitly prioritizes recent data, as the learnings from past data are gradually erased by subsequent updates. However in the real world, different kinds of information lose relevance at different rates, so there are two key issues: 1) By design they focus <em>exclusively</em> on the most recent data and lose any signal from older data that is erased. 2) Contributions from data instances decay <em>uniformly over time</em> irrespective of the contents of the data.
</p>
<p>
In our recent work, “<a href="https://arxiv.org/abs/2212.05908">Instance-Conditional Timescales of Decay for Non-Stationary Learning</a>”, we propose to assign each instance an importance score during training in order to maximize model performance on future data. To accomplish this, we employ an auxiliary model that produces these scores using the training instance as well as its age. This model is jointly learned with the primary model. We address both the above challenges and achieve significant gains over other robust learning methods on a range of benchmark datasets for nonstationary learning. For instance, on a <a href="https://arxiv.org/abs/2108.09020">recent large-scale benchmark</a> for nonstationary learning (~39M photos over a 10 year period), we show up to 15% relative accuracy gains through learned reweighting of training data.
</p>



<div style="line-height: 40%;">
    <br />
</div>
<h2>The challenge of concept drift for supervised learning</h2>


<p>
To gain quantitative insight into slow concept drift, we built classifiers on a <a href="https://arxiv.org/abs/2108.09020">recent photo categorization task</a>, comprising roughly 39M photographs sourced from social media websites over a 10 year period. We compared offline training, which iterated over all the training data multiple times in random order, and continual training, which iterated multiple times over each month of data in sequential (temporal) order. We measured model accuracy both during the training period and during a subsequent period where both models were frozen, i.e., not updated further on new data (shown below). At the end of the training period (left panel, x-axis = 0), both approaches have seen the same amount of data, but show a large performance gap. This is due to <a href="https://www.sciencedirect.com/science/article/abs/pii/S0079742108605368">catastrophic forgetting</a>, a problem in continual learning where a model’s knowledge of data from early on in the training sequence is diminished in an uncontrolled manner. On the other hand, forgetting has its advantages — over the test period (shown on the right), the continual trained model degrades much less rapidly than the offline model because it is less dependent on older data. The decay of both models’ accuracy in the test period is confirmation that the data is indeed evolving over time, and both models become increasingly less relevant.
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizQmgaL3NNsCLWbeTndyOxPikcGKqQIrpDisMVTy-7eAIxamEv3Klpncd5B4SB19yNnPmpySlfAz_hPN8x4zV7o0LPmcLKEnyVJBctKuLF8plITBmDz3BTR2aPHqlKarPPHZHpp0EY0M3HA9l5oV_IOaQS5UzS-uMaNq3Fi1D1qHUYJ6XC-4t0_xS91fnw/s1554/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizQmgaL3NNsCLWbeTndyOxPikcGKqQIrpDisMVTy-7eAIxamEv3Klpncd5B4SB19yNnPmpySlfAz_hPN8x4zV7o0LPmcLKEnyVJBctKuLF8plITBmDz3BTR2aPHqlKarPPHZHpp0EY0M3HA9l5oV_IOaQS5UzS-uMaNq3Fi1D1qHUYJ6XC-4t0_xS91fnw/s16000/image2.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Comparing offline and continually trained models on the photo classification task.</td></tr></tbody></table>


<br />
<div style="line-height: 40%;">
    <br />
</div>
<h2>Time-sensitive reweighting of training data</h2>


<p>
We design a method combining the benefits of offline learning (the flexibility of effectively reusing all available data) and continual learning (the ability to downplay older data) to address slow concept drift. We build upon offline learning, then add careful control over the influence of past data and an optimization objective, both designed to reduce model decay in the future. 
</p>
<p>
Suppose we wish to train a model, <em>M</em>,<em> </em>given some training data collected over time. We propose to also train a helper model that assigns a weight to each point based on its contents and age. This weight scales the contribution from that data point in the training objective for <em>M</em>. The objective of the weights is to improve the performance of <em>M</em> on future data. 
</p>
<p>
In <a href="https://arxiv.org/abs/2212.05908">our work</a>, we describe how the helper model can be <em>meta-learned, </em>i.e., learned alongside <em>M</em> in a manner that helps the learning of the model <em>M</em> itself. A key design choice of the helper model is that we separated out instance- and age-related contributions in a factored manner. Specifically, we set the weight by combining contributions from multiple different fixed timescales of decay, and learn an approximate “assignment” of a given instance to its most suited timescales. We find in our experiments that this form of the helper model outperforms many other alternatives we considered, ranging from unconstrained joint functions to a single timescale of decay (exponential or linear), due to its combination of simplicity and expressivity. Full details may be found in the <a href="https://arxiv.org/abs/2212.05908">paper</a>. 
</p>



<div style="line-height: 40%;">
    <br />
</div>
<h2>Instance weight scoring</h2>


<p>
The top figure below shows that our learned helper model indeed up-weights more modern-looking objects in the <a href="https://arxiv.org/abs/2201.06289">CLEAR object recognition challenge</a>; older-looking objects are correspondingly down-weighted. On closer examination (bottom figure below, gradient-based <a href="https://arxiv.org/abs/1610.02391">feature importance</a> assessment), we see that the helper model focuses on the primary object within the image, as opposed to, e.g., background features that may spuriously be correlated with instance age.
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggQImnpFiW7s3jeT9qoxQOM1kT8vIaHihnlAPusLRx8lJCaxyB7Lzhewn7J6qTiz9-qkWBJzzxLj-uHXhlB94WBMUVRsAgqZVBMBAnDaHGeCe6evZOo6hYgR5oXImP5vO9ZUNcF1q3Bpvau94hM9D71xwOGRqm9c8lJ6ixrB69w_JjneqW5JGcg_u6ZW2J/s1999/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggQImnpFiW7s3jeT9qoxQOM1kT8vIaHihnlAPusLRx8lJCaxyB7Lzhewn7J6qTiz9-qkWBJzzxLj-uHXhlB94WBMUVRsAgqZVBMBAnDaHGeCe6evZOo6hYgR5oXImP5vO9ZUNcF1q3Bpvau94hM9D71xwOGRqm9c8lJ6ixrB69w_JjneqW5JGcg_u6ZW2J/s16000/image1.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Sample images from the <a href="https://arxiv.org/abs/2201.06289">CLEAR</a> benchmark (camera &amp; computer categories) assigned the highest and lowest weights respectively by our helper model.</td></tr></tbody></table>

<br />




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiKCafyxNrHJUkwV3KjoFMJk_v9WSPlzfMyYa-TZCODZdBNCnUOLOZogf9njyGQp_TWzCZ-a6-P5smLhSyeHVFd_jaSBbmS9soN5A5AF6oTq_OWvk-xOWgKaDCIFYz8mhe-GoVEZ56QSsIpKxDduNmCA0ORnf_kgW8ph0uZci8UBCQDBHs0j4Nq5hb5J7e/s1999/image5.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhiKCafyxNrHJUkwV3KjoFMJk_v9WSPlzfMyYa-TZCODZdBNCnUOLOZogf9njyGQp_TWzCZ-a6-P5smLhSyeHVFd_jaSBbmS9soN5A5AF6oTq_OWvk-xOWgKaDCIFYz8mhe-GoVEZ56QSsIpKxDduNmCA0ORnf_kgW8ph0uZci8UBCQDBHs0j4Nq5hb5J7e/s16000/image5.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Feature importance analysis of our helper model on sample images from the <a href="https://arxiv.org/abs/2201.06289">CLEAR</a> benchmark.</td></tr></tbody></table>


<br />

<div style="line-height: 40%;">
    <br />
</div>
<h2>Results</h2>

<div style="line-height: 40%;">
    <br />
</div>
<h3>Gains on large-scale data </h3>


<p>
We first study the large-scale <a href="https://arxiv.org/abs/2108.09020">photo categorization task</a> (PCAT) on the <a href="https://arxiv.org/abs/1503.01817">YFCC100M dataset</a> discussed earlier, using the first five years of data for training and the next five years as test data. Our method (shown in red below) improves substantially over the no-reweighting baseline (black) as well as many other robust learning techniques. Interestingly, our method deliberately trades off accuracy on the distant past (training data unlikely to reoccur in the future) in exchange for marked improvements in the test period. Also, as desired, our method degrades less than other baselines in the test period. 
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLKQZo3e80Ttgw64eHAndZZc6BMXKBNLXAPTQZDP1tsFEQZpGckd6fzqG0aC1x_b5HQmiYlp6AzgbQ3gYRGVcHEZvhnPiDVsl1rxKh3vjVtqXJd20xp5og5yowR2SmyvqNdhhaSuNT5IY_rm_SJanFAsM4jt1Pf_TChyphenhyphenK8y0mNi2Jji1oDWcSiH_7vaC7b/s800/image3.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLKQZo3e80Ttgw64eHAndZZc6BMXKBNLXAPTQZDP1tsFEQZpGckd6fzqG0aC1x_b5HQmiYlp6AzgbQ3gYRGVcHEZvhnPiDVsl1rxKh3vjVtqXJd20xp5og5yowR2SmyvqNdhhaSuNT5IY_rm_SJanFAsM4jt1Pf_TChyphenhyphenK8y0mNi2Jji1oDWcSiH_7vaC7b/s16000/image3.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Comparison of our method and relevant baselines on the PCAT dataset.</td></tr></tbody></table>

<br />


<div style="line-height: 40%;">
    <br />
</div>
<h3>Broad applicability</h3>


<p>
We validated our findings on a wide range of nonstationary learning challenge datasets sourced from the academic literature (see <a href="https://arxiv.org/abs/2108.09020">1</a>, <a href="https://arxiv.org/abs/2201.06289">2</a>, <a href="https://arxiv.org/abs/2211.14238">3</a>, <a href="https://proceedings.mlr.press/v206/awasthi23b/awasthi23b.pdf">4</a> for details) that spans data sources and modalities (photos, satellite images, social media text, medical records, sensor readings, tabular data) and sizes (ranging from 10k to 39M instances). We report significant gains in the test period when compared to the nearest published benchmark method for each dataset (shown below). Note that the previous best-known method may be different for each dataset. These results showcase the broad applicability of our approach.
</p>





<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw95hIflfZ4eiNddWi0-YXONJYbMLT2yHp_Ekzm8v5e1WHpxeT5v7k21EYihoAqrplmlrtM76iiHjuBWtMQDbtj7TvtwIU0eZb44_QSeEe5U4k_z70y_9SsS3If8Y5xkMXKQYI5VzaTafWC7nVv5MgvNw_yL8HA6N7-gUPGGcJI2qtgKTcnqn2oN1ruBt-/s765/image7.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhw95hIflfZ4eiNddWi0-YXONJYbMLT2yHp_Ekzm8v5e1WHpxeT5v7k21EYihoAqrplmlrtM76iiHjuBWtMQDbtj7TvtwIU0eZb44_QSeEe5U4k_z70y_9SsS3If8Y5xkMXKQYI5VzaTafWC7nVv5MgvNw_yL8HA6N7-gUPGGcJI2qtgKTcnqn2oN1ruBt-/s16000/image7.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Performance gain of our method on a variety of tasks studying natural concept drift. Our reported gains are over the previous best-known method for each dataset.</td></tr></tbody></table>

<br />

<div style="line-height: 40%;">
    <br />
</div>
<h3>Extensions to continual learning</h3>


<p>
Finally, we consider an interesting extension of our work. The work above described how offline learning can be extended to handle concept drift using ideas inspired by continual learning. However, sometimes offline learning is infeasible — for example, if the amount of training data available is too large to maintain or process. We adapted our approach to continual learning in a straightforward manner by applying temporal reweighting <em>within the context of </em>each bucket of data being used to sequentially update the model. This proposal still retains some limitations of continual learning, e.g., model updates are performed only on most-recent data, and all optimization decisions (including our reweighting) are only made over that data. Nevertheless, our approach consistently beats regular continual learning as well as a wide range of other continual learning algorithms on the photo categorization benchmark (see below). Since our approach is complementary to the ideas in many baselines compared here, we anticipate even larger gains when combined with them.
</p>





<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtWaqgT_9wt2sckjfrLbQ8LhRK5gL1yTowCf0h2nMnHhBYqfKP7VBwWfbK-5Y5zbYXiKoaF0TKve71FWrHazA4g4SPFD3leb56aZHex95MM_yovx2Y_uO4c5rOA5GzTndUGyBO4HH0gL3jYd8Jk4oPbi4HuSYDuMkKY5kPlqsb0s-re13QKfei2IrMig6S/s800/image6.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtWaqgT_9wt2sckjfrLbQ8LhRK5gL1yTowCf0h2nMnHhBYqfKP7VBwWfbK-5Y5zbYXiKoaF0TKve71FWrHazA4g4SPFD3leb56aZHex95MM_yovx2Y_uO4c5rOA5GzTndUGyBO4HH0gL3jYd8Jk4oPbi4HuSYDuMkKY5kPlqsb0s-re13QKfei2IrMig6S/s16000/image6.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Results of our method adapted to continual learning, compared to the latest baselines.</td></tr></tbody></table>
  
 <br /> 
<div style="line-height: 40%;">
    <br />
</div>  
<h2>Conclusion</h2>


<p>
We addressed the challenge of data drift in learning by combining the strengths of previous approaches — offline learning with its effective reuse of data, and continual learning with its emphasis on more recent data. We hope that our work helps improve model robustness to concept drift in practice, and generates increased interest and new ideas in addressing the ubiquitous problem of slow concept drift.
</p>


<div style="line-height: 40%;">
    <br />
</div>
<h2>Acknowledgements</h2>


<p>
<em>We thank Mike Mozer for many interesting discussions in the early phase of this work, as well as very helpful advice and feedback during its development.</em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-5933365460125094774</id>
            <title>DP-Auditorium: A flexible library for auditing differential privacy</title>
            <link>http://blog.research.google/2024/02/dp-auditorium-flexible-library-for.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-5933365460125094774</guid>
            <pubDate></pubDate>
            <updated>2024-02-13T14:11:49.258-08:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNVpxjk-jj1rIYQ8AM3A-Syqxd3d8L8-wIy8NWwyobCXmTRK7mY9h94aJYgFCiC0gnehVFFoM8-in8HsOZjfhoNce03nbsrN5fxY07wADV6ULPC0POGmCc-8eL3OqA9KrDyzQxN38JKvh6xCmLV6FZ1g0UfaXtKORhtTy0WuJexlPqV6P2c9rPdg_W_5zP/s72-c/hero.jpg"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Mónica Ribero Díaz, Research Scientist, Google Research</span>


<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNVpxjk-jj1rIYQ8AM3A-Syqxd3d8L8-wIy8NWwyobCXmTRK7mY9h94aJYgFCiC0gnehVFFoM8-in8HsOZjfhoNce03nbsrN5fxY07wADV6ULPC0POGmCc-8eL3OqA9KrDyzQxN38JKvh6xCmLV6FZ1g0UfaXtKORhtTy0WuJexlPqV6P2c9rPdg_W_5zP/s320/hero.jpg" style="display: none;" />

<p>
<a href="https://en.wikipedia.org/wiki/Differential_privacy">Differential privacy</a> (DP) is a property of randomized mechanisms that limit the influence of any individual user’s information while processing and analyzing data. DP offers a robust solution to address growing concerns about data protection, enabling technologies <a href="https://blog.research.google/2022/02/federated-learning-with-formal.html">across</a> <a href="https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf">industries</a> and government applications (e.g., <a href="https://www.census.gov/programs-surveys/decennial-census/decade/2020/planning-management/process/disclosure-avoidance/differential-privacy.html">the US census</a>) without compromising individual user identities.  As its adoption increases, it’s important to identify the potential risks of developing mechanisms with faulty implementations. Researchers have recently found errors in the mathematical proofs of private mechanisms, and their implementations. For example, <a href="https://arxiv.org/pdf/1603.01699.pdf">researchers compared</a> six sparse vector technique (SVT) variations and found that only two of the six actually met the asserted privacy guarantee. Even when mathematical proofs are correct, the code implementing the mechanism is vulnerable to human error.
</p>
<a name="more"></a>
<p>
However, practical and efficient DP auditing is challenging primarily due to the inherent randomness of the mechanisms and the probabilistic nature of the tested guarantees. In addition, a range of guarantee types exist, (e.g., <a href="https://dl.acm.org/doi/10.1007/11681878_14">pure DP</a>, <a href="https://link.springer.com/chapter/10.1007/11761679_29">approximate DP</a>, <a href="https://arxiv.org/abs/1702.07476">Rényi DP</a>, and <a href="https://arxiv.org/pdf/1603.01887.pdf">concentrated DP</a>), and this diversity contributes to the complexity of formulating the auditing problem. Further, debugging mathematical proofs and code bases is an intractable task given the volume of proposed mechanisms. While <em>ad hoc</em> testing techniques exist under specific assumptions of mechanisms, few efforts have been made to develop an extensible tool for testing DP mechanisms. 
</p>

<p>
To that end, in “<a href="https://arxiv.org/abs/2307.05608">DP-Auditorium: A Large Scale Library for Auditing Differential Privacy</a>”, we introduce an <a href="https://github.com/google/differential-privacy/tree/main/python/dp_auditorium">open source library</a> for auditing DP guarantees with only black-box access to a mechanism (i.e., without any knowledge of the mechanism’s internal properties). DP-Auditorium is implemented in Python and provides a flexible interface that allows contributions to continuously improve its testing capabilities. We also introduce new testing algorithms that perform divergence optimization over function spaces for Rényi DP, pure DP, and approximate DP. We demonstrate that DP-Auditorium can efficiently identify DP guarantee violations, and suggest which tests are most suitable for detecting particular bugs under various privacy guarantees.
</p>


<div style="line-height: 40%;">
    <br />
</div>
<h2>DP guarantees</h2>


<p>
The output of a DP mechanism is a sample drawn from a probability distribution (<em>M</em> (<em>D</em>)) that satisfies a mathematical property ensuring the privacy of user data. A DP guarantee is thus tightly related to properties between pairs of probability distributions. A mechanism is differentially private if the probability distributions determined by <i>M</i> on dataset <em>D</em> and a neighboring dataset <em>D’</em>, which differ by only one record, are <em><a href="https://en.wikipedia.org/wiki/Computational_indistinguishability">indistinguishable</a></em> under a given divergence metric. 
</p>

<p>
For example, the classical <a href="https://software.imdea.org/~federico/pubs/2013.ICALP.pdf">approximate DP</a> definition states that a mechanism is approximately DP with parameters (<em>ε</em>, <em>δ</em>) if the <a href="https://arxiv.org/pdf/1508.00335.pdf">hockey-stick divergence</a> of order <em>e<sup>ε</sup></em>, between <em>M</em>(<em>D) </em>and <em>M</em>(<em>D’</em>), is at most <em>δ</em>. Pure DP is a special instance of approximate DP where <em>δ = 0</em>. Finally, a mechanism is considered <a href="https://arxiv.org/abs/1702.07476">Rényi DP</a> with parameters (<em>𝛼</em>, <em>ε)</em> if the <a href="https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy">Rényi divergence</a> of order <em>𝛼</em>, is at most <em>ε</em> (where <em>ε</em> is a small positive value). In these three definitions, <em>ε </em>is not interchangeable but intuitively conveys the same concept; larger values of <em>ε</em> imply larger divergences between the two distributions or less privacy, since the two distributions are easier to distinguish.  
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h2>DP-Auditorium</h2>


<p>
DP-Auditorium comprises two main components: property testers and dataset finders. Property testers take samples from a mechanism evaluated on specific datasets as input and aim to identify privacy guarantee violations in the provided datasets. Dataset finders suggest datasets where the privacy guarantee may fail. By combining both components, DP-Auditorium enables (1) automated testing of diverse mechanisms and privacy definitions and, (2) detection of bugs in privacy-preserving mechanisms. We implement various private and non-private mechanisms, including simple mechanisms that compute the mean of records and more complex mechanisms, such as different SVT and  <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">gradient descent</a> mechanism variants. 
</p>

<p>
<strong>Property testers</strong> determine if evidence exists to reject the hypothesis that a given divergence between two probability distributions, <em>P</em> and <em>Q</em>, is bounded by a prespecified budget determined by the DP guarantee being tested. They compute a lower bound from samples from <em>P</em> and <em>Q,</em> rejecting the property if the lower bound value exceeds the expected divergence. No guarantees are provided if the result is indeed bounded. To test for a range of privacy guarantees, DP-Auditorium introduces three novel testers: (1) HockeyStickPropertyTester, (2) RényiPropertyTester, and (3) MMDPropertyTester. Unlike other approaches, these testers don’t depend on explicit histogram approximations of the tested distributions. They rely on variational representations of the hockey-stick divergence, Rényi divergence, and <a href="https://jmlr.csail.mit.edu/papers/v13/gretton12a.html">maximum mean discrepancy</a> (MMD) that enable the estimation of divergences through optimization over function spaces. As a baseline, we implement <a href="https://arxiv.org/abs/1806.06427">HistogramPropertyTester</a>, a commonly used approximate DP tester. While our three testers follow a similar approach, for brevity, we focus on the HockeyStickPropertyTester in this post.
</p>

<p>
Given two neighboring datasets, <em>D</em> and <em>D’</em>, the HockeyStickPropertyTester finds a lower bound,<i><span>^</span>δ</i> &nbsp;for the hockey-stick divergence between <em>M</em>(<em>D) </em>and <em>M</em>(<em>D’</em>) that holds with high probability. Hockey-stick divergence enforces that the two distributions <em>M</em>(<em>D) </em>and <em>M</em>(<em>D’</em>) are close under an approximate DP guarantee. Therefore, if a privacy guarantee claims that the hockey-stick divergence is at most <em>δ</em>, and<i><span>^</span>δ</i>&nbsp; &gt; <em>δ</em>, then with high probability the divergence is higher than what was promised on <em>D</em> and <em>D’</em> and the mechanism cannot satisfy the given approximate DP guarantee. The lower bound<i><span>^</span>δ</i>&nbsp; is computed as an empirical and tractable counterpart of a variational formulation of the hockey-stick divergence (see <a href="https://arxiv.org/pdf/2307.05608.pdf">the paper</a> for more details). The accuracy of<i><span>^</span>δ</i>&nbsp; increases with the number of samples drawn from the mechanism, but decreases as the variational formulation is simplified. We balance these factors in order to ensure that<i><span>^</span>δ</i>&nbsp; is both accurate and easy to compute. 
</p>

<p>
<strong>Dataset finders</strong> use <a href="https://arxiv.org/pdf/2207.13676.pdf">black-box optimization</a> to find datasets <em>D</em> and <em>D’</em> that maximize<i><span>^</span>δ</i>, a lower bound on the divergence value <em>δ</em>. Note that black-box optimization techniques are specifically designed for settings where deriving gradients for an objective function may be impractical or even impossible. These optimization techniques oscillate between exploration and exploitation phases to estimate the shape of the objective function and predict areas where the objective can have optimal values. In contrast, a full exploration algorithm, such as the <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search">grid search method</a>, searches over the full space of neighboring datasets <em>D</em> and <em>D’</em>. DP-Auditorium implements different dataset finders through the open sourced black-box optimization library <a href="https://github.com/google/vizier">Vizier</a>. 
</p>
<p>
Running existing components on a new mechanism only requires defining the mechanism as a Python function that takes an array of data <em>D</em> and a desired number of samples <em>n</em> to be output by the mechanism computed on <em>D</em>. In addition, we provide flexible wrappers for testers and dataset finders that allow practitioners to implement their own testing and dataset search algorithms.
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h2>Key results</h2>


<p>
We assess the effectiveness of DP-Auditorium on  five private and nine non-private mechanisms with diverse output spaces. For each property tester, we repeat the test ten times on fixed datasets using different values of <em>ε</em>, and report the number of times each tester identifies privacy bugs. While no tester consistently outperforms the others, we identify bugs that would be missed by previous techniques (HistogramPropertyTester). Note that the HistogramPropertyTester is not applicable to SVT mechanisms. 
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlLYAUJ1cew8xCQNyNMvggKZ2c2bd5uHLzUdLx3xVdn_TW4ZBwd5tCI6zVVvVjmOWKJanJ4vP4swXOzNpZ4388x-iwISjqAzxnDAgM8F4-HL5gHLAGs3AIuqhns-gNJfA_AT9lmAMvItLRDEP5OjHPRFRA6OldJrY6Yost66LZ8Zsif8wIw6Uhkfa4PkN7/s785/image22.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhlLYAUJ1cew8xCQNyNMvggKZ2c2bd5uHLzUdLx3xVdn_TW4ZBwd5tCI6zVVvVjmOWKJanJ4vP4swXOzNpZ4388x-iwISjqAzxnDAgM8F4-HL5gHLAGs3AIuqhns-gNJfA_AT9lmAMvItLRDEP5OjHPRFRA6OldJrY6Yost66LZ8Zsif8wIw6Uhkfa4PkN7/s16000/image22.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Number of times each property tester finds the privacy violation for the tested non-private mechanisms. NonDPLaplaceMean and NonDPGaussianMean mechanisms are faulty implementations of the <a href="https://en.wikipedia.org/wiki/Additive_noise_differential_privacy_mechanisms#Laplace_Mechanism">Laplace</a> and <a href="https://en.wikipedia.org/wiki/Additive_noise_differential_privacy_mechanisms#Gaussian_Mechanism">Gaussian</a> mechanisms for computing the mean.</td></tr></tbody></table>

<br />


<p>
We also analyze the implementation of a <a href="https://github.com/tensorflow/privacy/blob/master/tensorflow_privacy/privacy/optimizers/dp_optimizer_keras.py">DP gradient descent algorithm</a> (DP-GD) in TensorFlow that computes gradients of the loss function on private data. To preserve privacy, DP-GD employs a clipping mechanism to bound the <a href="https://mathworld.wolfram.com/L2-Norm.html">l2-norm</a> of the gradients by a value <em>G</em>, followed by the addition of Gaussian noise. This implementation incorrectly assumes that the noise added has a scale of <em>G</em>, while in reality, the scale is <em>sG</em>, where <em>s</em> is a positive scalar. This discrepancy leads to an approximate DP guarantee that holds only for values of <em>s</em> greater than or equal to 1.
</p>

<p>
We evaluate the effectiveness of property testers in detecting this bug and show that HockeyStickPropertyTester and RényiPropertyTester exhibit superior performance in identifying privacy violations, outperforming MMDPropertyTester and HistogramPropertyTester. Notably, these testers detect the bug even for values of <em>s</em> as high as 0.6. It is worth highlighting that <em>s </em>= 0.5 corresponds to a <a href="https://github.com/tensorflow/privacy/blob/308cbda4db6ccad5d1e7d56248727274e4c0c79e/tensorflow_privacy/privacy/analysis/compute_dp_sgd_privacy_lib.py#L445C1-L446C1">common error</a> in literature that involves missing a factor of two when accounting for the privacy budget <em>ε</em>. DP-Auditorium successfully captures this bug as shown below. For more details see section 5.6 <a href="https://arxiv.org/pdf/2303.00654.pdf">here</a>.


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-pnMcLqTWv1vSIZWncvObk3acW_SkBS3Lp_KuspJPbGBSjlepwW0hTLkCgLA7yTgU35y-Kj4HC_ddRX1fXS6T_HoF5Na87cSIcdiTBAwHnQ1sQZV3pdir_SI5PuwT7HAMEYmQohCd7wI84bNjKSt4sUVdnk9dOAXtkxCUDgzd3KZs5r2G2Z4jIZR0-FJH/s836/image21.jpg" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-pnMcLqTWv1vSIZWncvObk3acW_SkBS3Lp_KuspJPbGBSjlepwW0hTLkCgLA7yTgU35y-Kj4HC_ddRX1fXS6T_HoF5Na87cSIcdiTBAwHnQ1sQZV3pdir_SI5PuwT7HAMEYmQohCd7wI84bNjKSt4sUVdnk9dOAXtkxCUDgzd3KZs5r2G2Z4jIZR0-FJH/s16000/image21.jpg" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Estimated divergences and test thresholds for different values of <em>s</em> when testing DP-GD with the HistogramPropertyTester (<strong>left</strong>) and the HockeyStickPropertyTester (<strong>right</strong>).</td></tr></tbody></table>



<br />

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibbce0TFnWcnJ4CoXPVVyuZrja_3JJTnBjsza7Ig-NibA14jHoh4TIuIhLRn9BgCdo_N4hSuft7Zpl3WgNjmteMUGkQ5xdjeFH2SzZlKmPR_PvXS-JeOIcwJO8J_h7SlR9_tknZ0fLbP2qOypalwVm-nZO118Oa67zgdi_VGc72tAzGKaYpGoWIl6p_ljD/s828/image20.jpg" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEibbce0TFnWcnJ4CoXPVVyuZrja_3JJTnBjsza7Ig-NibA14jHoh4TIuIhLRn9BgCdo_N4hSuft7Zpl3WgNjmteMUGkQ5xdjeFH2SzZlKmPR_PvXS-JeOIcwJO8J_h7SlR9_tknZ0fLbP2qOypalwVm-nZO118Oa67zgdi_VGc72tAzGKaYpGoWIl6p_ljD/s16000/image20.jpg" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Estimated divergences and test thresholds for different values of <em>s</em> when testing DP-GD with the RényiPropertyTester (<strong>left</strong>) and the MMDPropertyTester (<strong>right</strong>)</td></tr></tbody></table>
<br />


<p>
To test dataset finders, we compute the number of datasets explored before finding a privacy violation. On average, the majority of bugs are discovered in less than 10 calls to dataset finders. Randomized and exploration/exploitation methods are more efficient at finding datasets than grid search. For more details, see the <a href="https://arxiv.org/abs/2307.05608">paper</a>.
</p>


<div style="line-height: 40%;">
    <br />
</div>
<h2>Conclusion</h2>


<p>
DP is one of the most powerful frameworks for data protection. However, proper implementation of DP mechanisms can be challenging and prone to errors that cannot be easily detected using traditional unit testing methods. A unified testing framework can help auditors, regulators, and academics ensure that private mechanisms are indeed private. 
</p>

<p>
DP-Auditorium is a new approach to testing DP via divergence optimization over function spaces. Our results show that this type of function-based estimation consistently outperforms previous black-box access testers. Finally, we demonstrate that these function-based estimators allow for a better discovery rate of privacy bugs compared to histogram estimation. By <a href="https://github.com/google/differential-privacy/tree/main/python/dp_auditorium">open sourcing</a> DP-Auditorium, we aim to establish a standard for end-to-end testing of new differentially private algorithms.
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h2>Acknowledgements</h2>


<p>
<em>The work described here was done jointly with Andrés Muñoz Medina, William Kong and Umar Syed. We thank Chris Dibak and Vadym Doroshenko for helpful engineering support and interface suggestions for our library.</em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-3264694155710647310</id>
            <title>Graph neural networks in TensorFlow</title>
            <link>http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-3264694155710647310</guid>
            <pubDate></pubDate>
            <updated>2024-02-06T11:17:53.968-08:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcnTwrjg8cyZhVY1c-qi2ZEenIrDlkmlKlX0GsAuiKiIoxUu6i-phANh8tsCG4mUm5i-7t3zdLwuwn5DCcuQI5FKq-C3eibPnuqfoLuKFUsx-I3Ovim1Teps_JKiKZH7XqgHupnsOa2Y3peUgWcPNYG4ZIqA2_KQwxJpflo0WM6gNW8tXg5eDndiWx_dKK/s72-c/TFGNN%20hero.gif"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Dustin Zelle, Software Engineer, Google Research, and Arno Eigenwillig, Software Engineer, CoreML</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcnTwrjg8cyZhVY1c-qi2ZEenIrDlkmlKlX0GsAuiKiIoxUu6i-phANh8tsCG4mUm5i-7t3zdLwuwn5DCcuQI5FKq-C3eibPnuqfoLuKFUsx-I3Ovim1Teps_JKiKZH7XqgHupnsOa2Y3peUgWcPNYG4ZIqA2_KQwxJpflo0WM6gNW8tXg5eDndiWx_dKK/s1600/TFGNN%20hero.gif" style="display: none;" />

<p>
Objects and their relationships are ubiquitous in the world around us, and relationships can be as important to understanding an object as its own attributes viewed in isolation — take for example transportation networks, production networks, knowledge graphs, or social networks. Discrete mathematics and computer science have a long history of formalizing such networks as <em><a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graphs</a></em>, consisting of <em>nodes</em> connected by <em>edges</em> in various irregular ways. Yet most machine learning (ML) algorithms allow only for regular and uniform relations between input objects, such as a grid of pixels, a sequence of words, or no relation at all. 
</p>
<a name="more"></a>

<p>
<a href="https://distill.pub/2021/gnn-intro/">Graph neural networks</a>, or GNNs for short, have emerged as a powerful technique to leverage both the graph’s connectivity (as in the older algorithms <a href="http://perozzi.net/projects/deepwalk/">DeepWalk</a> and <a href="https://snap.stanford.edu/node2vec/">Node2Vec</a>) and the input features on the various nodes and edges. GNNs can make predictions for graphs as a whole (Does this molecule react in a certain way?), for individual nodes (What’s the topic of this document, given its citations?) or for potential edges (Is this product likely to be purchased together with that product?). Apart from making predictions about graphs, GNNs are a powerful tool used to bridge the chasm to more typical neural network use cases. They encode a graph's <em>discrete</em>, <em>relational</em> information in a <em>continuous</em> way so that it can be included naturally in another deep learning system.
</p>
<p>
We are excited to announce the release of <a href="https://github.com/tensorflow/gnn">TensorFlow GNN 1.0</a> (TF-GNN), a production-tested library for building GNNs at large scales. It supports both modeling and training in TensorFlow as well as the extraction of input graphs from huge data stores. TF-GNN is built from the ground up for heterogeneous graphs, where types of objects and relations are represented by distinct sets of nodes and edges. Real-world objects and their relations occur in distinct types, and TF-GNN's heterogeneous focus makes it natural to represent them.
</p>
<p>
  Inside TensorFlow, such graphs are represented by objects of type <code>tfgnn.GraphTensor</code>. This is a composite tensor type (a collection of tensors in one Python class) accepted as a <a href="https://en.wikipedia.org/wiki/First-class_citizen">first-class citizen</a> in <code>tf.data.Dataset</code>, <code>tf.function</code>, etc. It stores both the graph structure and its features attached to nodes, edges and the graph as a whole. Trainable transformations of GraphTensors can be defined as Layers objects in the high-level <a href="https://www.tensorflow.org/guide/keras">Keras API</a>, or directly using the <code>tfgnn.GraphTensor</code> primitive.
</p>



<div style="line-height: 40%;">
    <br />
</div>
<h2>GNNs: Making predictions for an object in context</h2>


<p>
For illustration, let’s look at one typical application of TF-GNN: predicting a property of a certain type of node in a graph defined by cross-referencing tables of a huge database. For example, a citation database of Computer Science (CS) arXiv papers with one-to-many cites and many-to-one cited relationships where we would like to predict the subject area of each paper.
</p>
<p>
Like most neural networks, a GNN is trained on a dataset of many labeled examples (~millions), but each training step consists only of a much smaller batch of training examples (say, hundreds). To scale to millions, the GNN gets trained on a stream of reasonably small subgraphs from the underlying graph. Each subgraph contains enough of the original data to compute the GNN result for the labeled node at its center and train the model. This process — typically referred to as subgraph sampling — is extremely consequential for GNN training. Most existing tooling accomplishes sampling in a batch way, producing static subgraphs for training. TF-GNN provides tooling to improve on this by sampling dynamically and interactively. 
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhE36FVnslwVrX4LjLgpe5NOcVgJ2WSHCaw64LT9pMhjhHOFt-1pjp1AhaXqjxfEODX04Buw93D1G36HOStu5_mWUEdNs0gZTa1c7MXJ6ir9DYOp_HCYpFMT5NZiBbHxNwvUmF-dwhN2rgKQX0CeFY25X9aFnoD0W7bzL_xtkDJFdP0guocAJDSOgBHIiZm/s800/image2.gif" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhE36FVnslwVrX4LjLgpe5NOcVgJ2WSHCaw64LT9pMhjhHOFt-1pjp1AhaXqjxfEODX04Buw93D1G36HOStu5_mWUEdNs0gZTa1c7MXJ6ir9DYOp_HCYpFMT5NZiBbHxNwvUmF-dwhN2rgKQX0CeFY25X9aFnoD0W7bzL_xtkDJFdP0guocAJDSOgBHIiZm/s16000/image2.gif" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Pictured, the process of subgraph sampling where small, tractable subgraphs are sampled from a larger graph to create input examples for GNN training.</td></tr></tbody></table>




<p>
TF-GNN 1.0 debuts a flexible Python API to configure dynamic or batch subgraph sampling at all relevant scales: interactively in a Colab notebook (like <a href="https://colab.research.google.com/github/tensorflow/gnn/blob/master/examples/notebooks/ogbn_mag_e2e.ipynb">this one</a>), for efficient sampling of a small dataset stored in the main memory of a single training host, or distributed by <a href="https://beam.apache.org/">Apache Beam</a> for huge datasets stored on a network filesystem (up to hundreds of millions of nodes and billions of edges). For details, please refer to our user guides for <a href="https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/inmemory_sampler.md">in-memory</a> and <a href="https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/beam_sampler.md">beam-based</a> sampling, respectively.
</p>
<p>
On those same sampled subgraphs, the GNN’s task is to compute a hidden (or latent) state at the root node; the hidden state aggregates and encodes the relevant information of the root node's neighborhood. One classical approach is <a href="https://research.google/pubs/neural-message-passing-for-quantum-chemistry/">message-passing neural networks</a>. In each round of message passing, nodes receive messages from their neighbors along incoming edges and update their own hidden state from them. After <em>n</em> rounds, the hidden state of the root node reflects the aggregate information from all nodes within <em>n</em> edges (pictured below for <em>n</em> = 2). The messages and the new hidden states are computed by hidden layers of the neural network. In a heterogeneous graph, it often makes sense to use separately trained hidden layers for the different types of nodes and edges
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrCrQ1SCcwhZfE33X46EifocYAmKCPXMVe1d4na1V6flQavJ_f_FKtnlQbe2vnvzbSEtx5mxJHZ2OlQbO9rsiEhiPLY1PKQOT-EwahobMIVC92PZJs8RroEuYswHCpEjjpwqPrpqzKsDgrNaiY4lM_E8NVnxVRsYn0PNxe3TghByKJpW9V_YRD0RnNnm4/s573/image1.gif" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMrCrQ1SCcwhZfE33X46EifocYAmKCPXMVe1d4na1V6flQavJ_f_FKtnlQbe2vnvzbSEtx5mxJHZ2OlQbO9rsiEhiPLY1PKQOT-EwahobMIVC92PZJs8RroEuYswHCpEjjpwqPrpqzKsDgrNaiY4lM_E8NVnxVRsYn0PNxe3TghByKJpW9V_YRD0RnNnm4/s16000/image1.gif" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Pictured, a simple message-passing neural network where, at each step, the node state is propagated from outer to inner nodes where it is pooled to compute new node states. Once the root node is reached, a final prediction can be made.</td></tr></tbody></table>
<p>
The training setup is completed by placing an output layer on top of the GNN’s hidden state for the labeled nodes, computing the <em>loss </em>(to measure the prediction error), and updating model weights by backpropagation, as usual in any neural network training. 
</p>
<p>
Beyond supervised training (i.e., minimizing a loss defined by labels), GNNs can also be trained in an unsupervised way (i.e., without labels). This lets us compute a <em>continuous</em> representation (or <em>embedding</em>) of the <em>discrete</em> graph structure of nodes and their features. These representations are then typically utilized in other ML systems. In this way, the discrete, relational information encoded by a graph can be included in more typical neural network use cases. TF-GNN supports a fine-grained specification of unsupervised objectives for heterogeneous graphs.
</p>




<div style="line-height: 40%;">
    <br />
</div>
<h2>Building GNN architectures</h2>


<p>
The TF-GNN library supports building and training GNNs at various levels of abstraction.
</p>
<p>
At the highest level, users can take any of the predefined models bundled with the library that are expressed in Keras layers. Besides a small collection of models from the research literature, TF-GNN comes with a highly configurable model template that provides a curated selection of modeling choices that we have found to provide strong baselines on many of our in-house problems. The templates implement GNN layers; users need only to initialize the Keras layers.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMfB8QoX14UU1GEAmFFOP0cAj__zxa_MKzVSiJoak9cVLNdbbhrSxbIWhqQM3OYKA5lo7zW8sWr6-9utm-rw0808rBOE4Cbw7NZxcmifenvF6DCH4opWhVQJHR-MLGcFoNu_WpET5h1PZRdXMhjcyKgBg3NchNTPq6gWVVluzcQNaO5qtonVp5KnJRgUaD/s1400/TFGNN%20code1.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMfB8QoX14UU1GEAmFFOP0cAj__zxa_MKzVSiJoak9cVLNdbbhrSxbIWhqQM3OYKA5lo7zW8sWr6-9utm-rw0808rBOE4Cbw7NZxcmifenvF6DCH4opWhVQJHR-MLGcFoNu_WpET5h1PZRdXMhjcyKgBg3NchNTPq6gWVVluzcQNaO5qtonVp5KnJRgUaD/s16000/TFGNN%20code1.png" /></a></td></tr></tbody></table>







<p>
At the lowest level, users can write a GNN model from scratch in terms of primitives for passing data around the graph, such as broadcasting data from a node to all its outgoing edges or pooling data into a node from all its incoming edges (e.g., computing the sum of incoming messages). TF-GNN’s graph data model treats nodes, edges and whole input graphs equally when it comes to features or hidden states, making it straightforward to express not only node-centric models like the MPNN discussed above but also more general forms of <a href="https://arxiv.org/abs/1806.01261">GraphNets</a>. This can, but need not, be done with Keras as a modeling framework on the top of core TensorFlow. For more details, and intermediate levels of modeling, see the TF-GNN <a href="https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/gnn_modeling.md">user guide</a> and <a href="https://github.com/tensorflow/gnn/tree/main/tensorflow_gnn/models">model collection</a>.
</p>




<div style="line-height: 40%;">
    <br />
</div>
<h2>Training orchestration</h2>


<p>
While advanced users are free to do custom model training, the <a href="https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/runner.md">TF-GNN Runner</a> also provides a succinct way to orchestrate the training of Keras models in the common cases. A simple invocation may look like this:
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxRRMrWL-AyxpHeyAhffhApAzlq-u7FoZaDnZFlwRsoYCljzZNi0LmRDDMwZ7mkXeBK0oUFujf_TDD-zlTQcgnLGhPedfrJ2vVs-D5-RPZFWXaaRpOJIt-MH3N8Tj7NZy-SFXTjxjDrhHQY_HVUA3-_C8_xQjfRWBlO-dzcFzgUL6wynMWJhUM7z_MYKvF/s1400/TFGNN%20code2.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxRRMrWL-AyxpHeyAhffhApAzlq-u7FoZaDnZFlwRsoYCljzZNi0LmRDDMwZ7mkXeBK0oUFujf_TDD-zlTQcgnLGhPedfrJ2vVs-D5-RPZFWXaaRpOJIt-MH3N8Tj7NZy-SFXTjxjDrhHQY_HVUA3-_C8_xQjfRWBlO-dzcFzgUL6wynMWJhUM7z_MYKvF/s16000/TFGNN%20code2.png" /></a></td></tr></tbody></table>




<p>
The Runner provides ready-to-use solutions for ML pains like distributed training and <code>tfgnn.GraphTensor</code> padding for fixed shapes on Cloud TPUs. Beyond training on a single task (as shown above), it supports joint training on multiple (two or more) tasks in concert. For example, unsupervised tasks can be mixed with supervised ones to inform a final continuous representation (or embedding) with application specific inductive biases. Callers only need substitute the task argument with a mapping of tasks:
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4GGpfZib5MAUnX7BRLywJC4xMVt9Tz8kSMhgyDGN5A-aS9k-gna_t0Fo3uxMaAb8gK0ovrOO3XkeSNZ3i24leBCNsALR2NU_MWI7M_s47p2bx-aviaUKy_DxDEkzndNYMI_52jcEmNKyJrqDFye3_PHaWJZz7MAQ1lVW-YpuWPOOYpSAfbrunU5q4M2ev/s1400/TFGNN%20code3.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4GGpfZib5MAUnX7BRLywJC4xMVt9Tz8kSMhgyDGN5A-aS9k-gna_t0Fo3uxMaAb8gK0ovrOO3XkeSNZ3i24leBCNsALR2NU_MWI7M_s47p2bx-aviaUKy_DxDEkzndNYMI_52jcEmNKyJrqDFye3_PHaWJZz7MAQ1lVW-YpuWPOOYpSAfbrunU5q4M2ev/s16000/TFGNN%20code3.png" /></a></td></tr></tbody></table>



<p>
Additionally, the TF-GNN Runner also includes an implementation of <a href="https://www.tensorflow.org/tutorials/interpretability/integrated_gradients">integrated gradients</a> for use in model attribution. Integrated gradients output is a GraphTensor with the same connectivity as the observed GraphTensor but its features replaced with gradient values where larger values contribute more than smaller values in the GNN prediction. Users can inspect gradient values to see which features their GNN uses the most. 
</p>




<div style="line-height: 40%;">
    <br />
</div>
<h2>Conclusion</h2>


<p>
In short, we hope TF-GNN will be useful to advance the application of GNNs in TensorFlow at scale and fuel further innovation in the field. If you’re curious to find out more, please try our <a href="https://colab.sandbox.google.com/github/tensorflow/gnn/blob/master/examples/notebooks/ogbn_mag_e2e.ipynb">Colab demo</a> with the popular OGBN-MAG benchmark (in your browser, no installation required), browse the rest of our <a href="https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/overview.md">user guides and Colabs</a>, or take a look at our <a href="https://arxiv.org/abs/2207.03522">paper</a>.
</p>




<div style="line-height: 40%;">
    <br />
</div>
<h2>Acknowledgements</h2>


<p>
<em>The TF-GNN release 1.0 was developed by a collaboration between Google Research: Sami Abu-El-Haija, Neslihan Bulut, Bahar Fatemi, Johannes Gasteiger, Pedro Gonnet, Jonathan Halcrow, Liangze Jiang, Silvio Lattanzi, Brandon Mayer, Vahab Mirrokni, Bryan Perozzi, Anton Tsitsulin, Dustin Zelle, Google Core ML: Arno Eigenwillig, Oleksandr Ferludin, Parth Kothari, Mihir Paradkar, Jan Pfeifer, Rachael Tamakloe, and Google DeepMind:<strong> </strong>Alvaro Sanchez-Gonzalez and Lisa Wang.</em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-6956767920612914706</id>
            <title>A decoder-only foundation model for time-series forecasting</title>
            <link>http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-6956767920612914706</guid>
            <pubDate></pubDate>
            <updated>2024-02-07T16:05:00.722-08:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjLAVI4q3e6yNyTPTCFiLZVQfFm71GOX1TosHg_Sb8M6tVSO1hyphenhyphenZccOlufnqSuXP1rVWHmqHcely6fgW1vex4JdxenniJcaJ7TOomZolUFut8RUdxnOFZDrbt0hrIHkcrK7rl6cq5-kUuWGrOYqIirPAKtnf4vMDauPX4lFAz2PQjiqzqHxMna7eja9gOF/s72-c/hero.jpg"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Rajat Sen and Yichen Zhou, Google Research</span>


<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjLAVI4q3e6yNyTPTCFiLZVQfFm71GOX1TosHg_Sb8M6tVSO1hyphenhyphenZccOlufnqSuXP1rVWHmqHcely6fgW1vex4JdxenniJcaJ7TOomZolUFut8RUdxnOFZDrbt0hrIHkcrK7rl6cq5-kUuWGrOYqIirPAKtnf4vMDauPX4lFAz2PQjiqzqHxMna7eja9gOF/s320/hero.jpg" style="display: none;" />

<p>
<a href="https://en.wikipedia.org/wiki/Time_series">Time-series</a> forecasting is ubiquitous in various domains, such as retail, finance, manufacturing, healthcare and natural sciences. In retail use cases, for example, it has been observed that <a href="https://www.mckinsey.com/featured-insights/artificial-intelligence/notes-from-the-ai-frontier-applications-and-value-of-deep-learning">improving demand forecasting accuracy</a> can meaningfully reduce inventory costs and increase revenue. Deep learning (DL) models have emerged as a popular approach for forecasting rich, multivariate, time-series data because they have proven to perform well in a variety of settings (e.g., DL models performed well in the <a href="https://www.sciencedirect.com/science/article/pii/S0169207021001874">M5 competition</a>).
</p>
<a name="more"></a>
<p>
At the same time, there has been rapid progress in large foundation language models used for natural language processing (NLP) tasks, such as <a href="https://en.wikipedia.org/wiki/Machine_translation">translation</a>, <a href="https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/">retrieval-augmented generation</a>, and <a href="https://en.wikipedia.org/wiki/Intelligent_code_completion">code completion</a>. These models are trained on massive amounts of <em>textual </em>data derived from a variety of sources like <a href="https://commoncrawl.org/">common crawl</a> and open-source code that allows them to identify patterns in languages. This makes them very powerful <a href="https://en.wikipedia.org/wiki/Zero-shot_learning">zero-shot</a> tools; for instance, <a href="https://blog.google/products/bard/google-bard-try-gemini-ai/">when paired with retrieval</a>, they can answer questions about and summarize current events.
</p>

<p>
Despite DL-based forecasters largely <a href="https://arxiv.org/abs/1704.04110">outperforming</a> traditional methods and progress being made in <a href="https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-forecasting">reducing training and inference costs</a>, they face challenges: most DL architectures require <a href="https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-forecasting">long and involved training and validation cycles</a> before a customer can test the model on a new time-series. A foundation model for time-series forecasting, in contrast, can provide decent out-of-the-box forecasts on unseen time-series data with no additional training, enabling users to focus on refining forecasts for the actual downstream task like <a href="https://en.wikipedia.org/wiki/Customer_demand_planning">retail demand planning</a>.
</p>

<p>
To that end, in “<a href="https://arxiv.org/pdf/2310.10688.pdf">A decoder-only foundation model for time-series forecasting</a>”, we introduce TimesFM, a single forecasting model pre-trained on a large time-series corpus of 100 billion real world time-points. Compared to the latest large language models (LLMs), TimesFM is much smaller (200M parameters), yet we show that even at such scales, its zero-shot performance on a variety of unseen datasets of different domains and temporal granularities come close to the state-of-the-art supervised approaches trained explicitly on these datasets. Later this year we plan to make this model available for external customers in <a href="https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting/train-model#aiplatform_create_training_pipeline_tabular_forecasting_sample-python">Google Cloud Vertex AI</a>.
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h2>A decoder-only foundation model for time-series forecasting</h2>


<p>
LLMs are usually trained in a <a href="https://arxiv.org/pdf/1801.10198.pdf">decoder-only</a> fashion that involves three steps. First, text is broken down into subwords called tokens. Then, the tokens are fed into stacked causal <a href="https://arxiv.org/abs/1706.03762">transformer</a> layers that produce an output corresponding to each input token (it cannot attend to future tokens). Finally, the output corresponding to the <em>i</em>-th token summarizes all the information from previous tokens and predicts the (<em>i</em>+1)-th token. During inference, the LLM generates the output one token at a time. For example, when prompted with “What is the capital of France?”, it might generate the token “The”, then condition on “What is the capital of France? The” to generate the next token “capital” and so on until it generates the complete answer: “The capital of France is Paris”.
</p>

<p>
A foundation model for time-series forecasting should adapt to variable context (what we observe) and horizon (what we query the model to forecast) lengths, while having enough capacity to encode all patterns from a large pretraining dataset. Similar to LLMs, we use stacked transformer layers (self-attention and <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">feedforward</a> layers) as the main building blocks for the TimesFM model. In the context of time-series forecasting, we treat a patch (a group of contiguous time-points) as a token that was popularized by a recent <a href="https://arxiv.org/abs/2211.14730">long-horizon forecasting work</a>. The task then is to forecast the (<em>i</em>+1)-th patch of time-points given the <em>i</em>-th output at the end of the stacked transformer layers. 
</p>

<p>
However, there are several key differences from language models. Firstly, we need a <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">multilayer perceptron</a> block with residual connections to convert a patch of time-series into a token that can be input to the transformer layers along with <a href="https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/">positional encodings</a> (PE). For that, we use a residual block similar to our prior work in <a href="https://arxiv.org/abs/2304.08424">long-horizon forecasting</a>. Secondly, at the other end, an output token from the stacked transformer can be used to predict a longer length of subsequent time-points than the input patch length, i.e., the output patch length can be larger than the input patch length.  
</p>

<p>
Consider a time-series of length 512 time-points being used to train a TimesFM model with input patch length 32 and output patch length 128. During training, the model is simultaneously trained to use the first 32 time-points to forecast the next 128 time-points, the first 64 time-points to forecast time-points 65 to 192, the first 96 time-points to forecast time-points 97 to 224 and so on. During inference, suppose the model is given a new time-series of length 256 and tasked with forecasting the next 256 time-points into the future. The model will first generate the future predictions for time-points 257 to 384, then condition on the initial 256 length input plus the generated output to generate time-points 385 to 512. On the other hand, if in our model the output patch length was equal to the input patch length of 32 then for the same task we would have to go through eight generation steps instead of just the two above. This increases the chances of more errors accumulating and therefore, in practice, we see that a longer output patch length yields better performance for long-horizon forecasting
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4G0lBOLUqlPIXJ3R68kjS984MBIKBPDBrCWtgmjVVTyQRqY6-rn3aHJjgxCbG-8csyBLsp0POILdeJ2VcsRy8lrip0k5DWsUpuL9LU1qOPXLW99mraNdd6HVU791NYqJeTyY7LjuMnOIo6RGmkxBQqqaPrSsC0dELrwy21QUs1Jgwxr8flmdNkDV2tZsT/s1084/image3.jpg" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4G0lBOLUqlPIXJ3R68kjS984MBIKBPDBrCWtgmjVVTyQRqY6-rn3aHJjgxCbG-8csyBLsp0POILdeJ2VcsRy8lrip0k5DWsUpuL9LU1qOPXLW99mraNdd6HVU791NYqJeTyY7LjuMnOIo6RGmkxBQqqaPrSsC0dELrwy21QUs1Jgwxr8flmdNkDV2tZsT/s16000/image3.jpg" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">TimesFM architecture.</td></tr></tbody></table>

<br />

<div style="line-height: 40%;">
    <br />
</div>
<h2>Pretraining data</h2>


<p>
Just like LLMs get better with more tokens, TimesFM requires a large volume of legitimate time series data to learn and improve. We have spent a great amount of time creating and assessing our training datasets, and the following is what we have found works best:
</p>




<div style="margin-left: 40px;">
<p>

    <strong>Synthetic data helps with the basics.</strong> Meaningful synthetic time-series data can be generated using statistical models or physical simulations. These basic temporal patterns can teach the model the grammar of time series forecasting.
</p></div>
  
<div style="margin-left: 40px;">
<p>  
    <strong>Real-world data adds real-world flavor.</strong> We comb through available public time series datasets, and selectively put together a large corpus of 100 billion time-points. Among these datasets there are <a href="https://trends.google.com/trends/">Google Trends</a> and <a href="https://meta.wikimedia.org/wiki/Research:Page_view">Wikipedia Pageviews</a>, which track what people are interested in, and that nicely mirrors trends and patterns in many other real-world time series. This helps TimesFM understand the bigger picture and generalize better when provided with domain-specific contexts not seen during training.
</p></div>

<div style="line-height: 40%;">
    <br />
</div>
<h2>Zero-shot evaluation results</h2>


<p>
We evaluate TimesFM zero-shot on data not seen during training using popular time-series benchmarks. We observe that TimesFM performs better than most statistical methods like <a href="https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average">ARIMA</a>, <a href="https://en.wikipedia.org/wiki/Exponential_smoothing">ETS</a> and can match or outperform powerful DL models like <a href="https://arxiv.org/abs/1704.04110">DeepAR</a>, <a href="https://arxiv.org/abs/2211.14730">PatchTST</a> that have been <em>explicitly trained</em> on the target time-series.
</p>

<p>
We used the <a href="https://huggingface.co/datasets/monash_tsf">Monash Forecasting Archive</a> to evaluate TimesFM’s out-of-the-box performance. This archive contains tens of thousands of time-series from various domains like traffic, weather, and demand forecasting covering frequencies ranging from few minutes to yearly data. Following existing literature, we inspect the <a href="https://en.wikipedia.org/wiki/Mean_absolute_error">mean absolute error</a> (MAE) <a href="https://arxiv.org/abs/2310.07820">appropriately scaled</a> so that it can be averaged across the datasets. We see that zero-shot (ZS) TimesFM is better than most supervised approaches, including recent deep learning models. We also compare TimesFM to <a href="https://platform.openai.com/docs/models/gpt-3-5">GPT-3.5</a> for forecasting using a specific prompting technique proposed by <a href="https://arxiv.org/abs/2310.07820">llmtime(ZS)</a>. We demonstrate that TimesFM performs better than llmtime(ZS) despite being orders of magnitude smaller.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIeNF6GcmbUvVvYpKxNSvwlm_swz6M3G7nTDl0INa2zq8AlvjTBCVuvwOw0dx48JCk4H3S0aBUcsvqj2BypV3340cblqgD6yktoLBXzpxA2fwoM4n_KU8m0TfaESjihc3nx29RYVTpO4g09RCK-rucPulH3gqEOU9jO7EZ_VbDcFnfB_RHXmdpuZO_T_-g/s1476/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIeNF6GcmbUvVvYpKxNSvwlm_swz6M3G7nTDl0INa2zq8AlvjTBCVuvwOw0dx48JCk4H3S0aBUcsvqj2BypV3340cblqgD6yktoLBXzpxA2fwoM4n_KU8m0TfaESjihc3nx29RYVTpO4g09RCK-rucPulH3gqEOU9jO7EZ_VbDcFnfB_RHXmdpuZO_T_-g/s16000/image2.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Scaled MAE (the lower the better) of TimesFM(ZS) against other supervised and zero-shot approaches on Monash datasets.</td></tr></tbody></table>
<br />



<p>
Most of the Monash datasets are short or medium horizon, i.e., the prediction length is not too long. We also test TimesFM on popular benchmarks for long horizon forecasting against a recent state-of-the-art baseline <a href="https://arxiv.org/abs/2211.14730">PatchTST</a> (and other long-horizon forecasting baselines). In the next figure, we plot the MAE on <a href="https://paperswithcode.com/dataset/ett">ETT</a> datasets for the task of predicting 96 and 192 time-points into the future. The metric has been calculated on the last test window of each dataset (as done by the <a href="https://arxiv.org/abs/2310.07820">llmtime</a> paper). We see that TimesFM not only surpasses the performance of llmtime(ZS) but also matches that of the supervised PatchTST model explicitly trained on the respective datasets. 
</p>





<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0DDM32GPO6zkmnIrObEP2OA92g45b-zSMHgCf-uNoj6Ed0M0zVsN7vmFmfgXT6Sh5p-W0xI1qj6YwXcqi3T6aD5hI9ZOJqT8Sobp43FGrtSsLUkI2poHnGml7Za4BMObSd6nEKUVL8wj7nHJDFYHbWaQOXOcfxvqXUcMxUZ3WVQW8Z5sabfFsi7M85_7I/s735/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0DDM32GPO6zkmnIrObEP2OA92g45b-zSMHgCf-uNoj6Ed0M0zVsN7vmFmfgXT6Sh5p-W0xI1qj6YwXcqi3T6aD5hI9ZOJqT8Sobp43FGrtSsLUkI2poHnGml7Za4BMObSd6nEKUVL8wj7nHJDFYHbWaQOXOcfxvqXUcMxUZ3WVQW8Z5sabfFsi7M85_7I/s16000/image1.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Last window MAE (the lower the better) of TimesFM(ZS) against llmtime(ZS) and long-horizon forecasting baselines on ETT datasets.</td></tr></tbody></table>

<br />

<div style="line-height: 40%;">
    <br />
</div>
<h2>Conclusion</h2>


<p>
We train a decoder-only foundation model for time-series forecasting using a large pretraining corpus of 100B real world time-points, the majority of which was search interest time-series data derived from Google Trends and pageviews from Wikipedia. We show that even a relatively small 200M parameter pretrained model that uses our TimesFM architecture displays impressive zero-shot performance on a variety of public benchmarks from different domains and granularities. 
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h2>Acknowledgements</h2>


<p>
<em>This work is the result of a collaboration between several individuals across Google Research and Google Cloud, including (in alphabetical order): Abhimanyu Das, Weihao Kong, Andrew Leach, Mike Lawrence, Alex Martin, Rajat Sen, Yang Yang, Skander Hannachi, Ivan Kuznetsov and Yichen Zhou.</em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-2254287928040727502</id>
            <title>Intervening on early readouts for mitigating spurious features and simplicity bias</title>
            <link>http://blog.research.google/2024/02/intervening-on-early-readouts-for.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-2254287928040727502</guid>
            <pubDate></pubDate>
            <updated>2024-02-02T09:49:36.211-08:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdBd5rMRA2U1nd8fetuEweTgmHncn49ASMQtPlm6dfsr5V29RwsoUR8UtK4B7oSE1eiIdW-vD-gjCUK4tGZTbsY4XdO0adL2YtAjpgbF1S3mL_Jw3f31SwLKYUtCOLJ807gdXdRmD5iVsrtc_Ii-BiqQacv89vbtRbNAIINa9PhKAF_sDAZu09FLs4599T/s72-c/SiFer%20Hero.png"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Rishabh Tiwari, Pre-doctoral Researcher, and Pradeep Shenoy, Research Scientist, Google Research</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdBd5rMRA2U1nd8fetuEweTgmHncn49ASMQtPlm6dfsr5V29RwsoUR8UtK4B7oSE1eiIdW-vD-gjCUK4tGZTbsY4XdO0adL2YtAjpgbF1S3mL_Jw3f31SwLKYUtCOLJ807gdXdRmD5iVsrtc_Ii-BiqQacv89vbtRbNAIINa9PhKAF_sDAZu09FLs4599T/s1600/SiFer%20Hero.png" style="display: none;" />

<p>
Machine learning models in the real world are often trained on limited data that may contain unintended <a href="https://en.wikipedia.org/wiki/Bias_(statistics)">statistical biases</a>. For example, in the <a href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CELEBA</a> celebrity image dataset, a disproportionate number of female celebrities have blond hair, leading to classifiers incorrectly predicting “blond” as the hair color for most female faces — here, gender is a spurious feature for predicting hair color. Such unfair biases could have significant consequences in critical applications such as <a href="https://www.researchgate.net/publication/362524426_Addressing_fairness_in_artificial_intelligence_for_medical_imaging">medical diagnosis</a>. 
</p>
<a name="more"></a>


<p>
Surprisingly, recent work has also discovered an inherent tendency of deep networks to <em>amplify such statistical biases</em>, through the so-called <a href="https://proceedings.neurips.cc/paper/2020/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf">simplicity bias</a> of deep learning. This bias is the tendency of deep networks to identify weakly predictive features early in the training, and continue to anchor on these features, failing to identify more complex and potentially more accurate features. 
</p>
<p>
With the above in mind, we propose simple and effective fixes to this dual challenge of spurious features and simplicity bias by applying <em>early readouts</em> and <em>feature forgetting</em>. First, in “<a href="https://arxiv.org/abs/2310.18590">Using Early Readouts to Mediate Featural Bias in Distillation</a>”, we show that making predictions from early layers of a deep network (referred to as “early readouts”) can automatically signal issues with the quality of the learned representations. In particular, these predictions are more often wrong, and more confidently wrong, when the network is relying on spurious features. We use this erroneous confidence to improve outcomes in <a href="https://arxiv.org/pdf/1503.02531.pdf">model distillation</a>, a setting where a larger “teacher” model guides the training of a smaller “student” model. Then in “<a href="https://arxiv.org/abs/2301.13293">Overcoming Simplicity Bias in Deep Networks using a Feature Sieve</a>”, we intervene directly on these indicator signals by making the network “forget” the problematic features and consequently look for better, more predictive features. This substantially improves the model’s ability to generalize to unseen domains compared to previous approaches. Our <a href="https://ai.google/responsibility/principles">AI Principles</a> and our <a href="https://ai.google/responsibility/responsible-ai-practices/">Responsible AI practices</a> guide how we research and develop these advanced applications and help us address the challenges posed by statistical biases.
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzG_p8Re7HHeTp_Qg_GwjX5LcHsE-TZDmHr3azTSOLKl4f1J4xcL9vxo46zicAl6QoIKIrTJaI2Z51iFq2oICjeb6Ut4-W1W74bytv87pH3hKVJOotWWWDk0gwB-ak_YZRmtZyimw8b9lSJ1DRzh6uIpvIBN2pbIw-6MuN47rUjTK_RzLLfYXPrIjtpjRz/s1080/image3.gif" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzG_p8Re7HHeTp_Qg_GwjX5LcHsE-TZDmHr3azTSOLKl4f1J4xcL9vxo46zicAl6QoIKIrTJaI2Z51iFq2oICjeb6Ut4-W1W74bytv87pH3hKVJOotWWWDk0gwB-ak_YZRmtZyimw8b9lSJ1DRzh6uIpvIBN2pbIw-6MuN47rUjTK_RzLLfYXPrIjtpjRz/s16000/image3.gif" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Animation comparing hypothetical responses from two models trained with and without the feature sieve.</td></tr></tbody></table>




<div style="line-height: 40%;">
    <br />
</div>
<h2>Early readouts for debiasing distillation</h2>


<p>
We first illustrate the diagnostic value of <em>early readouts</em> and their application in debiased distillation, i.e., making sure that the student model inherits the teacher model’s resilience to feature bias through distillation. We start with a standard distillation framework where the student is trained with a mixture of label matching (minimizing the <a href="https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e">cross-entropy loss</a> between student outputs and the ground-truth labels) and teacher matching (minimizing the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a> loss between student and teacher outputs for any given input). 
</p>
<p>
Suppose one trains a linear decoder, i.e., a small auxiliary neural network named as <em>Aux,</em> on top of an intermediate representation of the student model. We refer to the output of this linear decoder as an early readout of the network representation. Our finding is that early readouts make more errors on instances that contain spurious features, and further, the confidence on those errors is higher than the confidence associated with other errors. This suggests that confidence on errors from early readouts is a fairly strong, automated indicator of the model’s dependence on potentially spurious features.
</p>





<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixpq4OhPGxL9gGW30-0kqQ_CieDj3PJcqw8L4_7fBDZOFKuQpI67ljqIItOoJ3U9-dpPd1CpofAG_ld689r0HcPTrzFeTd1ceMQ42C3CRPWWJMYknydHpJhFjQUjb-M6mx8ILQbWEBIOv-NSgTauMGgDZ8t3EMGHE3j6UN9HIF3BJmB63GhOzFwOVmswlc/s1128/image5.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEixpq4OhPGxL9gGW30-0kqQ_CieDj3PJcqw8L4_7fBDZOFKuQpI67ljqIItOoJ3U9-dpPd1CpofAG_ld689r0HcPTrzFeTd1ceMQ42C3CRPWWJMYknydHpJhFjQUjb-M6mx8ILQbWEBIOv-NSgTauMGgDZ8t3EMGHE3j6UN9HIF3BJmB63GhOzFwOVmswlc/s16000/image5.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Illustrating the usage of early readouts (i.e., output from the auxiliary layer) in debiasing distillation. Instances that are confidently mispredicted in the early readouts are upweighted in the distillation loss.</td></tr></tbody></table>





<p>
We used this signal to modulate the contribution of the teacher in the distillation loss on a per-instance basis, and found significant improvements in the trained student model as a result.
</p>
<p>
We evaluated our approach on standard benchmark datasets known to contain spurious correlations (<a href="https://arxiv.org/pdf/1911.08731.pdf">Waterbirds</a>, <a href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a>, <a href="https://www.tensorflow.org/datasets/catalog/civil_comments">CivilComments</a>, <a href="https://cims.nyu.edu/~sbowman/multinli/">MNLI</a>). Each of these datasets contain groupings of data that share an attribute potentially correlated with the label in a spurious manner. As an example, the CelebA dataset mentioned above includes groups such as {blond male, blond female, non-blond male, non-blond female}, with models typically performing the worst on the {non-blond female} group when predicting hair color. Thus, a measure of model performance is its <em>worst group accuracy</em>, i.e., the lowest accuracy among all known groups present in the dataset. We improved the worst group accuracy of student models on all datasets; moreover, we also improved overall accuracy in three of the four datasets, showing that our improvement on any one group does not come at the expense of accuracy on other groups. More details are available in our <a href="https://arxiv.org/pdf/2310.18590.pdf">paper</a>.
</p>





<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpQiz04rM3DMtDiusAWyWl92FMUKbafR0l2dGvrj17fX3nuvPDnyXMQaumsxDvch3ScnOCL4Duq5_O32dWbv_CTsIu5aNc-c3xrVAIXjQ3kmn0jZ_TZ5SJ7C2lq1oxLZ33-VKXSSPRa_oGUB5jJlsBTZupsHMeUtSVXLh414e1NVEgI1IamqhTA1dqU0s5/s1270/image4.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="511" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpQiz04rM3DMtDiusAWyWl92FMUKbafR0l2dGvrj17fX3nuvPDnyXMQaumsxDvch3ScnOCL4Duq5_O32dWbv_CTsIu5aNc-c3xrVAIXjQ3kmn0jZ_TZ5SJ7C2lq1oxLZ33-VKXSSPRa_oGUB5jJlsBTZupsHMeUtSVXLh414e1NVEgI1IamqhTA1dqU0s5/w640-h511/image4.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Comparison of Worst Group Accuracies of different distillation techniques relative to that of the Teacher model. Our method outperforms other methods on all datasets.</td></tr></tbody></table>




<div style="line-height: 40%;">
    <br />
</div>
<h2>Overcoming simplicity bias with a feature sieve</h2>


<p>
In a second, closely related project, we intervene directly on the information provided by early readouts, to improve <a href="https://en.wikipedia.org/wiki/Feature_learning">feature learning</a> and <a href="https://developers.google.com/machine-learning/crash-course/generalization/video-lecture">generalization</a>. The workflow alternates between <em>identifying </em>problematic features and <em>erasing identified features</em> from the network. Our primary hypothesis is that early features are more prone to simplicity bias, and that by erasing (“sieving”) these features, we allow richer feature representations to be learned.  
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghN4NJ5vZ6jESH3koLTfGa3DpSenk5liLEg2awv2cOo1blDwwuDjLGVGxyeHSAzkLWTBUwO_swf4uGC2oShnD0WTNrebCL9KLAMOBIxR3ZZnw9eVS8g16s_lgP5kCbhZmVoTctASyDVvb3wtzIlzju01m4ADr7G21NpOWpac55hBllzYBaQVAXCjq8BIca/s1098/image6.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghN4NJ5vZ6jESH3koLTfGa3DpSenk5liLEg2awv2cOo1blDwwuDjLGVGxyeHSAzkLWTBUwO_swf4uGC2oShnD0WTNrebCL9KLAMOBIxR3ZZnw9eVS8g16s_lgP5kCbhZmVoTctASyDVvb3wtzIlzju01m4ADr7G21NpOWpac55hBllzYBaQVAXCjq8BIca/s16000/image6.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Training workflow with feature sieve. We alternate between identifying problematic features (using training iteration) and erasing them from the network (using forgetting iteration).</td></tr></tbody></table>



<p>
We describe the identification and erasure steps in more detail: 
</p>
<ul>

<li><b>Identifying simple features</b>:  We train the primary model and the readout model (AUX above) in conventional fashion via forward- and back-propagation. Note that feedback from the auxiliary layer does not back-propagate to the main network. This is to force the auxiliary layer to learn from already-available features rather than create or reinforce them in the main network. 

</li><li><b>Applying the feature sieve</b>: We aim to erase the identified features in the early layers of the neural network with the use of a novel <em>forgetting loss</em>,<em> L<sub>f </sub></em>, which is simply the cross-entropy between the readout and a uniform distribution over labels. Essentially, all information that leads to nontrivial readouts are erased from the primary network. In this step, the auxiliary network and upper layers of the main network are kept unchanged.
</li>
</ul>
<p>
We can control specifically how the feature sieve is applied to a given dataset through a small number of configuration parameters. By changing the position and complexity of the auxiliary network, we control the complexity of the identified- and erased features. By modifying the mixing of learning and forgetting steps, we control the degree to which the model is challenged to learn more complex features. These choices, which are dataset-dependent, are made via <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">hyperparameter search</a> to maximize validation accuracy, a  standard measure of generalization. Since we include “no-forgetting” (i.e., the baseline model) in the search space, we expect to find settings that are at least as good as the baseline.
</p>
<p>
Below we show features learned by the baseline model (middle row) and our model (bottom row) on two benchmark datasets — biased activity recognition (<a href="https://github.com/alinlab/BAR">BAR</a>) and animal categorization (<a href="https://arxiv.org/pdf/1906.02899v3.pdf">NICO</a>). Feature importance was estimated using post-hoc gradient-based importance scoring (<a href="https://arxiv.org/abs/1610.02391">GRAD-CAM</a>), with the orange-red end of the spectrum indicating high importance, while green-blue indicates low importance. Shown below, our trained models focus on the primary object of interest, whereas the baseline model tends to focus on background features that are simpler and spuriously correlated with the label. 
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgumwu2DQ-nPeTLxt_uS6q6tIR6oQZdlWOoM4_I5kUmYfyJi8xyWIpw7WusdRAsA_YthYgO2Zz8sj7V1Id3JOTsljM9zpK2vwhokMfnZQOxbAIWtaFvFN4sfN6qF0rkOklj10y-_rLfL-WQS4zf6AWCub7aUTS7a8LyEsZ5uhQmXjTai7neuWElZBbP_5UI/s1616/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgumwu2DQ-nPeTLxt_uS6q6tIR6oQZdlWOoM4_I5kUmYfyJi8xyWIpw7WusdRAsA_YthYgO2Zz8sj7V1Id3JOTsljM9zpK2vwhokMfnZQOxbAIWtaFvFN4sfN6qF0rkOklj10y-_rLfL-WQS4zf6AWCub7aUTS7a8LyEsZ5uhQmXjTai7neuWElZBbP_5UI/s16000/image2.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Feature importance scoring using GRAD-CAM on activity recognition (BAR) and animal categorization (NICO) generalization benchmarks. Our approach (last row) focuses on the relevant objects in the image, whereas the baseline (ERM; middle row) relies on background features that are spuriously correlated with the label.</td></tr></tbody></table>




<p>
Through this ability to learn better, generalizable features, we show substantial gains over a range of relevant baselines on real-world spurious feature benchmark datasets: <a href="https://github.com/alinlab/BAR">BAR</a>, <a href="https://arxiv.org/pdf/2104.06885.pdf">CelebA Hair</a>, <a href="https://nico.thumedialab.com/">NICO</a> and <a href="https://www.tensorflow.org/datasets/catalog/imagenet_a">ImagenetA</a>, by margins up to 11% (see figure below). More details are available in <a href="https://arxiv.org/abs/2301.13293">our paper</a>.
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjuXHls8mwfL2u-TVZlDlu5UMPrank9F2ODbf6h12q9oMLNrIYyfyv4OuQriS0XzI-z0BrQOs2xUiXt53lGLQtdzmKQDtGXFtv6TZEGg4pKua8JD9AkQn0J92mTjlQAlZTUPgqIYRAFpnsRTU0szE5J90_LeGNj3PTUKrsgq3WAMAjWSy30HQtMnNzevvY/s1082/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="640" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjjuXHls8mwfL2u-TVZlDlu5UMPrank9F2ODbf6h12q9oMLNrIYyfyv4OuQriS0XzI-z0BrQOs2xUiXt53lGLQtdzmKQDtGXFtv6TZEGg4pKua8JD9AkQn0J92mTjlQAlZTUPgqIYRAFpnsRTU0szE5J90_LeGNj3PTUKrsgq3WAMAjWSy30HQtMnNzevvY/w501-h640/image1.png" width="501" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Our feature sieve method improves accuracy by significant margins relative to the nearest baseline for a range of feature generalization benchmark datasets.</td></tr></tbody></table>




<div style="line-height: 40%;">
    <br />
</div>
<h2>Conclusion</h2>


<p>
We hope that our work on early readouts and their use in feature sieving for generalization will both spur the development of a new class of adversarial feature learning approaches and help improve the generalization capability and robustness of deep learning systems. 
</p>


<div style="line-height: 40%;">
    <br />
</div>
<h2>Acknowledgements </h2>


<p>
<em>The work on applying early readouts to debiasing distillation was conducted in collaboration with our academic partners Durga Sivasubramanian, Anmol Reddy and Prof. Ganesh Ramakrishnan at <a href="https://www.iitb.ac.in/">IIT Bombay</a>. We extend our sincere gratitude to Praneeth Netrapalli and Anshul Nasery for their feedback and recommendations. We are also grateful to Nishant Jain, Shreyas Havaldar, Rachit Bansal, Kartikeya Badola, Amandeep Kaur and the whole cohort of pre-doctoral researchers at Google Research India for taking part in research discussions. Special thanks to Tom Small for creating the animation used in this post.</em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-5966553114967673984</id>
            <title>MobileDiffusion: Rapid text-to-image generation on-device</title>
            <link>http://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-5966553114967673984</guid>
            <pubDate></pubDate>
            <updated>2024-01-31T13:59:36.056-08:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOndf55Pc7tkXJektbVBEYRsOlxbUVui2uwOdXvuHj9cNpoNw2One4-68fqFNl2_fvv11CcgYfoI1XVQIkpjA9DosaOeqdkIRj9aZZJNoDy8KqB_XCVDtDd_EvT5UGL2ZhXvL2PU3RjN8XBjI0eQe8VIJCKI0-20AG0TKGK58mO9tBZa80P58KSjTU_liK/s72-c/InstantTIGO%20hero.png"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Yang Zhao, Senior Software Engineer, and Tingbo Hou, Senior Staff Software Engineer, Core ML</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOndf55Pc7tkXJektbVBEYRsOlxbUVui2uwOdXvuHj9cNpoNw2One4-68fqFNl2_fvv11CcgYfoI1XVQIkpjA9DosaOeqdkIRj9aZZJNoDy8KqB_XCVDtDd_EvT5UGL2ZhXvL2PU3RjN8XBjI0eQe8VIJCKI0-20AG0TKGK58mO9tBZa80P58KSjTU_liK/s1600/InstantTIGO%20hero.png" style="display: none;" />

<p>
Text-to-image <a href="https://arxiv.org/abs/2006.11239">diffusion models</a> have shown exceptional capabilities in generating high-quality images from text prompts. However, leading models feature billions of parameters and are consequently expensive to run, requiring powerful desktops or servers (e.g., <a href="https://stability.ai/news/stable-diffusion-public-release">Stable Diffusion</a>, <a href="https://openai.com/research/dall-e">DALL·E</a>, and <a href="https://imagen.research.google/">Imagen</a>). While recent advancements in inference solutions on <a href="https://blog.research.google/2023/06/speed-is-all-you-need-on-device.html">Android</a> via MediaPipe and <a href="https://github.com/apple/ml-stable-diffusion">iOS</a> via Core ML have been made in the past year, rapid (sub-second) text-to-image generation on mobile devices has remained out of reach.
</p> <a name="more"></a>
<p>
To that end, in “<a href="https://arxiv.org/abs/2311.16567">MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices</a>”, we introduce a novel approach with the potential for rapid text-to-image generation on-device. MobileDiffusion is an efficient latent diffusion model specifically designed for mobile devices. We also adopt <a href="https://arxiv.org/abs/2311.09257">DiffusionGAN</a> to achieve one-step sampling during inference, which fine-tunes a pre-trained diffusion model while leveraging a GAN to model the denoising step. We have tested MobileDiffusion on iOS and Android premium devices, and it can run in half a second to generate a 512x512 high-quality image. Its comparably small model size of just 520M parameters makes it uniquely suited for mobile deployment.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody>
  <tr>
    <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgc9IegNp6IHze1sPewUyoR_WouBi8jMhiThcaavD0SXFld3788eA89uyOP6gpmdCXSZMMuacrgQMJ61ygVJsLfE51tqTmmYS0C-GI9SaF_hEGlhTp_zTFXdW_AgXIP5CLCejKQVCsPrhycF8p_Rj9qQHR0J_kTO8Md7VT5R47IMJHinO6dkHn23lUlU7rf/s800/image2.gif" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgc9IegNp6IHze1sPewUyoR_WouBi8jMhiThcaavD0SXFld3788eA89uyOP6gpmdCXSZMMuacrgQMJ61ygVJsLfE51tqTmmYS0C-GI9SaF_hEGlhTp_zTFXdW_AgXIP5CLCejKQVCsPrhycF8p_Rj9qQHR0J_kTO8Md7VT5R47IMJHinO6dkHn23lUlU7rf/s16000/image2.gif" /></a></td>
    
    <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
  
  
  <td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpz0XGSpMH9OVTd865uusar0AeXtu_26HD3tHzJHm2iEVeLYynBhi6pl0tidIYOoJVamc-NplnsNPCNl3vMX-qjqEZCYtndsl-9YjulMpLiDbP3Uws9cZ5ITjb0C3MNaVNC5mh-kbyKZYXn5rxBAuPLaHg_56ZAJfPOrkBfh44goI3CnEW-XZFDUvJgWAV/s800/image5.gif" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpz0XGSpMH9OVTd865uusar0AeXtu_26HD3tHzJHm2iEVeLYynBhi6pl0tidIYOoJVamc-NplnsNPCNl3vMX-qjqEZCYtndsl-9YjulMpLiDbP3Uws9cZ5ITjb0C3MNaVNC5mh-kbyKZYXn5rxBAuPLaHg_56ZAJfPOrkBfh44goI3CnEW-XZFDUvJgWAV/s16000/image5.gif" /></a></td>
  
  
  
  </tr></tbody></table>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td class="tr-caption" style="text-align: center;">Rapid text-to-image generation on-device.</td></tr></tbody></table>




<div style="line-height: 40%;">
    <br />
</div>
<h2>Background</h2>


<p>
The relative inefficiency of text-to-image diffusion models arises from two primary challenges. First, the inherent design of diffusion models requires <a href="https://blog.research.google/2023/06/on-device-diffusion-plugins-for.html">iterative denoising</a> to generate images, necessitating multiple evaluations of the model. Second, the complexity of the network architecture in text-to-image diffusion models involves a substantial number of parameters, regularly reaching into the billions and resulting in computationally expensive evaluations. As a result, despite the potential benefits of deploying generative models on mobile devices, such as enhancing user experience and addressing emerging privacy concerns, it remains relatively unexplored within the current literature.
</p>
<p>
The optimization of inference efficiency in text-to-image diffusion models has been an active research area. Previous studies predominantly concentrate on addressing the first challenge, seeking to reduce the number of function evaluations (NFEs). Leveraging advanced numerical solvers (e.g., <a href="https://arxiv.org/abs/2206.00927">DPM</a>) or distillation techniques (e.g., <a href="https://arxiv.org/abs/2202.00512">progressive distillation</a>, <a href="https://arxiv.org/abs/2303.01469">consistency distillation</a>), the number of necessary sampling steps have significantly reduced from several hundreds to single digits. Some recent techniques, like <a href="https://arxiv.org/abs/2311.09257">DiffusionGAN</a> and <a href="https://arxiv.org/abs/2311.17042#:~:text=We%20introduce%20Adversarial%20Diffusion%20Distillation,while%20maintaining%20high%20image%20quality.">Adversarial Diffusion Distillation</a>, even reduce to a single necessary step. 
</p>
<p>
However, on mobile devices, even a small number of evaluation steps can be slow due to the complexity of model architecture. Thus far, the architectural efficiency of text-to-image diffusion models has received comparatively less attention. A handful of earlier works briefly touches upon this matter, involving the removal of redundant neural network blocks (e.g., <a href="https://snap-research.github.io/SnapFusion/">SnapFusion</a>). However, these efforts lack a comprehensive analysis of each component within the model architecture, thereby falling short of providing a holistic guide for designing highly efficient architectures.
</p>




<div style="line-height: 40%;">
    <br />
</div>
<h2>MobileDiffusion</h2>


<p>
Effectively overcoming the challenges imposed by the limited computational power of mobile devices requires an in-depth and holistic exploration of the model's architectural efficiency. In pursuit of this objective, our research undertakes a detailed examination of each constituent and computational operation within Stable Diffusion’s <a href="https://arxiv.org/abs/2112.10752">UNet architecture</a>. We present a comprehensive guide for crafting highly efficient text-to-image diffusion models culminating in the MobileDiffusion.
</p>
<p>
The design of MobileDiffusion follows that of <a href="https://arxiv.org/abs/2112.10752">latent diffusion models</a>. It contains three components: a text encoder, a diffusion UNet, and an image decoder. For the text encoder, we use <a href="https://arxiv.org/abs/2103.00020">CLIP-ViT/L14</a>, which is a small model (125M parameters) suitable for mobile. We then turn our focus to the diffusion UNet and image decoder. 
</p>



<div style="line-height: 40%;">
    <br />
</div>
<h3>Diffusion UNet</h3>


<p>
As illustrated in the figure below, diffusion UNets commonly interleave transformer blocks and convolution blocks. We conduct a comprehensive investigation of these two fundamental building blocks. Throughout the study, we control the training pipeline (e.g., data, optimizer) to study the effects of different architectures.
</p>
<p>
In classic text-to-image diffusion models, a transformer block consists of a self-attention layer (SA) for modeling long-range dependencies among visual features, a cross-attention layer (CA) to capture interactions between text conditioning and visual features, and a feed-forward layer (FF) to post-process the output of attention layers. These transformer blocks hold a pivotal role in text-to-image diffusion models, serving as the primary components responsible for text comprehension. However, they also pose a significant efficiency challenge, given the computational expense of the attention operation, which is quadratic to the sequence length. We follow the idea of <a href="https://arxiv.org/abs/2301.11093">UViT</a> architecture, which places more transformer blocks at the bottleneck of the UNet. This design choice is motivated by the fact that the attention computation is less resource-intensive at the bottleneck due to its lower dimensionality. 
</p>





<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV/s915/image4.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV/s16000/image4.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Our UNet architecture incorporates more transformers in the middle, and skips self-attention (SA) layers at higher resolutions.</td></tr></tbody></table>




<p>
Convolution blocks, in particular <a href="https://arxiv.org/abs/1512.03385">ResNet</a> blocks, are deployed at each level of the UNet. While these blocks are instrumental for feature extraction and information flow, the associated computational costs, especially at high-resolution levels, can be substantial. One proven approach in this context is <a href="https://arxiv.org/abs/1704.04861">separable convolution</a>. We observed that replacing regular convolution layers with lightweight separable convolution layers in the deeper segments of the UNet yields similar performance.
</p>
<p>
In the figure below, we compare the UNets of several diffusion models. Our MobileDiffusion exhibits superior efficiency in terms of <a href="https://arxiv.org/pdf/2110.12894.pdf">FLOPs</a> (floating-point operations) and number of parameters. 
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s1200/image3.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s16000/image3.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Comparison of some diffusion UNets.</td></tr></tbody></table>




<div style="line-height: 40%;">
    <br />
</div>
<h3>Image decoder</h3>


<p>
In addition to the UNet, we also optimized the image decoder. We trained a <a href="https://arxiv.org/abs/2012.03715">variational autoencoder</a> (VAE) to encode an <a href="https://en.wikipedia.org/wiki/RGB_color_model">RGB</a> image to an 8-channel latent variable, with 8× smaller spatial size of the image. A latent variable can be decoded to an image and gets 8× larger in size.  To further enhance efficiency, we design a lightweight decoder architecture by pruning the original’s width and depth. The resulting lightweight decoder leads to a significant performance boost, with nearly 50% latency improvement and better quality. For more details, please refer to our <a href="https://arxiv.org/abs/2311.16567">paper</a>.
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s1124/image6.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s16000/image6.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">VAE reconstruction. Our VAE decoders have better visual quality than SD (Stable Diffusion).</td></tr></tbody></table>



<br />

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="text-align: center;">
  <tbody><tr>
   <td style="text-align: left;"><b>Decoder</b>
   </td>
   <td><b>&nbsp;&nbsp;#Params (M)&nbsp;&nbsp;</b>
   </td>
   <td><b>&nbsp;&nbsp;PSNR↑&nbsp;&nbsp;</b>
   </td>
   <td><b>&nbsp;&nbsp;SSIM↑&nbsp;&nbsp;</b>
   </td>
   <td><b>&nbsp;&nbsp;LPIPS↓&nbsp;&nbsp;</b>
   </td>
  </tr>
  <tr>
   <td style="text-align: left;"><b>SD</b>
   </td>
   <td>49.5
   </td>
   <td>26.7
   </td>
   <td>0.76
   </td>
   <td>0.037
   </td>
  </tr>
  <tr>
   <td style="text-align: left;"><b>Ours</b>
   </td>
   <td>39.3
   </td>
   <td>30.0
   </td>
   <td>0.83
   </td>
   <td>0.032
   </td>
  </tr>
  <tr>
   <td style="text-align: left;"><b>Ours-Lite&nbsp;&nbsp;&nbsp;&nbsp;</b>
   </td>
   <td>9.8
   </td>
   <td>30.2
   </td>
   <td>0.84
   </td>
   <td>0.032
   </td>
  </tr>
</tbody></table>
<br />

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td class="tr-caption" style="text-align: center;">Quality evaluation of VAE decoders. Our lite decoder is much smaller than SD, with better quality metrics, including <a href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio">peak signal-to-noise ratio</a> (PSNR), <a href="https://en.wikipedia.org/wiki/Structural_similarity">structural similarity index measure</a> (SSIM), and <a href="https://arxiv.org/abs/1801.03924">Learned Perceptual Image Patch Similarity</a> (LPIPS).</td></tr></tbody></table>


<div style="line-height: 40%;">
    <br />
</div>
<h3>One-step sampling</h3>


<p>
In addition to optimizing the model architecture, we adopt a <a href="https://arxiv.org/abs/2311.09257">DiffusionGAN hybrid</a> to achieve one-step sampling. Training DiffusionGAN hybrid models for text-to-image generation encounters several intricacies. Notably, the discriminator, a classifier distinguishing real data and generated data, must make judgments based on both texture and semantics. Moreover, the cost of training text-to-image models can be extremely high, particularly in the case of GAN-based models, where the discriminator introduces additional parameters. Purely GAN-based text-to-image models (e.g., <a href="https://arxiv.org/abs/2301.09515">StyleGAN-T</a>, <a href="https://arxiv.org/abs/2303.05511">GigaGAN</a>) confront similar complexities, resulting in highly intricate and expensive training.
</p>
<p>
To overcome these challenges, we use a pre-trained diffusion UNet to initialize the generator and discriminator. This design enables seamless initialization with the pre-trained diffusion model. We postulate that the internal features within the diffusion model contain rich information of the intricate interplay between textual and visual data. This initialization strategy significantly streamlines the training.
</p>
<p>
The figure below illustrates the training procedure. After initialization, a noisy image is sent to the generator for one-step diffusion. The result is evaluated against ground truth with a reconstruction loss, similar to diffusion model training. We then add noise to the output and send it to the discriminator, whose result is evaluated with a GAN loss, effectively adopting the GAN to model a denoising step. By using pre-trained weights to initialize the generator and the discriminator, the training becomes a fine-tuning process, which converges in less than 10K iterations.  
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ-UILKw/s960/image7.jpg" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ-UILKw/s16000/image7.jpg" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Illustration of DiffusionGAN fine-tuning.</td></tr></tbody></table>



<div style="line-height: 40%;">
    <br />
</div>
<h2>Results</h2>


<p>
Below we show example images generated by our MobileDiffusion with DiffusionGAN one-step sampling. With such a compact model (520M parameters in total), MobileDiffusion can generate high-quality diverse images for various domains.
</p>




<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s1728/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s16000/image1.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Images generated by our MobileDiffusion</td></tr></tbody></table>



<p>
We measured the performance of our MobileDiffusion on both iOS and Android devices, using different runtime optimizers. The latency numbers are reported below. We see that MobileDiffusion is very efficient and can run within half a second to generate a 512x512 image. This lightning speed potentially enables many interesting use cases on mobile devices.
</p>



<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s1184/image8.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s16000/image8.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Latency measurements (<b>s</b>) on mobile devices.</td></tr></tbody></table>



<div style="line-height: 40%;">
    <br />
</div>
<h2>Conclusion</h2>


<p>
With superior efficiency in terms of latency and size, MobileDiffusion has the potential to be a very friendly option for mobile deployments given its capability to enable a rapid image generation experience while typing text prompts. And we will ensure any application of this technology will be in-line with Google’s <a href="https://ai.google/responsibility/responsible-ai-practices/">responsible AI practices</a>.
</p>



<div style="line-height: 40%;">
    <br />
</div>
<h2>Acknowledgments</h2>


<p>
<em>We like to thank our collaborators and contributors that helped bring MobileDiffusion to on-device: Zhisheng Xiao, Yanwu Xu, Jiuqiang Tang, Haolin Jia, Lutz Justen, Daniel Fenner, Ronald Wotzlaw, Jianing Wei, Raman Sarokin, Juhyun Lee, Andrei Kulik, Chuo-Ling Chang, and Matthias Grundmann.</em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-5144906729109253495</id>
            <title>Mixed-input matrix multiplication performance optimizations</title>
            <link>http://blog.research.google/2024/01/mixed-input-matrix-multiplication.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-5144906729109253495</guid>
            <pubDate></pubDate>
            <updated>2024-01-26T11:56:23.553-08:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLpJ1H6BTC6k7BvVWl_gHJpJIOG2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/s72-c/matrixhero.png"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Manish Gupta, Staff Software Engineer, Google Research</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEKJJf1R773hab0veY6zffF2Nf_yfV2mk8YU9yRnuBDD3ak1o0iXecWlJw2x7bL-Ez2MX1c21MXk65VMK5IsoLpJ1H6BTC6k7BvVWl_gHJpJIOG2cm3BwP4V-HCScGHYIynuskbhvu1uorQGprHGbOFmfGI7E5UWemJcZ0xSC3tC5DolBYgyBwugl6OOLr/s1180/matrixhero.png" style="display: none;" />

<p>
AI-driven technologies are weaving themselves into the fabric of our daily routines, with the potential to enhance our access to knowledge and boost our overall productivity. The backbone of these applications lies in large language models (LLMs).  LLMs are memory-intensive and typically require specialized hardware accelerators to efficiently deliver <a href="https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e">tens of exaflops</a> of computing power. This blog post shows how we can start addressing the computational challenges by utilizing memory more effectively.
</p>
<a name="more"></a>
<p>
The bulk of an LLM’s memory and compute are consumed by <a href="https://arxiv.org/pdf/2005.14165.pdf">weights</a> in <a href="https://arxiv.org/pdf/2006.16668.pdf">matrix multiplication</a> operations. Using narrower <em><a href="https://en.wikipedia.org/wiki/Primitive_data_type">data types</a></em> reduces memory consumption. For example, storing weights in the 8-bit <a href="https://en.wikipedia.org/wiki/Integer_(computer_science)">integer</a> (i.e., U8 or S8) data type reduces the memory footprint by 4× relative to <a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format">single-precision</a> (F32) and 2× relative to <a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format">half-precision</a> (F16) or <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">bfloat16</a> (BF16). Furthermore, <a href="https://arxiv.org/pdf/2206.01861.pdf">previous work has</a> shown that LLM models running matrix multiplications with <em>weights</em> in S8 and <em>input</em> in F16 (preserving higher precision of the user-input) is an effective method for increasing the efficiency with acceptable trade-offs in accuracy. This technique is known as <em>weight-only quantization</em> and requires efficient implementation of matrix multiplication with <em>mixed-inputs</em>, e.g., half-precision input multiplied with 8-bits integer. Hardware accelerators, including GPUs, support a fixed set of data types, and thus, mixed-input matrix multiplication requires software transformations to map to the hardware operations.
</p>
<p>
To that end, in this blog we focus on mapping mixed-input matrix multiplication onto the <a href="https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/">NVIDIA Ampere architecture</a>. We present software techniques addressing data type conversion and layout conformance to map mixed-input matrix multiplication efficiently onto hardware-supported data types and layouts. Our results show that the overhead of additional work in software is minimal and enables performance close to the peak hardware capabilities. The software techniques described here are released in the open-source <a href="https://github.com/NVIDIA/cutlass/pull/1084">NVIDIA/CUTLASS</a> repository. 
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaLaSxuLbV_5ifXLyJsTGs0WLa23prrxrhX4IKSLZw5l3oSd2SPk5AgZtNgvUY_j-IbOyjttva-XIfkRr1cDBwCXghEz-3Q0G-6236m7_TIgTrm_K2UejYnTnhAEmZtKHq1mN9HKP0xxV8nqSxzTNHG1U0j-cVj236efpR7lSgmt082QEYNwKsGMTRiWZb/s1999/image3.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgaLaSxuLbV_5ifXLyJsTGs0WLa23prrxrhX4IKSLZw5l3oSd2SPk5AgZtNgvUY_j-IbOyjttva-XIfkRr1cDBwCXghEz-3Q0G-6236m7_TIgTrm_K2UejYnTnhAEmZtKHq1mN9HKP0xxV8nqSxzTNHG1U0j-cVj236efpR7lSgmt082QEYNwKsGMTRiWZb/s16000/image3.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Memory footprint for an 175B parameter LLM model with various data types formats.</td></tr></tbody></table>



<div style="line-height: 40%;">
    <br />
</div>
<h2>The matrix-multiply-accumulate operation</h2>


<p>
Modern AI hardware accelerators such as <a href="https://cloud.google.com/tpu/docs/intro-to-tpu#how_a_tpu_works">Google’s TPU</a> and <a href="https://www.nvidia.com/en-us/data-center/tensor-cores/">NVIDIA’s GPU</a> multiply matrices natively in the hardware by targeting Tensor Cores, which are specialized processing elements to accelerate matrix operations, particularly for AI workloads. In this blog, we focus on NVIDIA Ampere Tensor Cores, which provide the <em>matrix-multiply-accumulate</em> (<code><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma">mma</a></code>) operation. For the rest of the blog the reference to <span style="color: #54863f;"><code>mma</code></span> is for Ampere Tensor Cores. The supported data types, shapes, and data layout of the two input matrices (called operands) for the <span style="color: #54863f;"><code>mma</code></span> operation are fixed in hardware. This means that matrix multiplications with various data types and larger shapes are implemented in the software by tiling the problem onto hardware-supported data types, shapes, and layouts. 

</p>
<p>
The Tensor Core <span style="color: #54863f;"><code>mma</code></span> operation is defined by specifying two input matrices (e.g., <em>A</em> &amp; <em>B</em>, shown below) to produce a result matrix, <em>C</em>. The <span style="color: #54863f;"><code>mma</code></span> operation natively supports mixed-precision. <em><a href="https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/">Mixed-precision Tensor Cores</a></em> allow mixing input (<em>A</em> and <em>B</em>) data type with the result (<em>C</em>) data type. In contrast, <em>mixed-input </em>matrix multiplication involves mixing the input data types, and it is not supported by the hardware, so it needs to be implemented in the software.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS_vu1tTxHo9Gy6Mywfx1xbQ0G6XTpOOQ04-l-Nw_rM7qOAM9kXg_qDjIakIpx-IclRmfR96cTGGExo2k9fxnVdltW4I9nb7RHloRtqWFMFeOtZ68Yr5wve9uLTIsZKA3GxB_VaNo98Gfsa7zGGP0dCrjebZ0Fq1dutfoxoy25eByHXorHCwTTiqsFzw6M/s1039/image5.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS_vu1tTxHo9Gy6Mywfx1xbQ0G6XTpOOQ04-l-Nw_rM7qOAM9kXg_qDjIakIpx-IclRmfR96cTGGExo2k9fxnVdltW4I9nb7RHloRtqWFMFeOtZ68Yr5wve9uLTIsZKA3GxB_VaNo98Gfsa7zGGP0dCrjebZ0Fq1dutfoxoy25eByHXorHCwTTiqsFzw6M/s16000/image5.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Tensor Core operation of M-by-N-by-K on input matrix A of M-by-K and matrix B of K-by-N produces output matrix C of M-by-N.</td></tr></tbody></table>






<div style="line-height: 40%;">
    <br />
</div>
<h2>Challenges of mixed-input matrix multiplication</h2>


<p>
To simplify the discussion, we restrict to a specific example of mixed-input matrix multiplication: F16 for user input and U8 for the model weights (written as F16 * U8). The techniques described here work for various combinations of mixed-input data types. 
</p>
<p>
A GPU programmer can access a <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy">hierarchy of memory</a>, including global memory, shared memory, and registers, which are arranged in order of decreasing capacity but increasing speed. NVIDIA Ampere Tensor Core <span style="color: #54863f;"><code>mma</code></span> operations consume input matrices from registers. Furthermore, input and output matrices are required to conform to a layout of data within a group of 32 threads known as a <em>warp</em>. The supported data type <em>and</em> layout within a warp are fixed for an <span style="color: #54863f;"><code>mma</code></span> operation, so to implement mixed-input multiplication efficiently, it is necessary to solve the challenges of data type conversion and layout conformance in software. 
</p>

<div style="line-height: 40%;">
    <br />
</div>
<h3>Data type conversion </h3>


<p>
The <span style="color: #54863f;"><code>mma</code></span> operation requires two input matrices with the same data type. Thus, mixed-input matrix multiplication, where one of the operands is stored in U8 in global memory and other in F16, requires a data type conversion from U8 to F16. The conversion will bring two operands to F16, mapping the <em>mixed-input</em> matrix multiplication to hardware-supported <em>mixed-precision</em> Tensor Cores. Given the large number of weights, there are a large number of such operations, and our techniques show how to reduce their latency and improve  performance.
</p>


<div style="line-height: 40%;">
    <br />
</div>
<h3>Layout conformance </h3>


<p>
The <span style="color: #54863f;"><code>mma</code></span> operation also requires the layout of two input matrices, within the registers of a warp, to be conformat with hardware specification. The layout for the input matrix <em>B</em> of U8 data type in mixed-input matrix multiplication (F16 * U8) needs to conform with the converted F16 data type. This is called <em>layout conformance</em> and needs to be achieved in the software. 
</p>
<p>
The figure below shows an <span style="color: #54863f;"><code>mma</code></span> operation consuming matrix <em>A</em> and matrix <em>B</em> from registers to produce matrix <em>C</em> in registers, distributed across one warp. The thread <em>T0</em> is highlighted and zoomed in to show the weight matrix <em>B</em> goes through data type conversion and needs a layout conformance to be able to map to the hardware-supported Tensor Core operation.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMMvieW8Uyta8c4afsNM7SgyZtlB2ra7G7aBG4z7D73rn-T7NHge0J1zfK7A_edL9tsQIthWVtEd0hZmwAjfO5C-XM6d5hNkv8IEBlpRxHilOxFgjYi27qauWFAQTl5wV8ixQ9MrfvqpuEQrdFuqDtjPJESG795s6cH3FlPJIVS4TuvKo0gmd8L1HwOJ_6/s1999/image4.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMMvieW8Uyta8c4afsNM7SgyZtlB2ra7G7aBG4z7D73rn-T7NHge0J1zfK7A_edL9tsQIthWVtEd0hZmwAjfO5C-XM6d5hNkv8IEBlpRxHilOxFgjYi27qauWFAQTl5wV8ixQ9MrfvqpuEQrdFuqDtjPJESG795s6cH3FlPJIVS4TuvKo0gmd8L1HwOJ_6/s16000/image4.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The mapping of mixed-input (F32 = F16 * U8) operation in software to natively supported warp-level Tensor Cores in hardware (F32 = F16 * F16). (Original figure source <a href="https://www.nvidia.com/en-us/on-demand/session/gtcsj20-s21745/">Developing CUDA kernels to push Tensor Cores to the Absolute Limit on NVIDIA A100</a>.)</td></tr></tbody></table>







<div style="line-height: 40%;">
    <br />
</div>
<h2>Software strategies addressing challenges</h2>


<p>
A typical data type conversion involves a sequence of operations on 32-bit registers, shown below. Each rectangular block represents a register and the adjoining text are the operations. The entire sequence shows the conversion from 4xU8 to 2x(2xF16). The sequence involves roughly 10 operations. 
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyJ4C214tiBhdjds0fWCV9EWh8X_UEDQlFqkpeoo6CZR3QMMrWyqi5mfRjvHLtbHH55J4hM5oRxe0HouGnbE3KuPbmh8MKk-TtDMMZv1YMKPv-Q4gYAr5l3ZXdTIPUHKs7f8wfCgr3XPe6_jUO7u12pGEmZVFiAGn_LCOlUlQQRSF7_r7jlOrPJW9Oc4V1/s947/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyJ4C214tiBhdjds0fWCV9EWh8X_UEDQlFqkpeoo6CZR3QMMrWyqi5mfRjvHLtbHH55J4hM5oRxe0HouGnbE3KuPbmh8MKk-TtDMMZv1YMKPv-Q4gYAr5l3ZXdTIPUHKs7f8wfCgr3XPe6_jUO7u12pGEmZVFiAGn_LCOlUlQQRSF7_r7jlOrPJW9Oc4V1/s16000/image1.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><code><a href="https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L760">NumericArrayConvertor</a></code> from 4xU8 to 2x(2xF16) in 32-bit registers.</td></tr></tbody></table>






<p>
There are many ways of achieving layout conformance. Two of the existing solutions are:
</p>
<ol>

<li><em>Narrower bitwidth shared memory loads</em>: In this approach, threads issue narrow bitwidth memory loads moving the U8 data from shared memory to registers. This results in <em>two</em> 32-bit registers, with each register containing 2xF16 values (shown above for the matrix <em>B</em>’s thread <em>T0</em>). The narrower shared memory load achieves layout conformance directly into registers without needing any shuffles; however, it does not utilize the full shared memory bandwidth.

</li><li><em>Pre-processing in global memory</em>: An <a href="https://arxiv.org/pdf/2211.10017.pdf">alternative strategy</a> involves rearranging the data within the global memory (one level above the shared memory in <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy">memory hierarchy</a>), allowing wider shared memory loads. This approach maximizes the shared memory bandwidth utilization and ensures that the data is loaded in a conformant layout directly in the registers. Although the rearrangement process can be executed offline prior to the LLM deployment, ensuring no impact on the application performance, it introduces an additional, non-trivial hardware-specific pre-processing step that requires an extra program to rearrange the data. <a href="https://github.com/NVIDIA/FasterTransformer">NVIDIA/FasterTransformer</a> adopts this method to effectively address layout conformance challenges.
</li>
</ol>


<div style="line-height: 40%;">
    <br />
</div>
<h2>Optimized software strategies</h2>


<p>
To further optimize and reduce the overhead of data type conversion and layout conformance, we have implemented <code><a href="https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514">FastNumericArrayConvertor</a></code> and <code><a href="https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/gemm/warp/mma_mixed_input_tensor_op.h#L120">FragmentShuffler</a></code>, respectively. 

</p><p>
<code>FastNumericArrayConvertor</code> operates on 4xU8 in 32-bit registers without unpacking individual 1xU8 values. Furthermore, it uses less expensive arithmetic operations which reduces the number of instructions and increases the speed of the conversion. 
</p>
<p>
The conversion sequence for <a href="https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514">U8-to-F16</a> is shown below. The operations use packed 32b registers, avoiding explicit unpacking and packing. <code>FastNumericArrayConvertor</code> uses the <code><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-prmt">permute byte</a></code> to rearrange bytes of 4xU8 into two registers. Additionally, <code>FastNumericArrayConvertor</code> does not use expensive integer to floating-point conversion instructions and employs vectorized operations to obtain the packed results in <em>two</em> 32-bit registers containing  2x(2xF16) values. The <code>FastNumericArrayConvertor</code> for U8-to-F16 approximately uses six operations, a 1.6× reduction relative to the approach shown above.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRhtLljZ8wfnfnyXQsYZlNMDZ-cUqCV7wPvGimtPtU3JcKJLv6lCDT_PfBBmyp0TuHRgFIZ2cbgEDeL5bqke4FGUcpGMbAhcIBJxQcpcuWZIlqG1yXOHPf5BivF26_qlDnR9W2Y3RVE36ZB7rEGZO3x2Xva7-rqBZkoI7l4gnzBWLYfIrmhFBNN8DpaoEA/s1392/image201.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRhtLljZ8wfnfnyXQsYZlNMDZ-cUqCV7wPvGimtPtU3JcKJLv6lCDT_PfBBmyp0TuHRgFIZ2cbgEDeL5bqke4FGUcpGMbAhcIBJxQcpcuWZIlqG1yXOHPf5BivF26_qlDnR9W2Y3RVE36ZB7rEGZO3x2Xva7-rqBZkoI7l4gnzBWLYfIrmhFBNN8DpaoEA/s16000/image201.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><code>FastNumericArrayConvertor</code> utilizes <code><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-prmt">permute bytes</a></code> and packed arithmetic, reducing the number of instructions in the data type conversion.</td></tr></tbody></table>


<p>
<code>FragmentShuffler</code> handles the layout conformance by shuffling data in a way that allows the use of wider bitwidth load operation, increasing shared memory bandwidth utilization and reducing the total number of operations. 
</p>
<p>
NVIDIA Ampere architecture provides a load matrix instruction (<code><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix">ldmatrix</a></code>). The <span style="color: #54863f;"><code>ldmatrix</code></span> is a warp-level operation, where 32 threads of a warp move the data from shared memory to registers in the <em>shape</em> and <em>layout</em> that <span style="color: #54863f;"><code>mma</code></span> matrix <em>A</em> and <em>B</em> consume. The use of <span style="color: #54863f;"><code>ldmatrix</code></span> <em>reduces</em> the number of load instructions and <em>increases</em> the memory bandwidth utilization. Since the <span style="color: #54863f;"><code>ldmatrix</code></span> instruction moves U8 data to registers, the layout after the load conforms with U8*U8 <span style="color: #54863f;"><code>mma</code></span> operation, and not with F16*F16 <span style="color: #54863f;"><code>mma</code></span> operation. We implemented <code>FragmentShuffler</code> to rearrange the data within registers using shuffle (<code><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions">shfl.sync</a>)</code> operations to achieve the layout conformance. 

</p><p>
The most significant contribution of this work is to achieve layout conformance through register shuffles, avoiding offline pre-processing in global memory or narrower bitwidth shared memory loads. Furthermore, we provide implementations for <code>FastNumericArrayConvertor</code> covering data type conversion from <a href="https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2514">U8-to-F16</a>, <a href="https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2448">S8-to-F16</a>, <a href="https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2546">U8-to-BF16</a>, and <a href="https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L2588">S8-to-BF16</a>.
</p>



<div style="line-height: 40%;">
    <br />
</div>
<h2>Performance results</h2>


<p>
We measured the performance of eight mixed-input variants of <em>our method</em> (shown below in blue and red; varying the data types of matrix <em>A</em> and <em>B</em>) and two <em>mixed-precision</em> data types (shown in green) on an NVIDIA A100 SXM chip. The performance results are shown in <a href="https://en.wikipedia.org/wiki/FLOPS">FLOPS</a> (higher is better). Notably, the first eight matrix-multipications require additional operations relative to the last two, because the mixed-precision variants directly target hardware-accelerated Tensor Core operations and do not need data type conversion and layout conformance. Even so, our approach demonstrates mixed-input matrix multiplication performance only slightly below or on par with mixed-precision. 
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Dq_2LmFUlg0KlNIJvFufCUMZujNc9LcoMnSURpGQwGbM75vXuS-Nm9ZH-7ItgWmZaBSUS3yawN0u3K21tbWTdijU4fVNgEyS33jOztyGfvNvLEw6IBiJO3JSmpctQtN8tvZmagEYQNSP3mmBQnXJ8GeNlQymbeqrKjFycjkKnHL_5FC8V6WR858byfm_/s1999/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Dq_2LmFUlg0KlNIJvFufCUMZujNc9LcoMnSURpGQwGbM75vXuS-Nm9ZH-7ItgWmZaBSUS3yawN0u3K21tbWTdijU4fVNgEyS33jOztyGfvNvLEw6IBiJO3JSmpctQtN8tvZmagEYQNSP3mmBQnXJ8GeNlQymbeqrKjFycjkKnHL_5FC8V6WR858byfm_/s16000/image2.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Mixed-input matrix multiplication  performance on NVIDIA A100 40GB SMX4 chip for a compute-bound matrix problem shape <code>m=3456, n=4096, k=2048.</code></td></tr></tbody></table>






<div style="line-height: 40%;">
    <br />
</div>
<h2>Acknowledgements</h2>


<p>
<em>We would like to mention several folks who have contributed through technical brainstorming and improving the blog post including, Quentin Colombet, Jacques Pienaar, Allie Culp, Calin Cascaval, Ashish Gondimalla, Matt Walsh, Marek Kolodziej, and Aman Bhatia. We would like to thank our NVIDIA partners Rawn Henry, Pradeep Ramani, Vijay Thakkar, Haicheng Wu, Andrew Kerr, Matthew Nicely, and Vartika Singh.</em>
</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>tag:blogger.com,1999:blog-8474926331452026626.post-1418736582601940076</id>
            <title>Exphormer: Scaling transformers for graph-structured data</title>
            <link>http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html</link>
            <guid isPermaLink="false">tag:blogger.com,1999:blog-8474926331452026626.post-1418736582601940076</guid>
            <pubDate></pubDate>
            <updated>2024-01-23T14:27:09.785-08:00</updated>
                
            <media:thumbnail url="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd-7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRnN8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/s72-c/EXPHORMER%2005large.gif"/>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <span class="byline-author">Posted by Ameya Velingker, Research Scientist, Google Research, and Balaji Venkatachalam, Software Engineer, Google</span>

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbovKreBr7RlKc4L36E6rLqiZBZzJSq5GLijCkomHREon5tYXd-7C2pppMXnL5Mj2d82kZGnPlarrrMzQOfRnN8kVvqDh1GnadIJ-hbaaS8VjYzCpaD-DgYor5cKx-OhTGZk9iCy5MjtwG2Q9eTyQiipDr5ViMdl2vkxfbLzWnB3wmLb8YfvVsTJ1FnOmw/s1600/EXPHORMER%2005large.gif" style="display: none;" />

<p>
<a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">Graphs</a>, in which objects and their relations are represented as nodes (or vertices) and edges (or links) between pairs of nodes, are ubiquitous in computing and machine learning (ML). For example, social networks, road networks, and molecular structure and interactions are all domains in which underlying datasets have a natural graph structure. ML can be used to learn the properties of nodes, edges, or entire graphs. 
</p>
<a name="more"></a>

<p>
A common approach to learning on graphs are <a href="https://distill.pub/2021/gnn-intro/">graph neural networks</a> (GNNs), which operate on graph data by applying an optimizable transformation on node, edge, and global attributes. The most typical class of GNNs operates via a <a href="https://wandb.ai/graph-neural-networks/spatial/reports/An-Introduction-to-Message-Passing-Graph-Neural-Networks--VmlldzoyMDI2NTg2">message-passing</a> framework, whereby each layer aggregates the representation of a node with those of its immediate neighbors.
</p>
<p>
Recently, <a href="https://arxiv.org/abs/2012.09699">graph transformer models</a> have emerged as a popular alternative to message-passing GNNs. These models build on the success of <a href="https://en.wikipedia.org/wiki/Transformer_(machine-learning_model)">Transformer architectures</a> in natural language processing (NLP), adapting them to graph-structured data. The attention mechanism in graph transformers can be modeled by an interaction graph, in which edges represent pairs of nodes that attend to each other. Unlike message passing architectures, graph transformers have an interaction graph that is separate from the input graph. The typical interaction graph is a complete graph, which signifies a full attention mechanism<em> </em>that models direct interactions between all pairs of nodes. However, this creates quadratic computational and memory bottlenecks that limit the applicability of graph transformers to datasets on small graphs with at most a few thousand nodes. Making graph transformers scalable has been considered one of the most important research directions in the field (see <a href="https://towardsdatascience.com/graph-ml-in-2022-where-are-we-now-f7f8242599e0">the first open problem here</a>).
</p>
<p>
A natural remedy is to use a <em>sparse</em> interaction graph with fewer edges. <a href="https://dl.acm.org/doi/10.1145/3530811">Many sparse and efficient transformers have been proposed</a> to eliminate the quadratic bottleneck for sequences, however, they do not generally extend to graphs in a principled manner.
</p>
<p>
In “<a href="https://arxiv.org/abs/2303.06147">Exphormer: Sparse Transformers for Graphs</a>”, presented at <a href="https://icml.cc/Conferences/2023/Dates">ICML 2023</a>, we address the scalability challenge by introducing a sparse attention framework for transformers that is designed specifically for graph data. The Exphormer framework makes use of expander graphs, a powerful tool from <a href="https://en.wikipedia.org/wiki/Spectral_graph_theory">spectral graph theory</a>, and is able to achieve strong empirical results on a wide variety of datasets. Our implementation of Exphormer is now available on <a href="https://github.com/hamed1375/Exphormer">GitHub</a>.
</p>
<br /> 

<h2>Expander graphs</h2>


<p>
A key idea at the heart of Exphormer is the use of <a href="https://en.wikipedia.org/wiki/Expander_graph">expander graphs</a>, which are sparse yet well-connected graphs that have some useful properties — 1) the matrix representation of the graphs have similar linear-algebraic properties as a complete graph, and 2) they exhibit rapid mixing of random walks, i.e., a small number of steps in a random walk from any starting node is enough to ensure convergence to a “stable” distribution on the nodes of the graph. Expanders have found applications to diverse areas, such as algorithms, pseudorandomness, complexity theory, and error-correcting codes.
</p>
<p>
A common class of expander graphs are <em>d</em>-regular expanders, in which there are <em>d</em> edges from every node (i.e., every node has degree <em>d</em>). The quality of an expander graph is measured by its <em>spectral gap</em>, an algebraic property of its <a href="https://en.wikipedia.org/wiki/Adjacency_matrix">adjacency matrix</a> (a matrix representation of the graph in which rows and columns are indexed by nodes and entries indicate whether pairs of nodes are connected by an edge). Those that maximize the spectral gap are known as <a href="https://en.wikipedia.org/wiki/Ramanujan_graph">Ramanujan graphs</a> — they achieve a gap of <em>d</em> - 2*√(<em>d</em>-1), which is essentially the best possible among <em>d</em>-regular graphs. A number of deterministic and randomized constructions of Ramanujan graphs have been proposed over the years for various values of <em>d</em>. We use a <a href="https://arxiv.org/abs/cs/0405020">randomized expander construction of Friedman</a>, which produces near-Ramanujan graphs.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg495FZQZ12yMiNhU8C7XUKEJ88H5_v2PPrzhwcDOVnSaVEtdCXaL7py-LzwZZkybKwIaePLHKpdmD6qALfskdjeaA8ML9QYHMwWkxz2ZnhWYqoV1PpnNgbRRfm0pSVYJVrtUpONyyF5PfswJ_QoxD-9vI9F3rF6VQbIRDDIbgvOFc35vTEF9uxizKNpli9/s843/image1.gif" style="margin-left: auto; margin-right: auto;"><img border="0" height="320" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg495FZQZ12yMiNhU8C7XUKEJ88H5_v2PPrzhwcDOVnSaVEtdCXaL7py-LzwZZkybKwIaePLHKpdmD6qALfskdjeaA8ML9QYHMwWkxz2ZnhWYqoV1PpnNgbRRfm0pSVYJVrtUpONyyF5PfswJ_QoxD-9vI9F3rF6VQbIRDDIbgvOFc35vTEF9uxizKNpli9/s320/image1.gif" width="304" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><span id="docs-internal-guid-2920b38b-7fff-2fa8-a3cd-06dfd3ba9968"><span face="Arial, sans-serif" style="font-size: 10pt; font-style: italic; vertical-align: baseline;">Expander graphs are at the heart of Exphormer. A good expander is sparse yet exhibits rapid mixing of random walks, making its global connectivity suitable for an interaction graph in a graph transformer model.</span></span></td></tr></tbody></table>

<p>Exphormer replaces the dense, fully-connected interaction graph of a standard Transformer with edges of a sparse <em>d</em>-regular expander graph. Intuitively, the spectral approximation and mixing properties of an expander graph allow distant nodes to communicate with each other after one stacks multiple attention layers in a graph transformer architecture, even though the nodes may not attend to each other directly. Furthermore, by ensuring that <em>d</em> is constant (independent of the size of the number of nodes), we obtain a linear number of edges in the resulting interaction graph.</p>
<br /> 

<h2>Exphormer: Constructing a sparse interaction graph</h2>


<p>
Exphormer combines expander edges with the input graph and virtual nodes. More specifically, the sparse attention mechanism of Exphormer builds an interaction graph consisting of three types of edges:
</p>
<ul>

<li>Edges from the input graph (<em>local attention</em>)

</li><li>Edges from a constant-degree expander graph (<em>expander attention</em>)

</li><li>Edges from every node to a small set of virtual nodes (<em>global attention</em>)
</li>
</ul>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS7VdL6OcWCmXd-wTtx-qs_nA7qTYZFJOTHS7RZNS3Io_w4km3NM4opPsQBXu1u50KjDA43CsG0hoi1l7I9gq_KGBMvwKEjlWQKBzCeytLQHujF-4K4r9E4F4Q0APvw7le4twjGbDyEiVfEzhbsovhzk2_g4Xd4jwCo66HW7xbnLvm3WPBsHaoq-hDAYX8/s800/image1.gif" style="margin-left: auto; margin-right: auto;"><img border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiS7VdL6OcWCmXd-wTtx-qs_nA7qTYZFJOTHS7RZNS3Io_w4km3NM4opPsQBXu1u50KjDA43CsG0hoi1l7I9gq_KGBMvwKEjlWQKBzCeytLQHujF-4K4r9E4F4Q0APvw7le4twjGbDyEiVfEzhbsovhzk2_g4Xd4jwCo66HW7xbnLvm3WPBsHaoq-hDAYX8/s16000/image1.gif" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><span id="docs-internal-guid-ac11d16d-7fff-62da-cf18-7ba830f677d3"><span face="Arial, sans-serif" style="font-size: 10pt; font-style: italic; vertical-align: baseline;">Exphormer builds an interaction graph by combining three types of edges. The resulting graph has good connectivity properties and retains the inductive bias of the input dataset graph while still remaining sparse.</span></span></td></tr></tbody></table>

<p>
  Each component serves a specific purpose: the edges from the input graph retain the inductive bias from the input graph structure (which typically gets lost in a fully-connected attention module). Meanwhile, expander edges allow good global connectivity and random walk mixing properties (which spectrally approximate the complete graph with far fewer edges). Finally, virtual nodes serve as global “memory sinks” that can directly communicate with every node. While this results in additional edges from each virtual node equal to the number of nodes in the input graph, the resulting graph is still sparse. The degree of the expander graph and the number of virtual nodes are hyperparameters to tune for improving the quality metrics.
</p>
<p>
Furthermore, since we use an expander graph of constant degree and a small constant number of virtual nodes for the global attention, the resulting sparse attention mechanism is linear in the size of the original input graph, i.e., it models a number of direct interactions on the order of the total number of nodes and edges.
</p>
<p>
We additionally show that Exphormer is as expressive as the dense transformer and obeys universal approximation properties. In particular, when the sparse attention graph of Exphormer is augmented with self loops (edges connecting a node to itself), it can universally approximate continuous functions [<a href="https://arxiv.org/abs/1912.10077">1</a>, <a href="https://arxiv.org/abs/2006.04862">2</a>].
</p>
<div style="line-height: 40%;">
    <br />
</div>
<h3>Relation to sparse Transformers for sequences</h3>


<p>
It is interesting to compare Exphormer to sparse attention methods for sequences. Perhaps the architecture most conceptually similar to our approach is <a href="https://blog.research.google/2021/03/constructing-transformers-for-longer.html">BigBird</a>, which  builds an interaction graph by combining different components. BigBird also uses virtual nodes, but, unlike Exphormer, it uses window attention and random attention from an <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model">Erdős-Rényi</a> random graph model for the remaining components.
</p>
<p>
Window attention in BigBird looks at the tokens surrounding a token in a sequence — the local neighborhood attention in Exphormer can be viewed as a generalization of window attention to graphs.
</p>
<p>
The Erdős-Rényi graph on <em>n</em> nodes, <em>G(n, p)</em>, which connects every pair of nodes independently with probability <em>p</em>, also functions as an expander graph for suitably high <em>p</em>. However, a superlinear number of edges (Ω(<em>n</em> log <em>n</em>)) is needed to ensure that an Erdős-Rényi graph is connected, let alone a good expander. On the other hand, the expanders used in Exphormer have only a <em>linear</em> number of edges.
</p>
<br /> 

<h2>Experimental results</h2>


<p>
Earlier works have shown the use of full graph Transformer-based models on datasets with graphs of size up to 5,000 nodes. To evaluate the performance of Exphormer, we build upon the celebrated <a href="https://github.com/rampasek/GraphGPS">GraphGPS framework</a> [<a href="https://arxiv.org/abs/2205.12454">3</a>], which combines both message passing and graph transformers and achieves state-of-the-art performance on a number of datasets. We show that replacing dense attention with Exphormer for the graph attention component in the GraphGPS framework allows one to achieve models with comparable or better performance, often with fewer trainable parameters.
</p>
<p>
Furthermore, Exphormer notably allows graph transformer architectures to scale well beyond the usual graph size limits mentioned above. Exphormer can scale up to datasets of 10,000+ node graphs, such as the <a href="https://arxiv.org/abs/1811.05868">Coauthor dataset</a>, and even beyond to larger graphs such as the well-known <a href="https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv">ogbn-arxiv dataset</a>, a citation network, which consists of 170K nodes and 1.1 million edges.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-HJWH6mqX6N9ytZPbz6wawfMLzF2ey50Ot2BcowvPbQ3FaNwhlEZ3htvDbhq1C6ckLykf0yk3A1sIG0aPGaT8G_aSLj_A-AOfl8NIZdygdkn0C26RzZS9d-9KjyP1f_Zy7suN-iqvYR4zSCgqCXrhP8hVIirUgi6VGEBGx9I_AZikzc_ACKskBMBMPoSw/s1600/ExphormerPerformance.png" style="display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;"><img alt="" border="0" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-HJWH6mqX6N9ytZPbz6wawfMLzF2ey50Ot2BcowvPbQ3FaNwhlEZ3htvDbhq1C6ckLykf0yk3A1sIG0aPGaT8G_aSLj_A-AOfl8NIZdygdkn0C26RzZS9d-9KjyP1f_Zy7suN-iqvYR4zSCgqCXrhP8hVIirUgi6VGEBGx9I_AZikzc_ACKskBMBMPoSw/s1600/ExphormerPerformance.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Results comparing Exphormer to standard GraphGPS on the five <a href="https://arxiv.org/abs/2206.08164">Long Range Graph Benchmark</a> datasets. We note that Exphormer achieved state-of-the-art results on four of the five datasets (PascalVOC-SP, COCO-SP, Peptides-Struct, PCQM-Contact) at the time of the paper’s publication.</td></tr></tbody></table>

<!--<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDbVRMNKr2z64PowKGcaM4NDeiIfzrpfyXe02tRD8tpr_DS99oIjewDwOZZJkNgOr7ZSYwsE5jVqpwOz0Tj2z68SkQzCWtZrhC3cXf2WWfJEZmSfOq3xlGIjdfx-9V0CkbYYv6LU63i1B-suztAyK0Dx8udq2SYSX4TEeP5Erw021KZY8L4FEVNV3BOXaL/s1600/ExphormerPerformance.png" style="display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;"><img alt="" border="0" data-original-height="202" data-original-width="1655" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhDbVRMNKr2z64PowKGcaM4NDeiIfzrpfyXe02tRD8tpr_DS99oIjewDwOZZJkNgOr7ZSYwsE5jVqpwOz0Tj2z68SkQzCWtZrhC3cXf2WWfJEZmSfOq3xlGIjdfx-9V0CkbYYv6LU63i1B-suztAyK0Dx8udq2SYSX4TEeP5Erw021KZY8L4FEVNV3BOXaL/s1600/ExphormerPerformance.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Results comparing Exphormer to standard GraphGPS on the five <a href="https://arxiv.org/abs/2206.08164">Long Range Graph Benchmark</a> datasets. We note that Exphormer achieved state-of-the-art results on four of the five datasets (PascalVOC-SP, COCO-SP, Peptides-Struct, PCQM-Contact) at the time of publication.</td></tr></tbody></table>-->

<!--<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
  <tbody><tr>
   <td align="left"><strong>Model&nbsp;</strong>
   </td>
   <td align="center"><strong>&nbsp;PascalVOC-SP&nbsp;</strong>
<br>
     &nbsp;<font size="-1">F1 score </font><strong>↑</strong>&nbsp;
   </td>
   <td align="center"><strong>&nbsp;COCO-SP&nbsp;</strong>
<br>
     &nbsp;<font size="-1">F1 score </font><strong>↑</strong>&nbsp;
   </td>
   <td align="center"><strong>&nbsp;Peptides-Func&nbsp;</strong>
<br>
     &nbsp;<font size="-1">AP </font><strong>↑</strong>&nbsp;
   </td>
   <td align="center"><strong>&nbsp;Peptides-Struct&nbsp;</strong>
<br>
     &nbsp;<font size="-1">MAE </font><strong>↓</strong>&nbsp;
   </td>
   <td align="center"><strong>&nbsp;PCQM-Contact</strong>
<br>
     &nbsp;<font size="-1">MRR </font><strong>↑</strong>
   </td>
  </tr>
    <tr><td colspan="6"><div style="line-height: 40%;"><br /></div></td></tr> 
  <tr>
    <td>Standard GraphGPS&nbsp;
   </td>
   <td align="center">&nbsp;0.375 ± 0.011&nbsp;
   </td>
   <td align="center">&nbsp;0.341 ± 0.004&nbsp;
   </td>
    <td align="center">&nbsp;<strong>0.654 ± 0.004</strong> &nbsp;
   </td>
   <td align="center">&nbsp;0.250 ± 0.001&nbsp;
   </td>
   <td align="center">&nbsp;0.334 ± 0.001
   </td>
  </tr>
  <tr>
   <td><em>Exphormer (ours)&nbsp;</em>
   </td>
   <td align="center"><strong><em>&nbsp;0.398 ± 0.004&nbsp;</em></strong>
   </td>
   <td align="center"><strong><em>&nbsp;0.346 ± 0.001&nbsp;</em></strong>
   </td>
   <td align="center"><em>&nbsp;0.653 ± 0.004&nbsp;</em>
   </td>
   <td align="center"><strong><em>&nbsp;0.248 ± 0.001&nbsp;</em></strong>
   </td>
   <td align="center"><strong><em>&nbsp;0.364 ± 0.002</em></strong>
   </td>
  </tr> 
</tbody></table>--> 


<p>
Finally, we observe that Exphormer, which creates an overlay graph of small diameter via expanders, exhibits the ability to effectively learn long-range dependencies. The <a href="https://arxiv.org/abs/2206.08164">Long Range Graph Benchmark</a>&nbsp;is a suite of five graph learning datasets designed to measure the ability of models to capture long-range interactions. Results show that Exphormer-based models outperform standard GraphGPS models (which were previously state-of-the-art on four out of five datasets at the time of publication).
</p>
<br /> 

<h2>Conclusion</h2>


<p>
Graph transformers have emerged as an important architecture for ML that adapts the highly successful sequence-based transformers used in NLP to graph-structured data. Scalability has, however, proven to be a major challenge in enabling the use of graph transformers on datasets with large graphs. In this post, we have presented Exphormer, a sparse attention framework that uses expander graphs to improve scalability of graph transformers. Exphormer is shown to have important theoretical properties and exhibit strong empirical performance, particularly on datasets where it is crucial to learn long range dependencies. For more information, we point the reader to a short presentation <a href="https://icml.cc/virtual/2023/poster/23782">video</a> from ICML 2023.
</p>
<br /> 

<h2>Acknowledgements</h2>


<p>
<em>We thank our research collaborators Hamed Shirzad and Danica J. Sutherland from The University of British Columbia as well as Ali Kemal Sinop from Google Research. Special thanks to Tom Small for creating the animation used in this post.</em>
</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>